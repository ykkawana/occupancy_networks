diff --git a/README.md b/README.md
index 4378f27..16febfd 100644
--- a/README.md
+++ b/README.md
@@ -1,169 +1,138 @@
-# Occupancy Networks
-![Example 1](img/00.gif)
-![Example 2](img/01.gif)
-![Example 3](img/02.gif)
+# PSNet experiments
 
-This repository contains the code to reproduce the results from the paper
-[Occupancy Networks - Learning 3D Reconstruction in Function Space](https://avg.is.tuebingen.mpg.de/publications/occupancy-networks).
+## Directory structure
+`external/<model of previous works>/`: Place model of external works. It needs to be modularized.
+`out/submission/eval/`: Evaluation results are stored.
+`paper_resources/`: Tex resources and their generation scripts are stored.
 
-You can find detailed usage instructions for training your own models and using pretrained models below.
-
-If you find our code or paper useful, please consider citing
-
-    @inproceedings{Occupancy Networks,
-        title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
-        author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
-        booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
-        year = {2019}
-    }
-
-## Installation
-First you have to make sure that you have all dependencies in place.
-The simplest way to do so, is to use [anaconda](https://www.anaconda.com/). 
-
-You can create an anaconda environment called `mesh_funcspace` using
+## Prerequiste
+### venv
 ```
-conda env create -f environment.yaml
-conda activate mesh_funcspace
+source env/bin/activate
 ```
-
-Next, compile the extension modules.
-You can do this via
+### PYTHONPATH
 ```
-python setup.py build_ext --inplace
+export PYTHONPATH=$PYTHONPATH:external/periodic_shapes:$PWD:external/atlasnetv2
 ```
 
-To compile the dmc extension, you have to have a cuda enabled device set up.
-If you experience any errors, you can simply comment out the `dmc_*` dependencies in `setup.py`.
-You should then also comment out the `dmc` imports in `im2mesh/config.py`.
-
-## Demo
-![Example Input](img/example_input.png)
-![Example Output](img/example_output.gif)
-
-You can now test our code on the provided input images in the `demo` folder.
-To this end, simply run
+## Training
+### Single GPU
 ```
-python generate.py configs/demo.yaml
+CUDA_VISIBLE_DEVICES=0 python3 train.py out/img/pnet.config
 ```
-This script should create a folder `demo/generation` where the output meshes are stored.
-The script will copy the inputs into the `demo/generation/inputs` folder and creates the meshes in the `demo/generation/meshes` folder.
-Moreover, the script creates a `demo/generation/vis` folder where both inputs and outputs are copied together.
-
-## Dataset
-
-To evaluate a pretrained model or train a new model from scratch, you have to obtain the dataset.
-To this end, there are two options:
-
-1. you can download our preprocessed data
-2. you can download the ShapeNet dataset and run the preprocessing pipeline yourself
-
-Take in mind that running the preprocessing pipeline yourself requires a substantial amount time and space on your hard drive.
-Unless you want to apply our method to a new dataset, we therefore recommmend to use the first option.
-
-### Preprocessed data
-You can download our preprocessed data (73.4 GB) using
-
+### Multiple GPUs
 ```
-bash scripts/download_data.sh
+CUDA_VISIBLE_DEVICES=0,1,2 python3 dist_train.py out/img/pnet.config
 ```
 
-This script should download and unpack the data automatically into the `data/ShapeNet` folder.
-
-### Building the dataset
-Alternatively, you can also preprocess the dataset yourself.
-To this end, you have to follow the following steps:
-* download the [ShapeNet dataset v1](https://www.shapenet.org/) and put into `data/external/ShapeNet`. 
-* download the [renderings and voxelizations](http://3d-r2n2.stanford.edu/) from Choy et al. 2016 and unpack them in `data/external/Choy2016` 
-* build our modified version of [mesh-fusion](https://github.com/davidstutz/mesh-fusion) by following the instructions in the `external/mesh-fusion` folder
-
-You are now ready to build the dataset:
+## Evaluation
+### IoU
 ```
-cd scripts
-bash dataset_shapenet/build.sh
-``` 
-
-This command will build the dataset in `data/ShapeNet.build`.
-To install the dataset, run
+CUDA_VISIBLE_DEVICES=0 python3 eval.py out/img/pnet.yamla <config overwrite options. e.g. --test.threshold 0.999999>
 ```
-bash dataset_shapenet/install.sh
+This creates copied run directory `out/submission/eval/img/<out_dir>_<YYYYMMDD>`.
+`<out_dir>` is written in config yaml.
+For PSNet, `--test.threshold 0.999999` is usually used for IoU.
+#### Generate IoU result in existing directory
+Use `--dontcopy` option with config.yaml created by eval.py.
+E.g.
 ```
+CUDA_VISIBLE_DEVICES=0 python3 eval.py out/submission/eval/img/config.yaml
+```
+#### Generate IoU result of the run which doesn't have output dir under out/.
+Use `--no_copy_but_create_new` option.
 
-If everything worked out, this will copy the dataset into `data/ShapeNet`.
-
-## Usage
-When you have installed all binary dependencies and obtained the preprocessed data, you are ready to run our pretrained models and train new models from scratch.
-
-### Generation
-To generate meshes using a trained model, use
+#### Generate IoU result for BSPNet
 ```
-python generate.py CONFIG.yaml
+sh external/bspnet/scripts/eval_iou.sh <config path> <GPU id>
 ```
-where you replace `CONFIG.yaml` with the correct config file.
 
-The easiest way is to use a pretrained model.
-You can do this by using one of the config files
-```
-configs/img/onet_pretrained.yaml
-configs/pointcloud/onet_pretrained.yaml
-configs/voxels/onet_pretrained.yaml
-configs/unconditional/onet_cars_pretrained.yaml
-configs/unconditional/onet_airplanes_pretrained.yaml
-configs/unconditional/onet_sofas_pretrained.yaml
-configs/unconditional/onet_chairs_pretrained.yaml
-```
-which correspond to the experiments presented in the paper.
-Our script will automatically download the model checkpoints and run the generation.
-You can find the outputs in the `out/*/*/pretrained` folders.
-
-Please note that the config files  `*_pretrained.yaml` are only for generation, not for training new models: when these configs are used for training, the model will be trained from scratch, but during inference our code will still use the pretrained model.
-
-### Evaluation
-For evaluation of the models, we provide two scripts: `eval.py` and `eval_meshes.py`.
+### Generate Mesh
+For PSNet and AtlasNet V2,
+```
+sh im2mesh/<pnet or atlasnetv2>/script/generate_mesh.sh out/submission/eval/img/<out_dir>_<YYYYMMDD>/config.yaml
+```
+This automatically creates config yaml:
+```
+im2mesh/<pnet or atlasnetv2>/script/generate_mesh.sh out/submission/eval/img/<out_dir>_<YYYYMMDD>/gen_***_<yyyymmdd>.yaml <GPU id>
+```
 
-The main evaluation script is `eval_meshes.py`.
-You can run it using
+### Fscore
+Use generated config file by mesh generation.
 ```
-python eval_meshes.py CONFIG.yaml
+CUDA_VISIBLE_DEVICES=0 python3 eval_fscore.py out/submission/eval/img/<out_dir>_<YYYYMMDD>/gen_***_<yyyymmdd>.yaml
 ```
-The script takes the meshes generated in the previous step and evaluates them using a standardized protocol.
-The output will be written to `.pkl`/`.csv` files in the corresponding generation folder which can be processed using [pandas](https://pandas.pydata.org/).
 
-For a quick evaluation, you can also run
+### Chamfer distance, normal consistency
+Use generated config file by mesh generation.
 ```
-python eval.py CONFIG.yaml
+CUDA_VISIBLE_DEVICES=0 python3 eval_mesh.py out/submission/eval/img/<out_dir>_<YYYYMMDD>/gen_***_<yyyymmdd>.yaml
 ```
-This script will run a fast method specific evaluation to obtain some basic quantities that can be easily computed without extracting the meshes.
-This evaluation will also be conducted automatically on the validation set during training.
 
-All results reported in the paper were obtained using the `eval_meshes.py` script.
+## Experiment specific
+All paper related scripts are in `paper_resources/`.
+Generated resources for tex are stored in `paper_resources/<experiment>/resoureces`.
+Some experiments requires to run IoU and fscore evaluation before the experiment.
+Scripts for generating resources are stored in `paper_resources/<experiment>/scripts`.
+### Mesh quality comparison
+Under `paper_resources/compare_mesh_methods`.
+Run `paper_resources/compare_mesh_methods/scripts/mesh_quality_comparison.ipynb`.
 
-### Training
-Finally, to train a new network from scratch, run
+### SVR Pix3D evaluation
+Under `paper_resources/pix3d_comparison`
+Fist, You need to create OccNet experiment environment compatible Pix3D dataset.
+Run scripts in following steps to create the Dataset.
+1. Compute stats of ShapeNet renderings margin.
 ```
-python train.py CONFIG.yaml
+python3 paper_resources/pix3d_comparison/scripts/calc_stats_of_margin_of_rendering.py
 ```
-where you replace `CONFIG.yaml` with the name of the configuration file you want to use.
-
-You can monitor on <http://localhost:6006> the training process using [tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard):
+2. Create the compatible renderings of Pix3D.
+```
+python3 paper_resources/pix3d_comparison/scripts/convert_pix3d_images.py
+```
+3. Create the surface mesh.
+```
+python3 paper_resources/pix3d_comparison/scripts/convert_pix3d_models.py
 ```
-cd OUTPUT_DIR
-tensorboard --logdir ./logs --port 6006
+4. Create sample lists.
 ```
-where you replace `OUTPUT_DIR` with the respective output directory.
+python3 paper_resources/pix3d_comparison/scripts/generate_list.py
+```
+After finising Pix3D dataset conversion, generate resoureces.
+1. Generate mesh for Pix3D for PSNet (for atlasnetv2, TBD)
+```
+sh paper_resources/pix3d_comparison/scripts/generate_explicit_pix3d_mesh.sh  out/submission/eval/img/<out_dir>_<YYYYMMDD>/config.yaml <GPU id>
+```
+3. Generate Fscore
+2. Generate resoureces
+```
+CUDA_VISIBLE_DEVICES=0 python3 paper_resources/pix3d_comparison/scripts/generate_pix3d_table_and_generate_renderings.py
+```
+### SVR ShapeNet evaluation.
+Under `paper_resources/shapenet_svr_comparison`.
+1. Generate IoU, fscore, CD1 metrics.
+2. Prep csv tables of metrics of previous works under `paper_resources/shapenet_svr_comparison/csv`.
+3. Run `paper_resources/shapenet_svr_comparison/scripts/generate_svr_table.ipynb`.
+4. Tex table is generated in `paper_resources/shapenet_svr_comparison/table.txt`.
 
-For available training options, please take a look at `configs/default.yaml`.
+### Primitive visualization
+This experiment render primitives and evaluate part semseg.
+For visualize primitives,
+```
+CUDA_VISIBLE_DEVICES=0 python3 paper_resources/primitive_visualization/scripts/render_primitives.py
+```
+For evaluate part semseg,
+1. Generate semseg data.
+```
+python3 paper_resources/primitive_visualization/scripts/generate_semseg_data.py
+``` 
+2. TBD
 
-# Notes
-* In our paper we used random crops and scaling to augment the input images. 
-  However, we later found that this image augmentation decreases performance on the ShapeNet test set.
-  The pretrained model that is loaded in `configs/img/onet_pretrained.yaml` was hence trained without data augmentation and has slightly better performance than the model from the paper. The updated table looks a follows:
-  ![Updated table for single view 3D reconstruction experiment](img/table_img2mesh.png)
-  For completeness, we also provide the trained weights for the model which was used in the paper in  `configs/img/onet_legacy_pretrained.yaml`.
-* Note that training and evaluation of both our model and the baselines is performed with respect to the *watertight models*, but that normalization into the unit cube is performed with respect to the *non-watertight meshes* (to be consistent with the voxelizations from Choy et al.). As a result, the bounding box of the sampled point cloud is usually slightly bigger than the unit cube and may differ a little bit from a point cloud that was sampled from the original ShapeNet mesh.
+## Other notes
+### Rendering
+Rendering script is in `/home/mil/kawana/workspace/RenderForCNN`.
+Wrapper for this project is in `scripts/render_3dobj.sh`.
 
-# Futher Information
-Please also check out the following concurrent papers that have proposed similar ideas:
-* [Park et al. - DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation (2019)](https://arxiv.org/abs/1901.05103)
-* [Chen et al. - Learning Implicit Fields for Generative Shape Modeling (2019)](https://arxiv.org/abs/1812.02822)
-* [Michalkiewicz et al. - Deep Level Sets: Implicit Surface Representations for 3D Shape Inference (2019)](https://arxiv.org/abs/1901.06802)
+### Visualization
+Simple mesh visualization by trimesh, see `scripts/show_mesh.py`.
+Point cloud visualization by plotly, see `external/periodic_shapes/periodic_shapes/visualize/plot.py`.
diff --git a/configs/default.yaml b/configs/default.yaml
index 52c7f35..919e7da 100644
--- a/configs/default.yaml
+++ b/configs/default.yaml
@@ -61,8 +61,17 @@ test:
   eval_mesh: true
   eval_pointcloud: true
   eval_fscore: true
-  fscore_thresholds: [0.005, 0.01, 0.0107337006427915, 0.02, 0.05, 0.1, 0.2]
+  fscore_thresholds:
+  - 0.005053668503213957
+  - 0.0107337006427915 # as bbox's side length is slightly bigger as normalization happens before meshing.
+  - 0.02021467401285583
+  - 0.050536685032139574
+  - 0.10107337006427915
+  - 0.2021467401285583
   model_file: model_best.pt
+  n_points: 100000
+  is_sample_from_surface: false
+  is_normalize_by_side_length: false
 generation:
   batch_size: 100000
   refinement_step: 0
diff --git a/configs/img/debug_atlasnetv2.yaml b/configs/img/debug_atlasnetv2.yaml
index 2906cdc..9a3bfe3 100644
--- a/configs/img/debug_atlasnetv2.yaml
+++ b/configs/img/debug_atlasnetv2.yaml
@@ -16,14 +16,15 @@ model:
   encoder_latent: null
   decoder: atlasnetv2decoder 
   encoder: resnet18
-  c_dim: 1024
+  c_dim: 256
   z_dim: 0
   decoder_kwargs:
     npatch: 10
     patchDim: 2
     patchDeformDim: 3
+    hidden_size: 256
 training:
-  out_dir:  out/img/debug_atlasnetv2-2
+  out_dir:  out/img/debug_atlasnetv2
   batch_size: 20
   val_batch_size: 2
   model_selection_metric: cd1
@@ -36,6 +37,8 @@ training:
   learning_rage_decay_at:
     - 250
     - 300
+trainer:
+  point_scale: 2
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/debug_atlasnetv2_pointnet.yaml b/configs/img/debug_atlasnetv2_pointnet.yaml
deleted file mode 100644
index 9fda6fa..0000000
--- a/configs/img/debug_atlasnetv2_pointnet.yaml
+++ /dev/null
@@ -1,56 +0,0 @@
-method: atlasnetv2
-data:
-  path: data/ShapeNet
-  train_split: train # Overfit to train data, see if training is working
-  val_split: train
-  test_split: train
-  classes: ['02691156'] # plane
-  img_folder: img_choy2016
-  img_size: 224 
-  input_type: pointcloud
-  pointcloud_n: 2500
-  #pointcloud_noise: 0.000
-  pointcloud_target_n: 4096
-  patch_side_length: 16 # will be powered by dim - 1, if dim == 3, then 900
-  is_generate_mesh: false
-  debug:
-    sample_n: 50
-model:
-  encoder_latent: null
-  decoder: atlasnetv2decoder 
-  encoder: pointnet_atlasnetv2
-  encoder_kwargs:
-    hidden_dim: 1024
-  c_dim: 2500
-  z_dim: 0
-  decoder_kwargs:
-    npatch: 10
-    patchDim: 2
-    patchDeformDim: 3
-    hidden_size: 1024
-training:
-  out_dir:  out/img/debug_atlasnetv2_pointnet
-  batch_size: 10
-  val_batch_size: 2
-  model_selection_metric: cd1
-  model_selection_mode: minimize
-  visualize_every: 500
-  validate_every: 1000
-  checkpoint_every: 100000
-  skip_load_pretrained_optimizer: false
-  learning_rate: 1e-3
-  learning_rage_decay_at:
-    - 250
-    - 300
-test:
-  threshold: 0.5
-  eval_mesh: true
-  eval_pointcloud: false
-generation:
-  batch_size: 100000
-  refine: false
-  n_x: 128
-  n_z: 1
-  resolution_0: 32 
-  upsampling_steps: 2
-
diff --git a/configs/img/debug_onet.yaml b/configs/img/debug_onet.yaml
index b444c37..1a68a93 100644
--- a/configs/img/debug_onet.yaml
+++ b/configs/img/debug_onet.yaml
@@ -1,3 +1,5 @@
 inherit_from: configs/img/onet.yaml
 data:
   classes: ['02691156'] # plane
+training:
+    out_dir: out/img/debug_onet_eval
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml
index 3503f68..ffe24c1 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml
@@ -42,6 +42,7 @@ trainer:
   self_overlap_reg_threshold: 0.1
   is_normal_loss: false
   normal_loss_coef: 3
+  is_cvx_net_merged_loss: true
   cvx_net_merged_loss_coef: 100
   cvx_net_merged_loss_topk_samples: 10
 training:
@@ -55,6 +56,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: cffp5ly9
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml
index 99ac22c..b188ba7 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: qguu0ndb
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml
index 95164fe..3ae1e1d 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: biibxzh2
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
index 3455101..44242d0 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: o8pj91rl
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml
index c049e61..25f8f2c 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: 3cagj5tz
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml
index 6354414..7c32328 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: dqavijvk
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/eval.py b/eval.py
index 4f6589e..8a29102 100644
--- a/eval.py
+++ b/eval.py
@@ -12,6 +12,10 @@ from im2mesh.checkpoints import CheckpointIO
 import shutil
 import yaml
 from collections import OrderedDict
+import eval_utils
+from datetime import datetime
+
+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
 
 
 def represent_odict(dumper, instance):
@@ -30,57 +34,77 @@ yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
 parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
+parser.add_argument('--dontcopy', action='store_true', help='Do not use cuda.')
+parser.add_argument('--no_copy_but_create_new',
+                    action='store_true',
+                    help='Do not use cuda.')
+parser.add_argument('--use_config_in_eval_dir',
+                    action='store_true',
+                    help='Do not use cuda.')
 
 # Get configuration and basic arguments
 args, unknown_args = parser.parse_known_args()
 cfg = config.load_config(args.config, 'configs/default.yaml')
-for idx, arg in enumerate(unknown_args):
-    if arg.startswith('--'):
-        arg = arg.replace('--', '')
-        value = unknown_args[idx + 1]
-        keys = arg.split('.')
-        if keys[0] not in cfg:
-            cfg[keys[0]] = {}
-        child_cfg = cfg.get(keys[0], {})
-        for key in keys[1:]:
-            item = child_cfg.get(key, None)
-            if isinstance(item, dict):
-                child_cfg = item
-            elif item is None:
-                child_cfg[key] = value
-            else:
-                child_cfg[key] = type(item)(value)
+
+eval_utils.update_dict_with_options(cfg, unknown_args)
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
 # Shorthands
-if '--dontcopy' in unknown_args:
+
+if args.dontcopy:
     out_dir = cfg['training']['out_dir']
+elif args.use_config_in_eval_dir:
+    out_dir = os.path.dirname(args.config)
 else:
+    out_dir = os.path.join('out', cfg['data']['input_type'],
+                           os.path.basename(args.config).split('.')[0])
+    cfg['training']['out_dir'] = out_dir
     base_out_dir = cfg['training']['out_dir']
     out_dir = os.path.join(
         os.path.dirname(base_out_dir).replace('out', 'out/submission/eval'),
         os.path.basename(base_out_dir)) + '_' + datetime.now().strftime(
             ('%Y%m%d_%H%M%S'))
 print('out dir for eval: ', out_dir)
-if not '--dontcopy' in unknown_args:
+if not (args.dontcopy or args.use_config_in_eval_dir):
     if not os.path.exists(out_dir):
-        shutil.copytree(base_out_dir, out_dir)
+        if args.no_copy_but_create_new:
+            os.makedirs(out_dir)
+        else:
+            #shutil.copytree(base_out_dir, out_dir)
+            os.makedirs(out_dir)
+            best_file = cfg['test']['model_file']
+            best_path = os.path.join(base_out_dir, best_file)
+            shutil.copy2(best_path, out_dir)
     else:
         raise ValueError('out dir already exists')
 
-if not '--dontcopy' in unknown_args:
+threshold_txt_path = os.path.join(out_dir, 'threshold')
+if os.path.exists(threshold_txt_path):
+    with open(threshold_txt_path) as f:
+        threshold = float(f.readlines()[0].strip())
+        print('Use threshold in dir', threshold)
+        cfg['test']['threshold'] = threshold
+
+if not (args.dontcopy or args.use_config_in_eval_dir):
     patch_path = os.path.join(out_dir, 'diff.patch')
     subprocess.run('git diff > {}'.format(patch_path), shell=True)
-    weight_path = os.path.join(out_dir, cfg['test']['model_file'])
-    with open(weight_path, 'rb') as f:
-        md5 = hashlib.md5(f.read()).hexdigest()
-    cfg['test']['model_file_hash'] = md5
-    yaml.dump(cfg, open(os.path.join(out_dir, 'config.yaml'), 'w'))
+    if not cfg['test']['model_file'].startswith('http'):
+        weight_path = os.path.join(out_dir, cfg['test']['model_file'])
+        with open(weight_path, 'rb') as f:
+            md5 = hashlib.md5(f.read()).hexdigest()
+        cfg['test']['model_file_hash'] = md5
+yaml.dump(
+    cfg,
+    open(os.path.join(out_dir, 'eval_config_{}.yaml'.format(date_str)), 'w'))
 
 out_file = os.path.join(out_dir, 'eval_full.pkl')
 out_file_class = os.path.join(out_dir, 'eval.csv')
 
+if (args.dontcopy or args.use_config_in_eval_dir):
+    t = datetime.now().strftime('%Y%m%d_%H%M%S')
+    out_file = out_file.replace('.pkl', '_' + t + '.pkl')
+    out_file_class = out_file.replace('.csv', '_' + t + '.csv')
 # Dataset
 dataset = config.get_dataset('test', cfg, return_idx=True)
 model = config.get_model(cfg, device=device, dataset=dataset)
@@ -97,7 +121,6 @@ trainer = config.get_trainer(model, None, cfg, device=device)
 
 # Print model
 nparameters = sum(p.numel() for p in model.parameters())
-print(model)
 print('Total number of parameters: %d' % nparameters)
 
 # Evaluate
@@ -139,17 +162,6 @@ for it, data in enumerate(tqdm(test_loader)):
         'modelname': modelname,
     }
 
-    inputs = data.get('inputs', torch.empty(points.size(0), 0)).to(device)
-    angles = data.get('angles').to(device)
-    points = data.get('points').to(device)
-
-    feature = model.encode_inputs(inputs)
-
-    kwargs = {}
-    scaled_coord = points * cfg['trainer']['pnet_point_scale']
-    output = model.decode(scaled_coord, None, feature, angles=angles, **kwargs)
-    super_shape_point, surface_mask, sgn, sgn_BxNxNP, radius = output
-
     eval_dicts.append(eval_dict)
     eval_data = trainer.eval_step(data)
     eval_dict.update(eval_data)
diff --git a/eval_fscore.py b/eval_fscore.py
index 593d769..de3545c 100644
--- a/eval_fscore.py
+++ b/eval_fscore.py
@@ -10,6 +10,10 @@ from im2mesh import config, data
 from im2mesh.eval import MeshEvaluator
 from im2mesh.utils.io import load_pointcloud
 import numpy as np
+from datetime import datetime
+import yaml
+import eval_utils
+
 parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
@@ -17,8 +21,17 @@ parser.add_argument('--eval_input',
                     action='store_true',
                     help='Evaluate inputs instead.')
 
-args = parser.parse_args()
+parser.add_argument('--unique_name',
+                    default='',
+                    type=str,
+                    help='String name for generation.')
+args, unknown_args = parser.parse_known_args()
 cfg = config.load_config(args.config, 'configs/default.yaml')
+eval_utils.update_dict_with_options(cfg, unknown_args)
+
+if cfg['test']['is_normalize_by_side_length']:
+    cfg['test']['fscore_thresholds'] = [0.005, 0.01, 0.02, 0.1, 0.2]
+
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
@@ -26,22 +39,27 @@ is_eval_explicit_mesh = cfg['test'].get('is_eval_explicit_mesh', False)
 # Shorthands
 #out_dir = cfg['training']['out_dir']
 out_dir = os.path.dirname(args.config)
-generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
+generation_dir = os.path.dirname(args.config)
+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
+assert generation_dir.endswith(cfg['generation']['generation_dir'])
 if not args.eval_input:
     out_file = os.path.join(
-        generation_dir, 'eval_fscore_from_meshes_full{}.pkl'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_fscore_from_meshes_full_{}_{}_{}.pkl'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
     out_file_class = os.path.join(
-        generation_dir, 'eval_fscore_from_meshes{}.csv'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_fscore_from_meshes_{}_{}_{}.csv'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
 else:
     out_file = os.path.join(
-        generation_dir, 'eval_fscore_from_input_full{}.pkl'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_fscore_from_input_full_{}_{}_{}.pkl'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
     out_file_class = os.path.join(
-        generation_dir, 'eval_fscore_from_meshes_input{}.csv'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
-
+        generation_dir, 'eval_fscore_from_meshes_input_{}_{}_{}.csv'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
 # Dataset
 points_field = data.PointsField(
     cfg['data']['points_iou_file'],
@@ -66,8 +84,19 @@ if 'debug' in cfg['data']:
     dataset = torch_data.Subset(dataset,
                                 range(cfg['data']['debug']['sample_n']))
 
+yaml.dump(
+    cfg,
+    open(
+        os.path.join(
+            generation_dir,
+            'fscore_config_{}_{}.yaml'.format(args.unique_name, date_str)),
+        'w'))
+
 # Evaluator
-evaluator = MeshEvaluator(n_points=100000)
+evaluator = MeshEvaluator(
+    n_points=cfg['test']['n_points'],
+    is_sample_from_surface=cfg['test']['is_sample_from_surface'],
+    is_normalize_by_side_length=cfg['test']['is_normalize_by_side_length'])
 
 # Loader
 test_loader = torch.utils.data.DataLoader(dataset,
@@ -129,30 +158,67 @@ for it, data in enumerate(tqdm(test_loader)):
 
     # Evaluate mesh
     if cfg['test']['eval_fscore']:
-        mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
+        vertex_visibility = None
+        if cfg['method'] == 'pnet':
+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
 
-        if is_eval_explicit_mesh:
-            visbility_file = os.path.join(
-                mesh_dir, '%s_vertex_visbility.npz' % modelname)
-        if os.path.exists(mesh_file):
-            mesh = trimesh.load(mesh_file, process=False)
+            if os.path.exists(mesh_file):
+                mesh = trimesh.load(mesh_file, process=False)
+            else:
+                print('Warning: mesh file does not exist: %s' % mesh_file)
+                continue
             if is_eval_explicit_mesh:
-                vertex_visibility = np.load(
-                    visbility_file)['vertex_visibility']
+                visbility_file = os.path.join(
+                    mesh_dir, '%s_vertex_visbility.npz' % modelname)
+                if os.path.exists(visbility_file):
+                    vertex_visibility = np.load(
+                        visbility_file)['vertex_visibility']
+                else:
+                    print('Warning: vibility file does not exist: %s' %
+                          visbility_file)
+                    continue
+        elif cfg['method'] == 'bspnet':
+            vertex_file = os.path.join(mesh_dir,
+                                       '%s_vertex_attributes.npz' % modelname)
+            is_eval_explicit_mesh = True
+            if os.path.exists(vertex_file):
+                try:
+                    vertex_attributes = np.load(vertex_file)
+                except:
+                    print('Error in bspnet loading vertex')
+                    continue
+                mesh = trimesh.Trimesh(vertex_attributes['vertices'])
+                vertex_visibility = vertex_attributes['vertex_visibility']
+            else:
+                print('Warning: vertex file does not exist: %s' %
+                      visbility_file)
+                continue
+        elif cfg['method'] == 'atlasnetv2':
+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
+
+            if os.path.exists(mesh_file):
+                mesh = trimesh.load(mesh_file, process=False)
             else:
-                vertex_visibility = None
-
-            eval_dict_mesh = evaluator.eval_fscore_from_mesh(
-                mesh,
-                pointcloud_tgt,
-                cfg['test']['fscore_thresholds'],
-                is_eval_explicit_mesh=is_eval_explicit_mesh,
-                vertex_visibility=vertex_visibility)
-            if eval_dict_mesh is not None:
-                for k, v in eval_dict_mesh.items():
-                    eval_dict[k + ' (mesh)'] = v
+                print('Warning: mesh file does not exist: %s' % mesh_file)
+                continue
         else:
-            print('Warning: mesh does not exist: %s' % mesh_file)
+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
+
+            if os.path.exists(mesh_file):
+                mesh = trimesh.load(mesh_file, process=False)
+            else:
+                print('Warning: mesh file does not exist: %s' % mesh_file)
+                continue
+
+        eval_dict_mesh = evaluator.eval_fscore_from_mesh(
+            mesh,
+            pointcloud_tgt,
+            cfg['test']['fscore_thresholds'],
+            is_eval_explicit_mesh=is_eval_explicit_mesh,
+            vertex_visibility=vertex_visibility)
+        if eval_dict_mesh is not None:
+            for k, v in eval_dict_mesh.items():
+                eval_dict[k + ' (mesh)'] = v
 
 # Create pandas dataframe and save
 eval_df = pd.DataFrame(eval_dicts)
diff --git a/eval_meshes.py b/eval_meshes.py
index 44e5d88..2289e02 100644
--- a/eval_meshes.py
+++ b/eval_meshes.py
@@ -10,6 +10,10 @@ from im2mesh import config, data
 from im2mesh.eval import MeshEvaluator
 from im2mesh.utils.io import load_pointcloud
 import numpy as np
+from datetime import datetime
+import yaml
+import eval_utils
+
 parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
@@ -17,31 +21,42 @@ parser.add_argument('--eval_input',
                     action='store_true',
                     help='Evaluate inputs instead.')
 
-args = parser.parse_args()
+parser.add_argument('--unique_name',
+                    default='',
+                    type=str,
+                    help='String name for generation.')
+args, unknown_args = parser.parse_known_args()
 cfg = config.load_config(args.config, 'configs/default.yaml')
+eval_utils.update_dict_with_options(cfg, unknown_args)
+
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
 is_eval_explicit_mesh = cfg['test'].get('is_eval_explicit_mesh', False)
-
 # Shorthands
+
 out_dir = os.path.dirname(args.config)
-#out_dir = cfg['training']['out_dir']
-generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
+generation_dir = os.path.dirname(args.config)
+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
+assert generation_dir.endswith(cfg['generation']['generation_dir'])
 if not args.eval_input:
     out_file = os.path.join(
-        generation_dir, 'eval_meshes_full{}.pkl'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_meshes_full_{}_{}_{}.pkl'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
     out_file_class = os.path.join(
-        generation_dir, 'eval_meshes{}.csv'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_meshes_{}_{}_{}.csv'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
 else:
     out_file = os.path.join(
-        generation_dir, 'eval_input_full{}.pkl'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_input_full_{}_{}_{}.pkl'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
     out_file_class = os.path.join(
-        generation_dir, 'eval_input{}.csv'.format(
-            '_explicit' if is_eval_explicit_mesh else ''))
+        generation_dir, 'eval_input_{}_{}_{}.csv'.format(
+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
+            date_str))
 
 # Dataset
 points_field = data.PointsField(
@@ -66,9 +81,19 @@ dataset = data.Shapes3dDataset(dataset_folder,
 if 'debug' in cfg['data']:
     dataset = torch_data.Subset(dataset,
                                 range(cfg['data']['debug']['sample_n']))
+yaml.dump(
+    cfg,
+    open(
+        os.path.join(
+            generation_dir,
+            'eval_mesh_config_{}_{}.yaml'.format(args.unique_name, date_str)),
+        'w'))
 
 # Evaluator
-evaluator = MeshEvaluator(n_points=100000)
+evaluator = MeshEvaluator(
+    n_points=cfg['test']['n_points'],
+    is_sample_from_surface=cfg['test']['is_sample_from_surface'],
+    is_normalize_by_side_length=cfg['test']['is_normalize_by_side_length'])
 
 # Loader
 test_loader = torch.utils.data.DataLoader(dataset,
@@ -130,30 +155,73 @@ for it, data in enumerate(tqdm(test_loader)):
 
     # Evaluate mesh
     if cfg['test']['eval_mesh']:
-        mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
 
-        if is_eval_explicit_mesh:
-            visbility_file = os.path.join(
-                mesh_dir, '%s_vertex_visbility.npz' % modelname)
-        if os.path.exists(mesh_file):
-            mesh = trimesh.load(mesh_file, process=False)
+        vertex_visibility = None
+        if cfg['method'] == 'pnet':
+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
+
+            if os.path.exists(mesh_file):
+                mesh = trimesh.load(mesh_file, process=False)
+            else:
+                print('Warning: mesh file does not exist: %s' % mesh_file)
+                continue
             if is_eval_explicit_mesh:
-                vertex_visibility = np.load(
-                    visbility_file)['vertex_visibility']
+                visbility_file = os.path.join(
+                    mesh_dir, '%s_vertex_visbility.npz' % modelname)
+                if os.path.exists(visbility_file):
+                    vertex_visibility = np.load(
+                        visbility_file)['vertex_visibility']
+                else:
+                    print('Warning: vibility file does not exist: %s' %
+                          visbility_file)
+                    continue
+        elif cfg['method'] == 'bspnet':
+            vertex_file = os.path.join(mesh_dir,
+                                       '%s_vertex_attributes.npz' % modelname)
+            is_eval_explicit_mesh = True
+            if os.path.exists(vertex_file):
+                try:
+                    vertex_attributes = np.load(vertex_file)
+                except:
+                    print('Error in bspnet loading vertex')
+                    continue
+                mesh = trimesh.Trimesh(
+                    vertex_attributes['vertices'],
+                    vertex_normals=vertex_attributes['normals'])
+                vertex_visibility = vertex_attributes['vertex_visibility']
+            else:
+                print('Warning: vertex file does not exist: %s' %
+                      visbility_file)
+                continue
+        elif cfg['method'] == 'atlasnetv2':
+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
+            is_eval_explicit_mesh = False
+
+            if os.path.exists(mesh_file):
+                mesh = trimesh.load(mesh_file, process=False)
             else:
-                vertex_visibility = None
-            eval_dict_mesh = evaluator.eval_mesh(
-                mesh,
-                pointcloud_tgt,
-                normals_tgt,
-                points_tgt,
-                occ_tgt,
-                is_eval_explicit_mesh=is_eval_explicit_mesh,
-                vertex_visibility=vertex_visibility)
-            for k, v in eval_dict_mesh.items():
-                eval_dict[k + ' (mesh)'] = v
+                print('Warning: mesh file does not exist: %s' % mesh_file)
+                continue
         else:
-            print('Warning: mesh does not exist: %s' % mesh_file)
+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
+
+            if os.path.exists(mesh_file):
+                mesh = trimesh.load(mesh_file, process=False)
+            else:
+                print('Warning: mesh file does not exist: %s' % mesh_file)
+                continue
+
+        eval_dict_mesh = evaluator.eval_mesh(
+            mesh,
+            pointcloud_tgt,
+            normals_tgt,
+            points_tgt,
+            occ_tgt,
+            is_eval_explicit_mesh=is_eval_explicit_mesh,
+            vertex_visibility=vertex_visibility,
+            skip_iou=(cfg['method'] == 'atlasnetv2'))
+        for k, v in eval_dict_mesh.items():
+            eval_dict[k + ' (mesh)'] = v
 
     # Evaluate point cloud
     if cfg['test']['eval_pointcloud']:
diff --git a/external/atlasnetv2/atlasnetv2 b/external/atlasnetv2/atlasnetv2
--- a/external/atlasnetv2/atlasnetv2
+++ b/external/atlasnetv2/atlasnetv2
@@ -1 +1 @@
-Subproject commit 09739b6328f969a6de084376696a5b57890494e8
+Subproject commit 09739b6328f969a6de084376696a5b57890494e8-dirty
diff --git a/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py b/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py
index bf2958a..6eb8fc2 100644
--- a/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py
+++ b/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py
@@ -79,6 +79,5 @@ class PrimitiveWiseLinear(nn.Module):
         """
 
         B, N, D, P = input_data.shape
-        return self.main(input_data.view(B, N * D,
-                                         P)).view(B, N, self.output_channels,
-                                                  P)
+        out = self.main(input_data.view(B, N * D, P))
+        return out.view(B, N, self.output_channels, P)
diff --git a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
index 89672aa..a17e6c0 100644
--- a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
+++ b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
@@ -14,6 +14,7 @@ class BaseShapeSampler(nn.Module):
                  learn_pose=True,
                  linear_scaling=True,
                  disable_learn_pose_but_transition=False,
+                 extract_surface_point_by_max=False,
                  dim=2):
         """Intitialize SuperShapeSampler.
 
@@ -28,6 +29,7 @@ class BaseShapeSampler(nn.Module):
         self.learn_pose = learn_pose
         self.linear_scaling = linear_scaling
         self.disable_learn_pose_but_transition = disable_learn_pose_but_transition
+        self.extract_surface_point_by_max = extract_surface_point_by_max
 
         self.dim = dim
         if not self.dim in [2, 3]:
@@ -276,9 +278,14 @@ class BaseShapeSampler(nn.Module):
         P = NP // N
         output_sgn = output_sgn_BxNxNP.view(B, self.n_primitives,
                                             self.n_primitives, P)
-        sgn_p_BxPsN = nn.functional.relu(output_sgn).sum(1).view(
-            B, self.n_primitives, P)
-        surface_mask = (sgn_p_BxPsN <= 1e-1)
+        if self.extract_surface_point_by_max:
+            sgn_p_BxPsN = torch.relu(output_sgn.max(1)[0]).view(
+                B, self.n_primitives, P)
+            surface_mask = (sgn_p_BxPsN <= 1e-4)
+        else:
+            sgn_p_BxPsN = nn.functional.relu(output_sgn).sum(1).view(
+                B, self.n_primitives, P)
+            surface_mask = (sgn_p_BxPsN <= 1e-1)
         return surface_mask
 
     def extract_surface_point_std(self, super_shape_point, primitive_params,
diff --git a/external/periodic_shapes/periodic_shapes/models/decoder.py b/external/periodic_shapes/periodic_shapes/models/decoder.py
index a1dc597..028645d 100644
--- a/external/periodic_shapes/periodic_shapes/models/decoder.py
+++ b/external/periodic_shapes/periodic_shapes/models/decoder.py
@@ -118,7 +118,6 @@ class ShapeDecoderCBatchNorm(nn.Module):
                           P).transpose(2, 3).contiguous()
 
         # B, n_primitives, P, self.label_num = 1
-        print('radius', radius.mean().item())
         return radius
 
 
diff --git a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
index 084e5da..ccf971e 100644
--- a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
+++ b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
@@ -26,12 +26,14 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
                  is_feature_radius=True,
                  no_last_bias=False,
                  return_sdf=False,
+                 is_infer_r1r2=False,
                  **kwargs):
         super().__init__(*args, **kwargs)
         self.clamp = True
         self.factor = factor
         self.num_points = num_points
-        self.num_labels = 1  # Only infer r2 for 3D
+        self.num_labels = (1 if not is_infer_r1r2 or self.dim == 2 else 2
+                           )  # Only infer r2 for 3D
         self.theta_dim = 2 if self.dim == 2 else 4
         self.last_scale = last_scale
 
@@ -40,6 +42,9 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
         self.is_shape_sampler_sphere = is_shape_sampler_sphere
         self.spherical_angles = spherical_angles
         self.return_sdf = return_sdf
+        self.is_infer_r1r2 = is_infer_r1r2
+        if self.is_infer_r1r2:
+            assert not self.is_shape_sampler_sphere and not self.spherical_angles
 
         c64 = 64 // self.factor
         self.encoder_dim = c64 * 2
@@ -101,6 +106,9 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
             #print('mean r1 in points', r[..., 0].mean())
             final_r[..., 0] = r[..., 0] + periodic_net_r.squeeze(-1)
             #print('mean final r in points', final_r[..., 0].mean())
+
+        elif self.is_infer_r1r2:
+            final_r = r + periodic_net_r
         else:
             #print('mean r1 in points', r[..., -1].mean())
             final_r[..., -1] = r[..., -1] + periodic_net_r.squeeze(-1)
@@ -110,11 +118,11 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
             final_r = final_r.clamp(min=EPS)
         else:
             final_r = torch.relu(final_r) + EPS
-        print('r in ss stats',
-              final_r.mean().item(),
-              final_r.max().item(),
-              final_r.min().item(),
-              final_r.std().item())
+        #print('r in ss stats',
+        #      final_r.mean().item(),
+        #      final_r.max().item(),
+        #      final_r.min().item(),
+        #      final_r.std().item())
 
         # B, n_primitives, P, dim
         if self.is_shape_sampler_sphere and self.spherical_angles:
@@ -221,11 +229,21 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
 
         else:
             if is3d:
-                r2 = r2 + rp.squeeze(-1)
-                if self.clamp:
-                    r2 = r2.clamp(min=EPS)
+                if self.is_infer_r1r2:
+                    r1 = r1 + rp[..., 0]
+                    r2 = r2 + rp[..., -1]
+                    if self.clamp:
+                        r1 = r1.clamp(min=EPS)
+                        r2 = r2.clamp(min=EPS)
+                    else:
+                        r1 = nn.functional.relu(r1) + EPS
+                        r2 = nn.functional.relu(r2) + EPS
                 else:
-                    r2 = nn.functional.relu(r2) + EPS
+                    r2 = r2 + rp.squeeze(-1)
+                    if self.clamp:
+                        r2 = r2.clamp(min=EPS)
+                    else:
+                        r2 = nn.functional.relu(r2) + EPS
             else:
                 r1 = r1 + rp.squeeze(-1)
                 if self.clamp:
diff --git a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
index 4a46eae..8affe18 100644
--- a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
+++ b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
@@ -782,11 +782,12 @@ def test_decoder_consistency_OtherDecoders():
         #decoder_class='PrimitiveWiseGroupConvDecoderLegacy',
         last_scale=10,
         no_encoder=True,
-        is_shape_sampler_sphere=True,
-        spherical_angles=True,
-        is_feature_coord=True,
-        is_feature_angles=False,
+        is_shape_sampler_sphere=False,
+        spherical_angles=False,
+        is_feature_coord=False,
+        is_feature_angles=True,
         is_feature_radius=False,
+        is_infer_r1r2=False,
         dim=dim)
     preset_params = utils.generate_multiple_primitive_params(
         m,
diff --git a/generate.py b/generate.py
index bbf4e8d..9866c3c 100644
--- a/generate.py
+++ b/generate.py
@@ -1,6 +1,8 @@
-import torch
 # import torch.distributions as dist
 import os
+os.environ['CUDA_PATH'] = '/usr/local/cuda-10.0'
+
+import torch
 import shutil
 import argparse
 from tqdm import tqdm
@@ -18,6 +20,7 @@ import subprocess
 import yaml
 from datetime import datetime
 import subprocess
+import eval_utils
 
 
 def represent_odict(dumper, instance):
@@ -45,69 +48,66 @@ parser.add_argument('--unique_name',
                     default='',
                     type=str,
                     help='String name for generation.')
+parser.add_argument('--resume_generation_dir',
+                    default=None,
+                    type=str,
+                    help='String name for generation.')
 
 args, unknown_args = parser.parse_known_args()
+if args.resume_generation_dir is not None:
+    assert os.path.isabs(args.resume_generation_dir)
 
 cfg = config.load_config(args.config, 'configs/default.yaml')
 
-for idx, arg in enumerate(unknown_args):
-    if arg.startswith('--'):
-        arg = arg.replace('--', '')
-        value = unknown_args[idx + 1]
-        keys = arg.split('.')
-        if keys[0] not in cfg:
-            cfg[keys[0]] = {}
-        child_cfg = cfg.get(keys[0], {})
-        for key in keys[1:]:
-            item = child_cfg.get(key, None)
-            if isinstance(item, dict):
-                child_cfg = item
-            elif item is None:
-                if value == 'true':
-                    value = True
-                if value == 'false':
-                    value = False
-                if value == 'null':
-                    value = None
-                if isinstance(value, str) and value.isdigit():
-                    value = float(value)
-                child_cfg[key] = value
-            else:
-                child_cfg[key] = type(item)(value)
+eval_utils.update_dict_with_options(cfg, unknown_args)
+
 date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
 if args.explicit:
-    assert cfg['data'].get('is_normal_icosahedron', False) or cfg['data'].get(
-        'is_normal_uv_sphere', False)
+    if cfg['method'] == 'pnet':
+        assert cfg['data'].get('is_normal_icosahedron',
+                               False) or cfg['data'].get(
+                                   'is_normal_uv_sphere', False)
+    elif cfg['method'] == 'atlasnetv2':
+        assert cfg['data'].get('is_generate_mesh', False)
+
     cfg['generation']['is_explicit_mesh'] = True
     cfg['test']['is_eval_explicit_mesh'] = True
-if args.explicit:
     cfg['generation']['generation_dir'] += '_explicit'
-cfg['generation']['generation_dir'] += ('_' + date_str)
+
+cfg['generation']['generation_dir'] += ('_' + args.unique_name + '_' +
+                                        date_str)
 
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
-out_dir = os.path.dirname(args.config)
+if args.resume_generation_dir is None:
+    out_dir = os.path.dirname(args.config)
+    generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
+else:
+    out_dir = os.path.dirname(args.resume_generation_dir)
+    generation_dir = args.resume_generation_dir
 
-generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
-if not os.path.exists(generation_dir):
+if not os.path.exists(generation_dir) and args.resume_generation_dir is None:
     os.makedirs(generation_dir)
-out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')
-out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')
 
-patch_path = os.path.join(generation_dir, 'gen_diff.patch')
-subprocess.run('git diff > {}'.format(patch_path), shell=True)
-weight_path = os.path.join(out_dir, cfg['test']['model_file'])
-with open(weight_path, 'rb') as f:
-    md5 = hashlib.md5(f.read()).hexdigest()
-cfg['test']['model_file_hash'] = md5
-yaml.dump(
-    cfg,
-    open(
-        os.path.join(
-            out_dir, 'gen_config_{}_{}.yaml'.format(args.unique_name,
-                                                    date_str)), 'w'))
+if args.resume_generation_dir is None:
+    patch_path = os.path.join(generation_dir, 'gen_diff.patch')
+    subprocess.run('git diff > {}'.format(patch_path), shell=True)
+    if not cfg['test']['model_file'].startswith('http'):
+        weight_path = os.path.join(out_dir, cfg['test']['model_file'])
+        with open(weight_path, 'rb') as f:
+            md5 = hashlib.md5(f.read()).hexdigest()
+        cfg['test']['model_file_hash'] = md5
+    yaml.dump(
+        cfg,
+        open(
+            os.path.join(
+                generation_dir,
+                'gen_config_{}_{}.yaml'.format(args.unique_name, date_str)),
+            'w'))
 
+out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')
+out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')
 batch_size = cfg['generation']['batch_size']
 input_type = cfg['data']['input_type']
 vis_n_outputs = cfg['generation']['vis_n_outputs']
@@ -121,7 +121,7 @@ dataset = config.get_dataset('test', cfg, return_idx=True)
 model = config.get_model(cfg, device=device, dataset=dataset)
 
 checkpoint_io = CheckpointIO(out_dir, model=model)
-checkpoint_io.load(cfg['test']['model_file'])
+checkpoint_io.load(cfg['test']['model_file'], device=device)
 
 # Generator
 generator = config.get_generator(model, cfg, device=device)
@@ -222,28 +222,76 @@ for it, data in enumerate(tqdm(test_loader)):
         out_file_dict['gt'] = modelpath
 
     if generate_mesh:
+        # Checkfile exists
+        is_input_file_exists = False
+        if input_type == 'img':
+            inputs_path = os.path.join(in_dir, '%s.jpg' % modelname)
+        elif input_type == 'voxels':
+            inputs_path = os.path.join(in_dir, '%s.off' % modelname)
+        elif input_type == 'pointcloud':
+            inputs_path = os.path.join(in_dir, '%s.ply' % modelname)
+        if os.path.exists(input_type):
+            is_input_file_exists = True
+
+        # Write output
+        is_mesh_file_exists = False
+        mesh_out_file = os.path.join(mesh_dir, '%s.off' % modelname)
+        if os.path.exists(mesh_out_file):
+            is_mesh_file_exists = True
+
+        is_vertex_attribute_file_exists = False
+        visibility_out_file = ''
+        if cfg['generation'].get('is_explicit_mesh',
+                                 False) and cfg['method'] == 'pnet':
+            visibility_out_file = os.path.join(
+                mesh_dir, '%s_vertex_visbility.npz' % modelname)
+        elif cfg['method'] == 'bspnet':
+            visibility_out_file = os.path.join(
+                mesh_dir, '%s_vertex_attributes.npz' % modelname)
+        if os.path.exists(visibility_out_file):
+            is_vertex_attribute_file_exists = True
+
+        if is_input_file_exists and is_mesh_file_exists and is_vertex_attribute_file_exists and args.resume_generation_dir is not None:
+            print('pass', category_id, modelname)
+            continue
+
         t0 = time.time()
         out = generator.generate_mesh(data)
         time_dict['mesh'] = time.time() - t0
 
         # Get statistics
-        try:
-            mesh, stats_dict = out
-        except TypeError:
-            mesh, stats_dict = out, {}
+        if cfg['method'] == 'bspnet':
+            try:
+                mesh, stats_dict, vertices, normals, visibility = out
+            except TypeError:
+                mesh, vertices, normals, visibility = out
+                stats_dict = {}
+            if mesh is None:
+                continue
+        else:
+            try:
+                mesh, stats_dict = out
+            except TypeError:
+                mesh, stats_dict = out, {}
         time_dict.update(stats_dict)
 
         # Write output
-        mesh_out_file = os.path.join(mesh_dir, '%s.off' % modelname)
         mesh.export(mesh_out_file)
         out_file_dict['mesh'] = mesh_out_file
-        if cfg['generation'].get('is_explicit_mesh', False):
+        if cfg['generation'].get('is_explicit_mesh',
+                                 False) and cfg['method'] == 'pnet':
             visibility = mesh.vertex_attributes['vertex_visibility']
-            visibility_out_file = os.path.join(
-                mesh_dir, '%s_vertex_visbility.npz' % modelname)
+
             np.savez(visibility_out_file, vertex_visibility=visibility)
             out_file_dict['vertex_visibility'] = visibility_out_file
 
+        elif cfg['method'] == 'bspnet':
+            np.savez(visibility_out_file,
+                     vertex_visibility=visibility,
+                     vertices=vertices,
+                     normals=normals)
+            out_file_dict['vertex_attributes'] = visibility_out_file
+
     if generate_pointcloud:
         t0 = time.time()
         pointcloud = generator.generate_pointcloud(data)
@@ -257,7 +305,10 @@ for it, data in enumerate(tqdm(test_loader)):
         # Save inputs
         if input_type == 'img':
             inputs_path = os.path.join(in_dir, '%s.jpg' % modelname)
-            inputs = data['inputs'].squeeze(0).cpu()
+            if cfg['method'] == 'bspnet':
+                inputs = data['inputs'].squeeze(0).expand(3, -1, -1).cpu()
+            else:
+                inputs = data['inputs'].squeeze(0).cpu()
             visualize_data(inputs, 'img', inputs_path)
             out_file_dict['in'] = inputs_path
         elif input_type == 'voxels':
diff --git a/im2mesh/atlasnetv2/config.py b/im2mesh/atlasnetv2/config.py
index cea82f8..eee4061 100644
--- a/im2mesh/atlasnetv2/config.py
+++ b/im2mesh/atlasnetv2/config.py
@@ -6,6 +6,7 @@ from im2mesh.encoder import encoder_dict
 from im2mesh.atlasnetv2 import models, training, generation
 from im2mesh import data
 from im2mesh import config
+from atlasnetv2.auxiliary.utils import weights_init
 
 
 def get_model(cfg, device=None, dataset=None, **kwargs):
@@ -50,7 +51,8 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
                               encoder_latent,
                               p0_z,
                               device=device)
-
+    if cfg['data']['input_type'] == 'pointcloud':
+        model.apply(weights_init)
     return model
 
 
@@ -68,15 +70,15 @@ def get_trainer(model, optimizer, cfg, device, **kwargs):
     vis_dir = os.path.join(out_dir, 'vis')
     input_type = cfg['data']['input_type']
 
-    trainer = training.Trainer(
-        model,
-        optimizer,
-        device=device,
-        input_type=input_type,
-        vis_dir=vis_dir,
-        threshold=threshold,
-        eval_sample=cfg['training']['eval_sample'],
-    )
+    trainer = training.Trainer(model,
+                               optimizer,
+                               device=device,
+                               input_type=input_type,
+                               vis_dir=vis_dir,
+                               threshold=threshold,
+                               eval_sample=cfg['training']['eval_sample'],
+                               debugged=cfg['training'].get('debugged', False),
+                               **cfg['trainer'])
 
     return trainer
 
@@ -101,7 +103,8 @@ def get_generator(model, cfg, device, **kwargs):
         refinement_step=cfg['generation']['refinement_step'],
         simplify_nfaces=cfg['generation']['simplify_nfaces'],
         preprocessor=preprocessor,
-    )
+        debugged=cfg['training'].get('debugged', False),
+        point_scale=cfg['trainer']['point_scale'])
     return generator
 
 
diff --git a/im2mesh/atlasnetv2/generation.py b/im2mesh/atlasnetv2/generation.py
index 49786de..422aae4 100644
--- a/im2mesh/atlasnetv2/generation.py
+++ b/im2mesh/atlasnetv2/generation.py
@@ -8,6 +8,7 @@ from im2mesh.utils import libmcubes
 from im2mesh.common import make_3d_grid
 from im2mesh.utils.libsimplify import simplify_mesh
 from im2mesh.utils.libmise import MISE
+from periodic_shapes.models import model_utils
 import time
 
 
@@ -30,13 +31,22 @@ class Generator3D(object):
         simplify_nfaces (int): number of faces the mesh should be simplified to
         preprocessor (nn.Module): preprocessor for inputs
     '''
-
-    def __init__(self, model, points_batch_size=100000,
-                 threshold=0.5, refinement_step=0, device=None,
-                 resolution0=16, upsampling_steps=3,
-                 with_normals=False, padding=0.1, sample=False,
+    def __init__(self,
+                 model,
+                 points_batch_size=100000,
+                 threshold=0.5,
+                 refinement_step=0,
+                 device=None,
+                 resolution0=16,
+                 upsampling_steps=3,
+                 with_normals=False,
+                 padding=0.1,
+                 sample=False,
                  simplify_nfaces=None,
-                 preprocessor=None):
+                 preprocessor=None,
+                 point_scale=1,
+                 debugged=False,
+                 **kwargs):
         self.model = model.to(device)
         self.points_batch_size = points_batch_size
         self.refinement_step = refinement_step
@@ -49,6 +59,8 @@ class Generator3D(object):
         self.sample = sample
         self.simplify_nfaces = simplify_nfaces
         self.preprocessor = preprocessor
+        self.point_scale = point_scale
+        self.debugged = debugged
 
     def generate_mesh(self, data, return_stats=True):
         ''' Generates the output mesh.
@@ -61,7 +73,8 @@ class Generator3D(object):
         device = self.device
         stats_dict = {}
 
-        inputs = data.get('inputs', torch.empty(1, 0)).to(device)
+        inputs = data.get('inputs', torch.empty(
+            1, 0)).to(device) * (1 if self.debugged else self.point_scale)
         kwargs = {}
 
         # Preprocess if requires
@@ -77,15 +90,24 @@ class Generator3D(object):
             c = self.model.encode_inputs(inputs)
         stats_dict['time (encode inputs)'] = time.time() - t0
 
-        z = self.model.get_z_from_prior((1,), sample=self.sample).to(device)
-        mesh = self.generate_from_latent(z, c, stats_dict=stats_dict, **kwargs)
+        z = self.model.get_z_from_prior((1, ), sample=self.sample).to(device)
+        mesh = self.generate_from_latent(z,
+                                         c,
+                                         stats_dict=stats_dict,
+                                         data=data,
+                                         **kwargs)
 
         if return_stats:
             return mesh, stats_dict
         else:
             return mesh
 
-    def generate_from_latent(self, z, c=None, stats_dict={}, **kwargs):
+    def generate_from_latent(self,
+                             z,
+                             c=None,
+                             stats_dict={},
+                             data=None,
+                             **kwargs):
         ''' Generates mesh from latent.
 
         Args:
@@ -93,48 +115,31 @@ class Generator3D(object):
             c (tensor): latent conditioned code c
             stats_dict (dict): stats dictionary
         '''
-        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
+        assert data is not None
 
+        faces = data.get('patch.mesh_faces').to(self.device)
+        vertices = data.get('patch.mesh_vertices').to(self.device)
+        patch = data.get('patch').to(self.device)
         t0 = time.time()
-        # Compute bounding box size
-        box_size = 1 + self.padding
-
-        # Shortcut
-        if self.upsampling_steps == 0:
-            nx = self.resolution0
-            pointsf = box_size * make_3d_grid(
-                (-0.5,)*3, (0.5,)*3, (nx,)*3
-            )
-            values = self.eval_points(pointsf, z, c, **kwargs).cpu().numpy()
-            value_grid = values.reshape(nx, nx, nx)
-        else:
-            mesh_extractor = MISE(
-                self.resolution0, self.upsampling_steps, threshold)
-
-            points = mesh_extractor.query()
-
-            while points.shape[0] != 0:
-                # Query points
-                pointsf = torch.FloatTensor(points).to(self.device)
-                # Normalize to bounding box
-                pointsf = pointsf / mesh_extractor.resolution
-                pointsf = box_size * (pointsf - 0.5)
-                # Evaluate model and update
-                values = self.eval_points(
-                    pointsf, z, c, **kwargs).cpu().numpy()
-                values = values.astype(np.float64)
-                mesh_extractor.update(points, values)
-                points = mesh_extractor.query()
-
-            value_grid = mesh_extractor.to_dense()
-
-        # Extract mesh
+        predicted_vertices = self.model.decode(
+            None, z, c, grid=patch, **kwargs) / self.point_scale
         stats_dict['time (eval points)'] = time.time() - t0
 
-        mesh = self.extract_mesh(value_grid, z, c, stats_dict=stats_dict)
+        t0 = time.time()
+        B, N, P, D = predicted_vertices.shape
+        faces_all = torch.cat([(faces + idx * P) for idx in range(N)], axis=1)
+        assert B == 1
+        mem_t = time.time()
+        npverts = predicted_vertices.view(N * P, D).to('cpu').detach().numpy()
+        npfaces = faces_all.view(-1, 3).to('cpu').detach().numpy()
+        skip_t = time.time() - mem_t
+
+        mesh = trimesh.Trimesh(npverts, npfaces, process=False)
+        stats_dict['time (copy to trimesh)'] = time.time() - t0 - skip_t
+
         return mesh
 
-    def eval_points(self, p, z, c=None, **kwargs):
+    def eval_points(self, p, z, c=None, data=None, **kwargs):
         ''' Evaluates the occupancy values for the points.
 
         Args:
@@ -142,13 +147,26 @@ class Generator3D(object):
             z (tensor): latent code z
             c (tensor): latent conditioned code c
         '''
+        assert data is not None
         p_split = torch.split(p, self.points_batch_size)
+
+        angles = data.get('angles').to(self.device)
         occ_hats = []
 
         for pi in p_split:
             pi = pi.unsqueeze(0).to(self.device)
+            an = angles.to(self.device)
             with torch.no_grad():
-                occ_hat = self.model.decode(pi, z, c, **kwargs).logits
+                #_, _, sgn, _ = self.model.decode(pi, z, c, **kwargs).logits
+                _, _, sgn, _, _ = self.model.decode(pi * self.pnet_point_scale,
+                                                    z,
+                                                    c,
+                                                    angles=an,
+                                                    **kwargs)
+
+                occ_hat = (
+                    model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1) >
+                    self.threshold).float()
 
             occ_hats.append(occ_hat.squeeze(0).detach().cpu())
 
@@ -168,20 +186,20 @@ class Generator3D(object):
         # Some short hands
         n_x, n_y, n_z = occ_hat.shape
         box_size = 1 + self.padding
-        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
+        threshold = 0.
+        #threshold = np.log(self.threshold) - np.log(1. - self.threshold)
         # Make sure that mesh is watertight
         t0 = time.time()
-        occ_hat_padded = np.pad(
-            occ_hat, 1, 'constant', constant_values=-1e6)
-        vertices, triangles = libmcubes.marching_cubes(
-            occ_hat_padded, threshold)
+        occ_hat_padded = np.pad(occ_hat, 1, 'constant', constant_values=-1e6)
+        vertices, triangles = libmcubes.marching_cubes(occ_hat_padded,
+                                                       threshold)
         stats_dict['time (marching cubes)'] = time.time() - t0
         # Strange behaviour in libmcubes: vertices are shifted by 0.5
         vertices -= 0.5
         # Undo padding
         vertices -= 1
         # Normalize to bounding box
-        vertices /= np.array([n_x-1, n_y-1, n_z-1])
+        vertices /= np.array([n_x - 1, n_y - 1, n_z - 1])
         vertices = box_size * (vertices - 0.5)
 
         # mesh_pymesh = pymesh.form_mesh(vertices, triangles)
@@ -197,7 +215,8 @@ class Generator3D(object):
             normals = None
 
         # Create mesh
-        mesh = trimesh.Trimesh(vertices, triangles,
+        mesh = trimesh.Trimesh(vertices,
+                               triangles,
                                vertex_normals=normals,
                                process=False)
 
@@ -261,7 +280,7 @@ class Generator3D(object):
 
         # Some shorthands
         n_x, n_y, n_z = occ_hat.shape
-        assert(n_x == n_y == n_z)
+        assert (n_x == n_y == n_z)
         # threshold = np.log(self.threshold) - np.log(1. - self.threshold)
         threshold = self.threshold
 
@@ -290,10 +309,9 @@ class Generator3D(object):
             face_normal = face_normal / \
                 (face_normal.norm(dim=1, keepdim=True) + 1e-10)
             face_value = torch.sigmoid(
-                self.model.decode(face_point.unsqueeze(0), z, c).logits
-            )
-            normal_target = -autograd.grad(
-                [face_value.sum()], [face_point], create_graph=True)[0]
+                self.model.decode(face_point.unsqueeze(0), z, c).logits)
+            normal_target = -autograd.grad([face_value.sum()], [face_point],
+                                           create_graph=True)[0]
 
             normal_target = \
                 normal_target / \
diff --git a/im2mesh/atlasnetv2/models/decoder.py b/im2mesh/atlasnetv2/models/decoder.py
index e35f724..0aeefc6 100644
--- a/im2mesh/atlasnetv2/models/decoder.py
+++ b/im2mesh/atlasnetv2/models/decoder.py
@@ -5,7 +5,8 @@ from im2mesh.layers import (ResnetBlockFC, CResnetBlockConv1d, CBatchNorm1d,
                             CBatchNorm1d_legacy, ResnetBlockConv1d)
 
 from atlasnetv2.auxiliary.model import mlpAdj, patchDeformationMLP
-from atlasnetv2.auxiliary.utils import weights_init
+
+from periodic_shapes.layers import primitive_wise_layers
 
 
 class AtlasNetV2Decoder(nn.Module):
@@ -32,18 +33,16 @@ class AtlasNetV2Decoder(nn.Module):
         self.c_dim = c_dim
         decoder_kwargs['nlatent'] = hidden_size
         self.options = type('', (), decoder_kwargs)
+        #self.decoder = PatchDeformGroupWiseMLPAdjInOcc(self.options)
         self.decoder = PatchDeformMLPAdjInOcc(self.options)
-        self.decoder.apply(weights_init)
 
     def forward(self, p, z, color_feature, grid=None, **kwargs):
         # grid (B, P, dim) -> (B, dim, P)
         transposed_grid = grid[:, 0, :, :].transpose(-1, -2)
         out, _ = self.decoder(color_feature, transposed_grid)
-
         #
         B = transposed_grid.shape[0]
-        out = out.transpose(-1, -2).contiguous().view(B, self.options.npatch,
-                                                      -1, 3)
+        out = out.view(B, self.options.npatch, -1, 3)
         return out
 
 
@@ -104,3 +103,134 @@ class PatchDeformMLPAdjInOcc(nn.Module):
             #==========================================================================
 
         return torch.cat(outs, 2).transpose(2, 1).contiguous(), patches
+
+
+class PatchDeformGroupWiseMLPAdjInOcc(nn.Module):
+    """Atlas net auto encoder"""
+    def __init__(self, options):
+
+        super(PatchDeformGroupWiseMLPAdjInOcc, self).__init__()
+
+        self.npatch = options.npatch
+        self.nlatent = options.nlatent
+        self.patchDim = options.patchDim
+        assert self.patchDim == 2
+        self.patchDeformDim = options.patchDeformDim
+
+        #encoder decoder and patch deformation module
+        #==============================================================================
+        self.decoder = GroupWisemlpAdj(nlatent=self.patchDeformDim +
+                                       self.nlatent,
+                                       npatch=self.npatch)
+        self.patchDeformation = patchDeformationGroupWiseMLP(
+            patchDim=self.patchDim,
+            patchDeformDim=self.patchDeformDim,
+            npatch=self.npatch)
+        #==============================================================================
+
+    def forward(self, x, grid):
+
+        #encoder
+        #==============================================================================
+        #x = self.encoder(x.transpose(2, 1).contiguous())
+        #==============================================================================
+
+        # B, P, dims
+        grid[:, 2:, :] = 0
+        # B, N, dims, P
+        rand_grid1 = grid.unsqueeze(1).expand(-1, self.npatch, -1,
+                                              -1).contiguous()
+
+        #random planar patch
+        #==========================================================================
+        rand_grid2 = self.patchDeformation(rand_grid1)
+        #==========================================================================
+
+        #cat with latent vector and decode
+        #==========================================================================
+        # B, nlatent -> B, N, nlatent, P
+        y1 = x.view(x.size(0), 1, x.size(1), 1).expand(-1, self.npatch, -1,
+                                                       rand_grid2.size(3))
+        y2 = torch.cat([y1, rand_grid2], axis=2).contiguous()
+
+        # B, N, defromdim, P
+        out = self.decoder(y2)
+        #==========================================================================
+
+        #B, N, P, deformdim
+        return out.transpose(3, 2).contiguous(), None
+
+
+class patchDeformationGroupWiseMLP(nn.Module):
+    """deformation of a 2D patch into a 3D surface"""
+    def __init__(self, patchDim=2, patchDeformDim=3, npatch=16, tanh=True):
+
+        super(patchDeformationGroupWiseMLP, self).__init__()
+        layer_size = 128
+        self.tanh = tanh
+        #self.conv1 = torch.nn.Conv1d(patchDim, layer_size, 1)
+        #self.conv2 = torch.nn.Conv1d(layer_size, layer_size, 1)
+        #self.conv3 = torch.nn.Conv1d(layer_size, patchDeformDim, 1)
+        self.bn1 = torch.nn.BatchNorm1d(layer_size * npatch)
+        self.bn2 = torch.nn.BatchNorm1d(layer_size * npatch)
+
+        self.conv1 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
+                                                               patchDim,
+                                                               layer_size,
+                                                               act='none')
+        self.conv2 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
+                                                               layer_size,
+                                                               layer_size,
+                                                               act='none')
+        self.conv3 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
+                                                               layer_size,
+                                                               patchDeformDim,
+                                                               act='none')
+        self.th = nn.Tanh()
+
+    def forward(self, x):
+        B, N, _, P = x.shape
+        x = F.relu(self.bn1(self.conv1(x).view(B, -1, P))).view(B, N, -1, P)
+        x = F.relu(self.bn2(self.conv2(x).view(B, -1, P))).view(B, N, -1, P)
+        if self.tanh:
+            x = self.th(self.conv3(x))
+        else:
+            x = self.conv3(x)
+        return x
+
+
+class GroupWisemlpAdj(nn.Module):
+    def __init__(self, nlatent=1024, npatch=16):
+        """Atlas decoder"""
+
+        super(GroupWisemlpAdj, self).__init__()
+        self.nlatent = nlatent
+        self.conv1 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
+                                                               self.nlatent,
+                                                               self.nlatent,
+                                                               act='none')
+        self.conv2 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
+                                                               self.nlatent,
+                                                               self.nlatent //
+                                                               2,
+                                                               act='none')
+        self.conv3 = primitive_wise_layers.PrimitiveWiseLinear(
+            npatch, self.nlatent // 2, self.nlatent // 4, act='none')
+        self.conv4 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
+                                                               self.nlatent //
+                                                               4,
+                                                               3,
+                                                               act='none')
+
+        self.th = nn.Tanh()
+        self.bn1 = torch.nn.BatchNorm1d(self.nlatent * npatch)
+        self.bn2 = torch.nn.BatchNorm1d(self.nlatent // 2 * npatch)
+        self.bn3 = torch.nn.BatchNorm1d(self.nlatent // 4 * npatch)
+
+    def forward(self, x):
+        B, N, _, P = x.shape
+        x = F.relu(self.bn1(self.conv1(x).view(B, -1, P))).view(B, N, -1, P)
+        x = F.relu(self.bn2(self.conv2(x).view(B, -1, P))).view(B, N, -1, P)
+        x = F.relu(self.bn3(self.conv3(x).view(B, -1, P))).view(B, N, -1, P)
+        x = self.th(self.conv4(x).view(B, -1, P)).view(B, N, -1, P)
+        return x
diff --git a/im2mesh/atlasnetv2/training.py b/im2mesh/atlasnetv2/training.py
index 208c3e0..e109a38 100644
--- a/im2mesh/atlasnetv2/training.py
+++ b/im2mesh/atlasnetv2/training.py
@@ -31,6 +31,9 @@ class Trainer(BaseTrainer):
                  input_type='img',
                  vis_dir=None,
                  threshold=0.5,
+                 point_scale=1,
+                 is_pykeops_loss=True,
+                 debugged=False,
                  eval_sample=False):
         self.model = model
         self.optimizer = optimizer
@@ -39,6 +42,9 @@ class Trainer(BaseTrainer):
         self.vis_dir = vis_dir
         self.threshold = threshold
         self.eval_sample = eval_sample
+        self.point_scale = point_scale
+        self.is_pykeops_loss = is_pykeops_loss
+        self.debugged = debugged
 
         self.distChamferL2 = dist_chamfer.chamferDist()
 
@@ -67,10 +73,11 @@ class Trainer(BaseTrainer):
         self.model.eval()
 
         device = self.device
-        pointcloud = data.get('pointcloud').to(device)
+        pointcloud = data.get('pointcloud').to(device) * self.point_scale
         patch = data.get('patch').to(device)
-        inputs = data.get('inputs', torch.empty(pointcloud.size(0),
-                                                0)).to(device)
+        inputs = data.get('inputs', torch.empty(
+            pointcloud.size(0),
+            0)).to(device) * (1 if self.debugged else self.point_scale)
 
         feature = self.model.encode_inputs(inputs)
 
@@ -99,10 +106,11 @@ class Trainer(BaseTrainer):
         '''
         device = self.device
 
-        pointcloud = data.get('pointcloud').to(device)
+        pointcloud = data.get('pointcloud').to(device) * self.point_scale
         patch = data.get('patch').to(device)
-        inputs = data.get('inputs', torch.empty(pointcloud.size(0),
-                                                0)).to(device)
+        inputs = data.get('inputs', torch.empty(
+            pointcloud.size(0),
+            0)).to(device) * (1 if self.debugged else self.point_scale)
 
         feature = self.model.encode_inputs(inputs)
 
@@ -122,17 +130,17 @@ class Trainer(BaseTrainer):
         for i in trange(B):
             if not inputs.ndim == 1:  # no input image
                 input_img_path = os.path.join(self.vis_dir, '%03d_in.png' % i)
-                plot = vis.visualize_data(inputs[i].cpu(),
+                plot = vis.visualize_data(inputs[i].cpu() / self.point_scale,
                                           self.input_type,
                                           input_img_path,
                                           return_plot=True)
                 input_images.append(
                     wandb.Image(plot, caption='input image {}'.format(i)))
-            plot = vis.visualize_pointcloud(coords[i].cpu().view(N * P, dims),
-                                            normals=None,
-                                            out_file=os.path.join(
-                                                self.vis_dir, '%03d.png' % i),
-                                            return_plot=True)
+            plot = vis.visualize_pointcloud(
+                coords[i].cpu().view(N * P, dims) / self.point_scale,
+                normals=None,
+                out_file=os.path.join(self.vis_dir, '%03d.png' % i),
+                return_plot=True)
             voxels_images.append(
                 wandb.Image(plot, caption='voxel {}'.format(i)))
         if not inputs.ndim == 1:  # no input image
@@ -146,10 +154,11 @@ class Trainer(BaseTrainer):
             data (dict): data dictionary
         '''
         device = self.device
-        pointcloud = data.get('pointcloud').to(device)
+        pointcloud = data.get('pointcloud').to(device) * self.point_scale
         patch = data.get('patch').to(device)
-        inputs = data.get('inputs', torch.empty(pointcloud.size(0),
-                                                0)).to(device)
+        inputs = data.get('inputs', torch.empty(
+            pointcloud.size(0),
+            0)).to(device) * (1 if self.debugged else self.point_scale)
 
         kwargs = {}
 
@@ -162,11 +171,14 @@ class Trainer(BaseTrainer):
                                    grid=patch,
                                    **kwargs)
         B, N, P, dims = coords.shape
-        """
-        dist1, dist2 = self.distChamferL2(coords.view(B, N * P, dims), pointcloud)
-        loss = torch.mean(dist1) + torch.mean(dist2)
-        """
 
-        loss = atv2_utils.chamfer_loss(coords.view(B, N * P, dims), pointcloud)
+        if self.is_pykeops_loss:
+            loss = atv2_utils.chamfer_loss(coords.view(B, N * P, dims),
+                                           pointcloud)
+        else:
+            dist1, dist2 = self.distChamferL2(coords.view(B, N * P, dims),
+                                              pointcloud)
+            loss = torch.mean(dist1) + torch.mean(dist2)
+
         losses = {'total_loss': loss}
         return losses
diff --git a/im2mesh/checkpoints.py b/im2mesh/checkpoints.py
index 75ff6ab..709646b 100644
--- a/im2mesh/checkpoints.py
+++ b/im2mesh/checkpoints.py
@@ -41,18 +41,18 @@ class CheckpointIO(object):
 
         wandb.save(filename)
 
-    def load(self, filename):
+    def load(self, filename, device=None):
         '''Loads a module dictionary from local file or url.
         
         Args:
             filename (str): name of saved module dictionary
         '''
         if is_url(filename):
-            return self.load_url(filename)
+            return self.load_url(filename, device=device)
         else:
-            return self.load_file(filename)
+            return self.load_file(filename, device=device)
 
-    def load_file(self, filename):
+    def load_file(self, filename, device=None):
         '''Loads a module dictionary from file.
         
         Args:
@@ -66,12 +66,15 @@ class CheckpointIO(object):
             print(filename)
             print('=> Loading checkpoint from local file...')
             state_dict = torch.load(filename)
-            scalars = self.parse_state_dict(state_dict)
+            if 'model' not in state_dict:
+                print('Detect weight file trained outside of occ env.')
+                state_dict = {'model': state_dict}
+            scalars = self.parse_state_dict(state_dict, device=device)
             return scalars
         else:
             raise FileExistsError
 
-    def load_url(self, url):
+    def load_url(self, url, device=None):
         '''Load a module dictionary from url.
         
         Args:
@@ -80,10 +83,10 @@ class CheckpointIO(object):
         print(url)
         print('=> Loading checkpoint from url...')
         state_dict = model_zoo.load_url(url, progress=True)
-        scalars = self.parse_state_dict(state_dict)
+        scalars = self.parse_state_dict(state_dict, device=device)
         return scalars
 
-    def parse_state_dict(self, state_dict):
+    def parse_state_dict(self, state_dict, device=None):
         '''Parse state_dict of model and return scalars.
         
         Args:
@@ -98,6 +101,7 @@ class CheckpointIO(object):
         """
         for k, v in self.module_dict.items():
             if k in state_dict:
+                print('load parameter')
                 #if False:
                 if k == 'model':
                     pretrained_dict = state_dict[k]
@@ -122,13 +126,18 @@ class CheckpointIO(object):
                     for key in pretrained_dict_new_param:
                         print(key, pretrained_dict_new_param[key].shape)
                     new_pretrained_dict.update(pretrained_dict_new_param)
-                    v.load_state_dict(new_pretrained_dict)
+                    if device is not None:
+                        v.load_state_dict(new_pretrained_dict)
+                    else:
+                        v.load_state_dict(new_pretrained_dict)
+
                 else:
                     v.load_state_dict(state_dict[k])
         scalars = {
             k: v
             for k, v in state_dict.items() if k not in self.module_dict
         }
+        print('load done')
         return scalars
 
 
diff --git a/im2mesh/config.py b/im2mesh/config.py
index 00e3fc4..eb843e7 100644
--- a/im2mesh/config.py
+++ b/im2mesh/config.py
@@ -1,7 +1,7 @@
 import yaml
 from torchvision import transforms
 from im2mesh import data
-from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet, atlasnetv2
+from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet, atlasnetv2, bspnet
 from im2mesh import preprocess
 from torch.utils import data as torch_data
 
@@ -12,7 +12,8 @@ method_dict = {
     'pix2mesh': pix2mesh,
     'dmc': dmc,
     'pnet': pnet,
-    'atlasnetv2': atlasnetv2
+    'atlasnetv2': atlasnetv2,
+    'bspnet': bspnet
 }
 
 
@@ -149,12 +150,11 @@ def get_dataset(mode, cfg, return_idx=False, return_category=False):
         if return_category:
             fields['category'] = data.CategoryField()
 
-        dataset = data.Shapes3dDataset(
-            dataset_folder,
-            fields,
-            split=split,
-            categories=categories,
-        )
+        dataset = data.Shapes3dDataset(dataset_folder,
+                                       fields,
+                                       split=split,
+                                       categories=categories,
+                                       cfg=cfg)
     elif dataset_type == 'kitti':
         dataset = data.KittiDataset(dataset_folder,
                                     img_size=cfg['data']['img_size'],
@@ -215,6 +215,9 @@ def get_inputs_field(mode, cfg):
 
         inputs_field = data.ImagesField(cfg['data']['img_folder'],
                                         transform,
+                                        cfg,
+                                        extension=cfg['data'].get(
+                                            'img_extension', 'jpg'),
                                         with_camera=with_camera,
                                         random_view=random_view)
     elif input_type == 'pointcloud':
diff --git a/im2mesh/data/__init__.py b/im2mesh/data/__init__.py
index 870a44f..50b1a6d 100644
--- a/im2mesh/data/__init__.py
+++ b/im2mesh/data/__init__.py
@@ -4,7 +4,7 @@ from im2mesh.data.core import (
 )
 from im2mesh.data.fields import (
     IndexField, CategoryField, ImagesField, PointsField,
-    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField, RawIDField, SDFPointsField, PlanarPatchField
+    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField, RawIDField, SDFPointsField, PlanarPatchField, PartLabeledPointCloudField
 )
 from im2mesh.data.transforms import (
     PointcloudNoise, SubsamplePointcloud,
@@ -41,4 +41,5 @@ __all__ = [
     KittiDataset,
     OnlineProductDataset,
     ImageDataset,
+    PartLabeledPointCloudField,
 ]
diff --git a/im2mesh/data/core.py b/im2mesh/data/core.py
index a224c72..1f2f128 100644
--- a/im2mesh/data/core.py
+++ b/im2mesh/data/core.py
@@ -39,7 +39,8 @@ class Shapes3dDataset(data.Dataset):
                  split=None,
                  categories=None,
                  no_except=True,
-                 transform=None):
+                 transform=None,
+                 cfg=None):
         ''' Initialization of the the 3D shape dataset.
 
         Args:
@@ -84,6 +85,12 @@ class Shapes3dDataset(data.Dataset):
             if not os.path.isdir(subpath):
                 logger.warning('Category %s does not exist in dataset.' % c)
 
+            if cfg is not None and 'semseg_path' in cfg['data']:
+                subpath = subpath.replace(cfg['data']['path'],
+                                          cfg['data']['semseg_path'])
+            elif cfg is not None and 'bspnet' in cfg['data']:
+                subpath = subpath.replace(cfg['data']['path'],
+                                          cfg['data']['bspnet']['path'])
             split_file = os.path.join(subpath, split + '.lst')
             with open(split_file, 'r') as f:
                 models_c = f.read().split('\n')
diff --git a/im2mesh/data/fields.py b/im2mesh/data/fields.py
index 64f1ae8..18e148c 100644
--- a/im2mesh/data/fields.py
+++ b/im2mesh/data/fields.py
@@ -68,6 +68,7 @@ class ImagesField(Field):
     def __init__(self,
                  folder_name,
                  transform=None,
+                 cfg=None,
                  extension='jpg',
                  random_view=True,
                  with_camera=False):
@@ -76,6 +77,19 @@ class ImagesField(Field):
         self.extension = extension
         self.random_view = random_view
         self.with_camera = with_camera
+        self.cfg = cfg
+
+        self.is_bspnet = False
+        if cfg is not None and self.cfg['method'] == 'bspnet':
+            self.is_bspnet = True
+            assert 'bspnet' in self.cfg['data']
+            bspnet_config = self.cfg['data']['bspnet']
+            assert bspnet_config['path'].startswith('data')
+
+            assert not self.with_camera
+
+            self.extension = bspnet_config['extension']
+            self.folder_name = bspnet_config['img_folder']
 
     def load(self, model_path, idx, category):
         ''' Loads the data point.
@@ -85,6 +99,10 @@ class ImagesField(Field):
             idx (int): ID of data point
             category (int): index of category
         '''
+        if self.is_bspnet:
+            model_path = model_path.replace(self.cfg['data']['path'],
+                                            self.cfg['data']['bspnet']['path'])
+
         folder = os.path.join(model_path, self.folder_name)
         files = glob.glob(os.path.join(folder, '*.%s' % self.extension))
         if self.random_view:
@@ -93,9 +111,15 @@ class ImagesField(Field):
             idx_img = 0
         filename = files[idx_img]
 
-        image = Image.open(filename).convert('RGB')
-        if self.transform is not None:
-            image = self.transform(image)
+        if self.is_bspnet:
+
+            image = np.load(filename)['image']
+            # 1 x 128 x 128
+            image = torch.from_numpy(image)
+        else:
+            image = Image.open(filename).convert('RGB')
+            if self.transform is not None:
+                image = self.transform(image)
 
         data = {None: image}
 
@@ -238,10 +262,23 @@ class PointCloudField(Field):
         with_transforms (bool): whether scaling and rotation dat should be
             provided
     '''
-    def __init__(self, file_name, transform=None, with_transforms=False):
+    def __init__(self,
+                 file_name,
+                 transform=None,
+                 cfg=None,
+                 with_transforms=False):
         self.file_name = file_name
         self.transform = transform
         self.with_transforms = with_transforms
+        self.cfg = cfg
+
+        self.is_bspnet = False
+        if cfg is not None and self.cfg['method'] == 'bspnet':
+            self.is_bspnet = True
+            assert 'bspnet' in self.cfg['data']
+            bspnet_config = self.cfg['data']['bspnet']
+            assert bspnet_config['path'].startswith('data')
+            assert 'pointcloud_file' in bspnet_config
 
     def load(self, model_path, idx, category):
         ''' Loads the data point.
@@ -263,6 +300,15 @@ class PointCloudField(Field):
             'normals': normals,
         }
 
+        if self.is_bspnet:
+            bsp_model_path = model_path.replace(
+                self.cfg['data']['path'], self.cfg['data']['bspnet']['path'])
+
+            data['imnet_points'] = torch.from_numpy(
+                trimesh.load(
+                    os.path.join(bsp_model_path, self.cfg['data']['bspnet']
+                                 ['pointcloud_file'])).vertices)
+
         if self.with_transforms and 'loc' in data and 'scale' in data:
             data['loc'] = pointcloud_dict['loc'].astype(np.float32)
             data['scale'] = pointcloud_dict['scale'].astype(np.float32)
@@ -559,3 +605,57 @@ class PlanarPatchField(Field):
                 'mesh_faces': self.faces.clone()
             })
         return data
+
+
+class PartLabeledPointCloudField(Field):
+    ''' Point cloud field.
+
+    It provides the field used for point cloud data. These are the points
+    randomly sampled on the mesh.
+
+    Args:
+        file_name (str): file name
+        transform (list): list of transformations applied to data points
+        with_transforms (bool): whether scaling and rotation dat should be
+            provided
+    '''
+    def __init__(self, file_name, cfg, transform=None):
+        self.file_name = file_name
+        self.transform = transform
+        self.shapenet_path = cfg['data']['path']
+        self.semseg_shapenet_path = cfg['data']['semseg_path']
+
+    def load(self, model_path, idx, category):
+        ''' Loads the data point.
+
+        Args:
+            model_path (str): path to model
+            idx (int): ID of data point
+            category (int): index of category
+        '''
+        model_path = model_path.replace(self.shapenet_path,
+                                        self.semseg_shapenet_path)
+        file_path = os.path.join(model_path, self.file_name)
+
+        pointcloud_dict = np.load(file_path)
+
+        points = pointcloud_dict['points'].astype(np.float32)
+        labels = pointcloud_dict['labels'].astype(np.float32)
+
+        data = {
+            None: points,
+            'labels': labels,
+        }
+
+        if self.transform is not None:
+            data = self.transform(data)
+
+        return data
+
+    def check_complete(self, files):
+        ''' Check if field is complete.
+        
+        Args:
+            files: files
+        '''
+        return True
diff --git a/im2mesh/eval.py b/im2mesh/eval.py
index 91052f8..820fafc 100644
--- a/im2mesh/eval.py
+++ b/im2mesh/eval.py
@@ -11,7 +11,8 @@ from pykeops.torch import LazyTensor
 import kaolin as kal
 import torch
 import warnings
-
+import time
+random.seed(0)
 # Maximum values for bounding box [-0.5, 0.5]^3
 EMPTY_PCL_DICT = {
     'completeness': np.sqrt(3),
@@ -38,8 +39,13 @@ class MeshEvaluator(object):
     Args:
         n_points (int): number of points to be used for evaluation
     '''
-    def __init__(self, n_points=100000):
+    def __init__(self,
+                 n_points=100000,
+                 is_sample_from_surface=False,
+                 is_normalize_by_side_length=False):
         self.n_points = n_points
+        self.is_sample_from_surface = is_sample_from_surface
+        self.is_normalize_by_side_length = is_normalize_by_side_length
 
     def eval_mesh(self,
                   mesh,
@@ -48,7 +54,8 @@ class MeshEvaluator(object):
                   points_iou,
                   occ_tgt,
                   is_eval_explicit_mesh=False,
-                  vertex_visibility=None):
+                  vertex_visibility=None,
+                  skip_iou=False):
         ''' Evaluates a mesh.
 
         Args:
@@ -58,31 +65,44 @@ class MeshEvaluator(object):
             points_iou (numpy_array): points tensor for IoU evaluation
             occ_tgt (numpy_array): GT occupancy values for IoU points
         '''
+        t0 = time.time()
         if is_eval_explicit_mesh:
-            assert vertex_visibility is not None
-            sampled_vertex_idx = np.zeros_like(vertex_visibility).astype(
-                np.bool)
-            pointcloud = mesh.vertices[vertex_visibility, :]
-            normals = mesh.vertex_normals[vertex_visibility, :]
+            if vertex_visibility is not None:
+                select_idx = vertex_visibility
+                if self.is_sample_from_surface and select_idx.sum(
+                ) > self.n_points:
+                    select_idx = np.random.choice(np.nonzero(select_idx)[0],
+                                                  size=self.n_points,
+                                                  replace=False)
+
+                pointcloud = mesh.vertices[select_idx, :]
+                normals = mesh.vertex_normals[select_idx, :]
+            else:
+                pointcloud = mesh.vertices
+                normals = mesh.vertex_normals
+            t0 = time.time()
             pointcloud = pointcloud.astype(np.float32)
             normals = normals.astype(np.float32)
+            #print('copy pcd and normals to cpu', time.time() - t0)
 
+            t0 = time.time()
             if pointcloud.shape[0] > self.n_points:
                 select_idx = random.sample(range(pointcloud.shape[0]),
                                            self.n_points)
                 pointcloud = pointcloud[select_idx:]
-            if normals.shape[0] > self.n_points:
-                select_idx = random.sample(range(normals.shape[0]),
-                                           self.n_points)
+                #if normals.shape[0] > self.n_points:
+                #    select_idx = random.sample(range(normals.shape[0]),
+                #                               self.n_points)
                 normals = normals[select_idx:]
             if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
-                select_idx = random.sample(range(pointcloud.shape[0]),
+                select_idx = random.sample(range(pointcloud_tgt.shape[0]),
                                            pointcloud.shape[0])
                 pointcloud_tgt = pointcloud_tgt[select_idx, :]
-            if normals_tgt.shape[0] > normals.shape[0]:
-                select_idx = random.sample(range(normals.shape[0]),
-                                           pointcloud.shape[0])
+                #if normals_tgt.shape[0] > normals.shape[0]:
+                #    select_idx = random.sample(range(normals.shape[0]),
+                #                               pointcloud.shape[0])
                 normals_tgt = normals_tgt[select_idx, :]
+            #print('random sample points', time.time() - t0)
         else:
             if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
                 pointcloud, idx = mesh.sample(self.n_points, return_index=True)
@@ -92,15 +112,21 @@ class MeshEvaluator(object):
                 pointcloud = np.empty((0, 3))
                 normals = np.empty((0, 3))
 
+        t0 = time.time()
         out_dict = self.eval_pointcloud(pointcloud, pointcloud_tgt, normals,
                                         normals_tgt)
+        #print('eval point cloud', time.time() - t0)
 
-        if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
+        t0 = time.time()
+        if len(mesh.vertices) != 0 and len(
+                mesh.faces) != 0 and not skip_iou and False:
             occ = check_mesh_contains(mesh, points_iou)
             out_dict['iou'] = compute_iou(occ, occ_tgt)
         else:
             out_dict['iou'] = 0.
+        #print('iou', time.time() - t0)
 
+        #print("eval_mesh", time.time() - t0)
         return out_dict
 
     def eval_pointcloud(self,
@@ -131,21 +157,31 @@ class MeshEvaluator(object):
         # from thre predicted point cloud
         completeness, completeness_normals = distance_p2p(
             pointcloud_tgt, normals_tgt, pointcloud, normals)
-        completeness2 = completeness**2
 
+        if self.is_normalize_by_side_length:
+            normalize_scale = 1. / (
+                (pointcloud_tgt.max(axis=0) -
+                 pointcloud_tgt.min(axis=0)).max() / 10).item()
+            completeness = completeness * normalize_scale
+        t0 = time.time()
+        completeness2 = completeness**2
         completeness = completeness.mean()
         completeness2 = completeness2.mean()
         completeness_normals = completeness_normals.mean()
+        #print('calc compness', time.time() - t0)
 
         # Accuracy: how far are th points of the predicted pointcloud
         # from the target pointcloud
         accuracy, accuracy_normals = distance_p2p(pointcloud, normals,
                                                   pointcloud_tgt, normals_tgt)
+        if self.is_normalize_by_side_length:
+            accuracy = accuracy * normalize_scale
+        t0 = time.time()
         accuracy2 = accuracy**2
-
         accuracy = accuracy.mean()
         accuracy2 = accuracy2.mean()
         accuracy_normals = accuracy_normals.mean()
+        #print('calc accuracy', time.time() - t0)
 
         # Chamfer distance
         chamferL2 = 0.5 * (completeness2 + accuracy2)
@@ -184,8 +220,17 @@ class MeshEvaluator(object):
         '''
 
         if is_eval_explicit_mesh:
-            assert vertex_visibility is not None
-            pointcloud = mesh.vertices[vertex_visibility, :]
+            if vertex_visibility is not None:
+                select_idx = vertex_visibility
+                if self.is_sample_from_surface and select_idx.sum(
+                ) > self.n_points:
+                    select_idx = np.random.choice(np.nonzero(select_idx)[0],
+                                                  size=self.n_points,
+                                                  replace=False)
+                pointcloud = mesh.vertices[select_idx, :]
+            else:
+                pointcloud = mesh.vertices
+
             pointcloud = pointcloud.astype(np.float32)
 
             if pointcloud.shape[0] > self.n_points:
@@ -194,7 +239,7 @@ class MeshEvaluator(object):
                 pointcloud = pointcloud[select_idx:]
 
             if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
-                select_idx = random.sample(range(pointcloud.shape[0]),
+                select_idx = random.sample(range(pointcloud_tgt.shape[0]),
                                            pointcloud.shape[0])
                 pointcloud_tgt = pointcloud_tgt[select_idx, :]
 
@@ -212,10 +257,18 @@ class MeshEvaluator(object):
             pointcloud = np.empty((0, 3))
         """
 
+        if self.is_normalize_by_side_length:
+            normalize_scale = 1. / (
+                (pointcloud_tgt.max(axis=0) -
+                 pointcloud_tgt.min(axis=0)).max() / 10).item()
+        else:
+            normalize_scale = 1.
+
         out_dict = fscore(pointcloud[np.newaxis, ...],
                           pointcloud_tgt[np.newaxis, ...],
                           thresholds=thresholds,
-                          mode='pykeops')
+                          mode='pykeops',
+                          normalize_scale=normalize_scale)
         if out_dict is None:
             return out_dict
         else:
@@ -245,7 +298,11 @@ class MeshEvaluator(object):
         return out_dict
 
 
-def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
+def distance_p2p(points_src,
+                 normals_src,
+                 points_tgt,
+                 normals_tgt,
+                 mode='pykeops'):
     ''' Computes minimal distances of each point in points_src to points_tgt.
 
     Args:
@@ -254,22 +311,46 @@ def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
         points_tgt (numpy array): target points
         normals_tgt (numpy array): target normals
     '''
-    kdtree = KDTree(points_tgt)
-    dist, idx = kdtree.query(points_src)
-
-    if normals_src is not None and normals_tgt is not None:
-        normals_src = \
-            normals_src / np.linalg.norm(normals_src, axis=-1, keepdims=True)
-        normals_tgt = \
-            normals_tgt / np.linalg.norm(normals_tgt, axis=-1, keepdims=True)
-
-        normals_dot_product = (normals_tgt[idx] * normals_src).sum(axis=-1)
-        # Handle normals that point into wrong direction gracefully
-        # (mostly due to mehtod not caring about this in generation)
-        normals_dot_product = np.abs(normals_dot_product)
+    assert mode in ['pykeops', 'original']
+    if mode == 'pykeops':
+        # output is in torch tensor
+        t0 = time.time()
+        dist, idx = one_sided_chamfer_distance_with_index(
+            points_src, points_tgt)
+        #print('calc dist and dix', time.time() - t0)
+        t0 = time.time()
+        dist = dist.to('cpu').numpy()
+        #print('copy dist to cpu', time.time() - t0)
+        if normals_src is not None and normals_tgt is not None:
+            t0 = time.time()
+            normals_src = torch.nn.functional.normalize(
+                torch.from_numpy(normals_src).to('cuda'), dim=-1)
+            normals_tgt = torch.nn.functional.normalize(
+                torch.from_numpy(normals_tgt).to('cuda'), dim=-1)
+
+            normals_dot_product = (normals_tgt[idx] * normals_src).sum(
+                axis=-1).abs().to('cpu').numpy()
+            #print('calc normal const', time.time() - t0)
+        else:
+            normals_dot_product = np.array([np.nan] * points_src.shape[0],
+                                           dtype=np.float32)
     else:
-        normals_dot_product = np.array([np.nan] * points_src.shape[0],
-                                       dtype=np.float32)
+        kdtree = KDTree(points_tgt)
+        dist, idx = kdtree.query(points_src)
+
+        if normals_src is not None and normals_tgt is not None:
+            normals_src = \
+                normals_src / np.linalg.norm(normals_src, axis=-1, keepdims=True)
+            normals_tgt = \
+                normals_tgt / np.linalg.norm(normals_tgt, axis=-1, keepdims=True)
+
+            normals_dot_product = (normals_tgt[idx] * normals_src).sum(axis=-1)
+            # Handle normals that point into wrong direction gracefully
+            # (mostly due to mehtod not caring about this in generation)
+            normals_dot_product = np.abs(normals_dot_product)
+        else:
+            normals_dot_product = np.array([np.nan] * points_src.shape[0],
+                                           dtype=np.float32)
     return dist, normals_dot_product
 
 
@@ -304,7 +385,41 @@ def chamfer_distance(pred, target, pykeops=True):
     return pred2target, target2pred
 
 
-def fscore(pred_points, target_points, thresholds=[0.01], mode='pykeops'):
+def one_sided_chamfer_distance_with_index(source_points, target_points):
+    assert source_points.ndim in [2, 3]
+    assert target_points.ndim in [2, 3]
+    assert target_points.ndim == source_points.ndim
+    original_ndim = target_points.ndim
+    if isinstance(source_points, np.ndarray):
+        source_points = torch.from_numpy(source_points).to('cuda')
+    if isinstance(target_points, np.ndarray):
+        target_points = torch.from_numpy(target_points).to('cuda')
+
+    if source_points.ndim == 2:
+        source_points = source_points.unsqueeze(0)
+    if target_points.ndim == 2:
+        target_points = target_points.unsqueeze(0)
+
+    G_i1 = LazyTensor(source_points.unsqueeze(2))
+    X_j1 = LazyTensor(target_points.unsqueeze(1))
+
+    dist = (G_i1 - X_j1).norm2()
+
+    # N
+    idx = dist.argmin(dim=2).squeeze(-1)
+    pred2target = dist.min(2).squeeze(-1)
+    if original_ndim == 2:
+        idx = idx[0]
+        pred2target = pred2target[0]
+
+    return pred2target, idx
+
+
+def fscore(pred_points,
+           target_points,
+           thresholds=[0.01],
+           mode='pykeops',
+           normalize_scale=1.):
     assert mode in ['kaolin', 'pykeops']
     assert isinstance(thresholds, list)
 
@@ -343,6 +458,9 @@ def fscore(pred_points, target_points, thresholds=[0.01], mode='pykeops'):
         gt_distances, pred_distances = chamfer_distance(
             pred_points, target_points)
 
+        gt_distances = gt_distances * normalize_scale
+        pred_distances = pred_distances * normalize_scale
+
         for threshold in thresholds:
             fn = (pred_distances > threshold).sum(-1).float()
             fp = (gt_distances > threshold).sum(-1).float()
diff --git a/im2mesh/pnet/config.py b/im2mesh/pnet/config.py
index 73fc786..67ee7f5 100644
--- a/im2mesh/pnet/config.py
+++ b/im2mesh/pnet/config.py
@@ -153,14 +153,21 @@ def get_data_fields(mode, cfg):
             sdf_points_transform,
             with_transforms=with_transforms)
 
-    pointcloud_transform = data.SubsamplePointcloud(
-        cfg['data']['pointcloud_target_n'])
-    if cfg.get('sdf_generation', False):
-        pointcloud_transform = None
+    if cfg['test'].get('is_eval_semseg', False):
+        fields['pointcloud'] = data.PartLabeledPointCloudField(
+            cfg['data']['semseg_pointcloud_file'], cfg)
 
-    fields['pointcloud'] = data.PointCloudField(cfg['data']['pointcloud_file'],
-                                                pointcloud_transform,
-                                                with_transforms=True)
+    else:
+        pointcloud_transform = data.SubsamplePointcloud(
+            cfg['data']['pointcloud_target_n'])
+        if cfg.get('sdf_generation', False):
+            pointcloud_transform = None
+
+        fields['pointcloud'] = data.PointCloudField(
+            cfg['data']['pointcloud_file'],
+            pointcloud_transform,
+            cfg,
+            with_transforms=True)
     fields['angles'] = data.SphericalCoordinateField(
         cfg['data']['primitive_points_sample_n'],
         mode,
diff --git a/im2mesh/pnet/models/decoder.py b/im2mesh/pnet/models/decoder.py
index 32fe043..32d791d 100644
--- a/im2mesh/pnet/models/decoder.py
+++ b/im2mesh/pnet/models/decoder.py
@@ -60,6 +60,7 @@ class PeriodicShapeDecoderSimplest(nn.Module):
         return_sdf=False,
         is_radius_reg=False,
         spherical_angles=False,
+        extract_surface_point_by_max=False,
         last_scale=.1):
         super().__init__()
         assert dim in [2, 3]
@@ -113,6 +114,7 @@ class PeriodicShapeDecoderSimplest(nn.Module):
             is_feature_radius=is_feature_radius,
             no_last_bias=no_last_bias,
             spherical_angles=spherical_angles,
+            extract_surface_point_by_max=extract_surface_point_by_max,
             return_sdf=return_sdf)
         # simple_sampler = super_shape_sampler.SuperShapeSampler(max_m,
         #                                                        n_primitives,
@@ -142,25 +144,26 @@ class PeriodicShapeDecoderSimplest(nn.Module):
                                     points=feature,
                                     return_surface_mask=True)
             pcoord, o1, o2, o3 = output
-            if self.is_radius_reg:
-                # B, N, P, dim
-                # pcoord
-
-                # B, N, 1, dim
-                transition = params['transition'].unsqueeze(2)
-                pcentered_coord = pcoord - transition
-                radius = (pcentered_coord**2).sum(-1).clamp(min=EPS).sqrt()
-                output = (pcoord, o1, o2, o3, radius)
-
-            else:
-                output = (pcoord, o1, o2, o3, None)
-
         else:
             output = self.simple_sampler(params,
                                          thetas=angles,
                                          coord=coord,
                                          points=color_feature,
                                          return_surface_mask=True)
+            pcoord, o1, o2, o3 = output
+        if self.is_radius_reg:
+            # B, N, P, dim
+            # pcoord
+
+            # B, N, 1, dim
+            transition = params['transition'].unsqueeze(2)
+            pcentered_coord = pcoord - transition
+            radius = (pcentered_coord**2).sum(-1).clamp(min=EPS).sqrt()
+            output = (pcoord, o1, o2, o3, radius)
+
+        else:
+            output = (pcoord, o1, o2, o3, None)
+
         #shapes = [[10, 12, 900, 3], [10, 12, 900], [10, 12, 2048]]  #,
         #[10, 12, 10800]]
         """
diff --git a/im2mesh/pnet/training.py b/im2mesh/pnet/training.py
index 53db728..629b999 100644
--- a/im2mesh/pnet/training.py
+++ b/im2mesh/pnet/training.py
@@ -327,15 +327,27 @@ class Trainer(BaseTrainer):
                     pointcloud.shape[:2], device=occ.device, dtype=occ.dtype)
             ],
                             axis=1)
-
+        """
         c = self.model.encode_inputs(inputs)
         q_z = self.model.infer_z(points, occ, c, **kwargs)
         z = q_z.rsample()
 
         # General points
 
+        output = self.model.decode(scaled_coord,
+                                          z,
+                                          c,
+                                          angles=angles,
+                                          **kwargs)
+
+        """
         scaled_coord = points * self.pnet_point_scale
-        output = self.model.decode(scaled_coord, z, c, angles=angles, **kwargs)
+        output = self.model(scaled_coord,
+                            inputs,
+                            sample=True,
+                            angles=angles,
+                            **kwargs)
+
         super_shape_point, surface_mask, sgn, sgn_BxNxNP, radius = output
 
         # losses
@@ -396,11 +408,19 @@ class Trainer(BaseTrainer):
             apply_surface_mask_before_chamfer=self.is_strict_chamfer)
 
         if self.is_normal_loss:
+            """
             output = self.model.decode(scaled_coord,
                                        z,
                                        c,
                                        angles=normal_angles,
                                        **kwargs)
+            """
+            output = self.model(scaled_coord,
+                                inputs,
+                                sample=True,
+                                angles=normal_angles,
+                                **kwargs)
+
             normal_vertices, normal_mask, _, _, _ = output
 
             B, N, P, D = normal_vertices.shape
diff --git a/paper_resources/pix3d_comparison/scripts/generate_pix3d_table.py b/paper_resources/pix3d_comparison/scripts/generate_pix3d_table.py
deleted file mode 100644
index 54187a8..0000000
--- a/paper_resources/pix3d_comparison/scripts/generate_pix3d_table.py
+++ /dev/null
@@ -1,193 +0,0 @@
-# To add a new cell, type '# %%'
-# To add a new markdown cell, type '# %% [markdown]'
-# %%
-import pandas as pd
-import numpy as np
-import os
-from collections import OrderedDict
-import pickle
-import subprocess
-import glob
-
-
-def join(*args):
-    return os.path.join(*args)
-
-
-# %%
-resource_base_dir_path = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/pix3d_comparison/resources'
-fscore_values = [0.005, 0.0107337006427915, 0.05, 0.1, 0.2]
-fscore_display_values = ['0.5\%', '1\%', '5\%', '10\%', '20\%']
-sample_generation_classes = [
-    'chair', 'table', 'bed', 'bookcase', 'misc', 'sofa'
-]
-
-shapenetv1_path = '/data/unagi0/kawana/workspace/ShapeNetCore.v1'
-shapenetv2_path = '/data/unagi0/kawana/workspace/ShapeNetCore.v2'
-shapenetocc_path = '/home/mil/kawana/workspace/occupancy_networks/data/ShapeNet'
-pix3d_base_path = '/home/mil/kawana/workspace/occupancy_networks/data/Pix3D'
-rendering_script_path = '/home/mil/kawana/workspace/occupancy_networks/scripts/render_3dobj.sh'
-
-side_length_scale = 0.0107337006427915
-ours_name = 'PSNet30'
-theirs_name = 'OccNet'
-
-rendering_out_base_dir = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/pix3d_comparison'
-
-pix3d_df_path = join(pix3d_base_path, 'pix3d_*.pkl')
-rendering_out_dir = os.path.join(rendering_out_base_dir, 'resources')
-rendering_gt_mesh_cache_dir = os.path.join(rendering_out_base_dir, 'cache')
-
-id_to_dir_path_map = {
-    'OccNet':
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/onet_pretrained',
-    'PSNet30':
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_20200417_123951'
-}
-id_to_mesh_dir_name = {
-    'OccNet':
-    'pretrained_pix3d_class_agnostic_margin_224',
-    'PSNet30':
-    'generation_pix3d_class_agnostic_margin_224_explicit_20200417_163408'
-}
-id_to_fscore_map = {
-    'OccNet': 'eval_fscore_from_meshes.csv',
-    'PSNet30': 'eval_fscore_from_meshes_explicit.csv'
-}
-id_to_full_fscore_map = {
-    'OccNet': 'eval_fscore_from_meshes_full.pkl',
-    'PSNet30': 'eval_fscore_from_meshes_full_explicit.pkl'
-}
-id_to_cd1_map = {
-    'OccNet': 'eval_meshes_full.csv',
-    'PSNet30': 'eval_meshes_full_explicit.csv'
-}
-ids = list(id_to_cd1_map.keys())
-fscore_key_strs = ['fscore_th={} (mesh)'.format(val) for val in fscore_values]
-
-# %%
-data = []
-for idx in ids:
-    fdf = pd.read_csv(
-        os.path.join(id_to_dir_path_map[idx], id_to_mesh_dir_name[idx],
-                     id_to_fscore_map[idx]))
-    data.append(
-        OrderedDict({
-            disp_val: fdf[key].mean()
-            for disp_val, key in zip(fscore_display_values, fscore_key_strs)
-        }))
-df = pd.DataFrame(data, columns=fscore_display_values)
-
-# %%
-body = ""
-names = ' & '.join(['Threshold(%)', *df.columns]) + " \\ \hline"
-body += names
-for idx in range(len(df)):
-    method_id = ids[idx]
-
-    def cutdeci(s):
-        if isinstance(s, str):
-            return s
-        return "{:.3f}".format(s)
-
-    els = [method_id, *map(cutdeci, df.loc[idx].tolist())]
-    row = ' & '.join(els) + " \\ "
-    body += ('\n' + row)
-
-print(body)
-
-with open(os.path.join(resource_base_dir_path, 'table.txt'), 'w') as f:
-    print(body.replace('\\', '\\\\').replace('\\\h',
-                                             '\h').replace('\\\mu', '\mu'),
-          file=f)
-
-# %%
-
-synset_to_label = {
-    '04256520': 'sofa',
-    '04379243': 'table',
-    '02691156': 'bed',
-    '02828884': 'bookcase',
-    '02933112': 'desk',
-    '02958343': 'misc',
-    '03001627': 'chair',
-    '03211117': 'tool',
-    '03636649': 'wardrobe'
-}
-
-label_to_synset = {v: k for k, v in synset_to_label.items()}
-
-# %%
-oursdf = pickle.load(
-    open(
-        os.path.join(id_to_dir_path_map[ours_name],
-                     id_to_mesh_dir_name[ours_name],
-                     id_to_full_fscore_map[ours_name]), 'rb'))
-
-theirsdf = pickle.load(
-    open(
-        os.path.join(id_to_dir_path_map[theirs_name],
-                     id_to_mesh_dir_name[theirs_name],
-                     id_to_full_fscore_map[theirs_name]), 'rb'))
-
-# %%
-pix3d_df_paths = glob.glob(pix3d_df_path)
-dfs = []
-for path in pix3d_df_paths:
-    pix3d_df = pickle.load(open(path, 'rb'))
-    dfs.append(pix3d_df)
-pix3ddf = pd.concat(dfs)
-
-for class_name in sample_generation_classes:
-    class_id = label_to_synset[class_name]
-
-    oursdf_cls = oursdf[oursdf['class id'] == class_id]
-    theirsdf_cls = theirsdf[theirsdf['class id'] == class_id]
-
-    assert len(oursdf_cls) == len(theirsdf_cls), (len(oursdf_cls),
-                                                  len(theirsdf_cls))
-
-    fscore_key = 'fscore_th={} (mesh)'.format(0.005)
-    """
-    oursdf_cls['diff'] = (oursdf_cls[fscore_key] - theirsdf_cls[fscore_key])
-
-    filter = oursdf_cls['diff'] > 0
-    idx = oursdf_cls[filter]['diff'].argmax()
-    model_id = oursdf_cls[filter]['modelname'].iloc[idx]
-    """
-
-    model_idx = oursdf_cls[fscore_key].argmax()
-    print(model_idx, class_name)
-    print(oursdf_cls['modelname'].iloc[model_idx])
-    model_id = oursdf_cls['modelname'].iloc[model_idx]
-
-    #filter = oursdf_cls[fscore_key] > 0.5
-    #idx = oursdf_cls[filter]['diff'].argmax()
-    #model_id = oursdf_cls[filter]['modelname'].iloc[idx]
-
-    model_paths = {
-        idx: os.path.join(id_to_dir_path_map[idx], id_to_mesh_dir_name[idx],
-                          'meshes', label_to_synset[class_name],
-                          str(model_id) + '.off')
-        for idx in ids
-    }
-
-    model_paths['gt'] = os.path.join(pix3d_base_path, class_id, model_id,
-                                     'model.off')
-
-    if not os.path.exists(rendering_out_dir):
-        os.makedirs(rendering_out_dir)
-
-    camera_param_path = os.path.join(rendering_out_base_dir,
-                                     'camera_param.txt')
-    for idx in model_paths:
-        command = 'sh {script} {camera_param} {model} {out_dir} {idx}'.format(
-            script=rendering_script_path,
-            camera_param=camera_param_path,
-            model=model_paths[idx],
-            out_dir=rendering_out_dir,
-            idx=idx + '_' + class_name)
-        print(command)
-        subprocess.run(command, shell=True)
-
-# %%
diff --git a/paper_resources/radius_distribution/scripts/generate_radius_distribution.py b/paper_resources/radius_distribution/scripts/generate_radius_distribution.py
index ce00069..94a36b5 100644
--- a/paper_resources/radius_distribution/scripts/generate_radius_distribution.py
+++ b/paper_resources/radius_distribution/scripts/generate_radius_distribution.py
@@ -1,5 +1,6 @@
 # %%
 import argparse
+import torch
 import os
 import subprocess
 import hashlib
@@ -9,33 +10,30 @@ import numpy as np
 from datetime import datetime
 from tqdm import tqdm
 import sys
-sys.path.insert(0, '/home/mil/kawana/workspace/occupancy_networks/im2mesh')
-from im2mesh import config, data
-from im2mesh.checkpoints import CheckpointIO
 import shutil
 import yaml
+import matplotlib.pyplot as plt
 from collections import OrderedDict
+sys.path.insert(0, '/home/mil/kawana/workspace/occupancy_networks')
+sys.path.insert(
+    0, '/home/mil/kawana/workspace/occupancy_networks/external/atlasnetv2')
+sys.path.insert(
+    0,
+    '/home/mil/kawana/workspace/occupancy_networks/external/periodic_shapes')
+from im2mesh import config, data
+from im2mesh.checkpoints import CheckpointIO
 
 # %%
-config_path = ''
+config_path = '/home/mil/kawana/workspace/occupancy_networks/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml'
+"""
+config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_20200414_051415/config.yaml'
+"""
 resource_path = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/radius_distribution/resources'
-use_cache_flag = False
-
-# %%
-
-
-def represent_odict(dumper, instance):
-    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
-
-
-yaml.add_representer(OrderedDict, represent_odict)
-
-
-def construct_odict(loader, node):
-    return OrderedDict(loader.construct_pairs(node))
 
+project_base_path = '/home/mil/kawana/workspace/occupancy_networks'
+use_cache_flag = False
 
-yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
+os.chdir(project_base_path)
 
 # %%
 # Get configuration and basic arguments
@@ -43,7 +41,9 @@ cfg = config.load_config(config_path, 'configs/default.yaml')
 cfg['data']['icosahedron_uv_margin'] = 0
 cfg['data']['icosahedron_uv_margin_phi'] = 0
 cfg['data']['points_subsample'] = 2
-cfg['data']['debug'] = {'sample_n': 50}
+# Enable to get radius
+cfg['trainer']['is_radius_reg'] = True
+cfg['data']['debug'] = {'sample_n': 10}
 
 is_cuda = torch.cuda.is_available()
 device = torch.device("cuda" if is_cuda else "cpu")
@@ -67,7 +67,6 @@ trainer = config.get_trainer(model, None, cfg, device=device)
 
 # Print model
 nparameters = sum(p.numel() for p in model.parameters())
-print(model)
 print('Total number of parameters: %d' % nparameters)
 
 # Evaluate
@@ -85,6 +84,9 @@ thetas = np.linspace(-np.pi, np.pi - 2 * np.pi / 100, 100)
 phis = np.linspace(-np.pi / 2, np.pi / 2 - np.pi / 100, 100)
 theta_radius_list = []
 phi_radius_list = []
+theta_intra_primitive_radius_list_std = []
+theta_intra_primitive_radius_list_mean = []
+phi_intra_primitive_radius_list = []
 # Handle each dataset separately
 for it, data in enumerate(tqdm(test_loader)):
     if data is None:
@@ -112,40 +114,109 @@ for it, data in enumerate(tqdm(test_loader)):
         'class name': category_name,
         'modelname': modelname,
     }
+    points = data.get('points').to(device)
     inputs = data.get('inputs', torch.empty(points.size(0), 0)).to(device)
-    theta = torch.meshgrid(
-        [torch.tensor(thetas, device=device),
-         torch.zeros([1], device=device)],
-        device=device).unsqueeze(0)
-    phi = torch.meshgrid(
-        [torch.zeros([1], device=device),
-         torch.tensor(phis, device=device)],
-        device=device).unsqueeze(0)
+    theta = torch.cat(torch.meshgrid([
+        torch.tensor(thetas, device=device).float(),
+        torch.zeros([1], device=device).float()
+    ], ),
+                      axis=-1).unsqueeze(0)
+    phi = torch.cat(torch.meshgrid([
+        torch.tensor(phis, device=device).float(),
+        torch.zeros([1], device=device).float()
+    ], ),
+                    axis=-1)[..., [1, 0]].unsqueeze(0)
     points = data.get('points').to(device)
 
     feature = model.encode_inputs(inputs)
 
     kwargs = {}
-    scaled_coord = points * cfg['trainer']['pnet_point_scale']
+    point_scale = cfg['trainer']['pnet_point_scale']
+
+    scaled_coord = points * point_scale
     output_theta = model.decode(scaled_coord,
                                 None,
                                 feature,
                                 angles=theta,
                                 **kwargs)
     _, _, _, _, theta_radius = output_theta
-    theta_radius_list.append(theta_radius[:, :, :,
-                                          0].mean(axis=(0, 1)).cpu().numpy())
+    theta_radius /= point_scale
+    theta_radius_list.append(
+        theta_radius.mean(axis=(0, 1)).cpu().detach().numpy())
+    theta_intra_primitive_radius_list_std.append(
+        theta_radius.std(axis=2).mean(axis=(0)).cpu().detach().numpy())
+    theta_intra_primitive_radius_list_mean.append(
+        theta_radius.mean(axis=2).mean(axis=(0)).cpu().detach().numpy())
     output_phi = model.decode(scaled_coord,
                               None,
                               feature,
                               angles=phi,
                               **kwargs)
     _, _, _, _, phi_radius = output_phi
-    phi_radius_list.append(phi_radius[:, :, :,
-                                      1].mean(axis=(0, 1)).cpu().numpy())
-
+    phi_radius /= point_scale
+    phi_radius_list.append(phi_radius.mean(axis=(0, 1)).cpu().detach().numpy())
+    phi_intra_primitive_radius_list.append(
+        phi_radius.std(axis=2).mean(axis=(0)).cpu().detach().numpy())
+# %%
+thetas_in_deg = thetas / np.pi * 180
 # Create pandas dataframe and save
-theta_df = pd.DataFrame(theta_radius_list, columns=thetas).mean()
-theta_df.to_pickle('test')
+thetas_mean_df = pd.DataFrame(theta_radius_list, columns=thetas_in_deg).mean()
+thetas_std_df = pd.DataFrame(theta_radius_list, columns=thetas_in_deg).std()
+
+thetas_df = pd.DataFrame()
+thetas_df['deg'] = thetas_in_deg
+thetas_df['mean'] = thetas_mean_df.values
+thetas_df['std'] = thetas_std_df.values
+#theta_df.to_pickle('test')
 
-print(theta_df)
+phis_in_deg = phis / np.pi * 180
+# Create pandas dataframe and save
+phis_mean_df = pd.DataFrame(phi_radius_list, columns=phis_in_deg).mean()
+phis_std_df = pd.DataFrame(phi_radius_list, columns=phis_in_deg).std()
+
+phis_df = pd.DataFrame()
+phis_df['deg'] = phis_in_deg
+phis_df['mean'] = phis_mean_df.values
+phis_df['std'] = phis_std_df.values
+#theta_df.to_pickle('test')
+
+thetas_intra_df_mean = pd.DataFrame(
+    theta_intra_primitive_radius_list_mean).mean()
+thetas_intra_df_std = pd.DataFrame(
+    theta_intra_primitive_radius_list_std).mean()
+
+thetas_intra_df = pd.DataFrame()
+thetas_intra_df['deg'] = np.arange(0, len(thetas_intra_df_mean))
+thetas_intra_df['mean'] = thetas_intra_df_mean.values
+thetas_intra_df['std'] = thetas_intra_df_std.values
+# %%
+fig = plt.figure()
+degs = thetas_df['deg']
+band = thetas_df['std'] * 2
+mean = thetas_df['mean']
+ax = fig.add_subplot(1, 2, 1)
+ax.plot(degs, mean)
+ax.fill_between(degs, mean - band, mean + band, color='gray', alpha=0.2)
+ax.set_xticks([-180, -90, 0, 90, 180])
+ax.set_xticklabels(['$-\pi$', '$-\pi/2$', '0', '$\pi/2$', '$\pi$'])
+ax.legend(['mean', '$2\sigma$'])
+
+degs = phis_df['deg']
+band = phis_df['std'] * 2
+mean = phis_df['mean']
+ax = fig.add_subplot(1, 2, 2)
+ax.plot(degs, mean)
+ax.fill_between(degs, mean - band, mean + band, color='gray', alpha=0.2)
+ax.set_xticks([-90, 0, 90])
+ax.set_xticklabels(['$-\pi/2$', '0', '$\pi/2$'])
+ax.legend(['mean', '$2\sigma$'])
+
+fig.show()
+
+fig = plt.figure()
+ax = fig.add_subplot(1, 2, 1)
+degs = thetas_intra_df['deg']
+band = thetas_intra_df['std'] * 2
+mean = thetas_intra_df['mean']
+ax = fig.add_subplot(1, 2, 1)
+plt.errorbar(degs, mean, yerr=band)
diff --git a/scripts/render_3dobj.sh b/scripts/render_3dobj.sh
index fb2adbf..61ed991 100755
--- a/scripts/render_3dobj.sh
+++ b/scripts/render_3dobj.sh
@@ -4,6 +4,7 @@ param=$1
 model_path=$2
 out=$3
 name=$4
+skip_reconvert_flag=${5:-"false"}
 RENDER_FOR_CNN_PATH=/home/mil/kawana/workspace/RenderForCNN
 
 PYTHONPATH=$PYTHONPATH:RENDER_FOR_CNN_PATH \
@@ -11,5 +12,6 @@ PYTHONPATH=$PYTHONPATH:RENDER_FOR_CNN_PATH \
 ${RENDER_FOR_CNN_PATH}/render_pipeline/blank.blend \
 --background \
 --python ${RENDER_FOR_CNN_PATH}/render_pipeline/render_model_views.py  \
+${skip_reconvert_flag} \
 ${model_path} \
 ${name}  ${param} ${out}
diff --git a/train.py b/train.py
index d761de2..6a2c7d9 100755
--- a/train.py
+++ b/train.py
@@ -12,13 +12,18 @@ from im2mesh.checkpoints import CheckpointIO
 import wandb
 import dotenv
 
-dotenv.load_dotenv(verbose=True)
+dotenv.load_dotenv('/home/mil/kawana/workspace/occupancy_networks/.env',
+                   verbose=True)
+os.environ['WANDB_PROJECT'] = 'periodic_shape_occupancy_networks'
 
 # Arguments
 parser = argparse.ArgumentParser(
     description='Train a 3D reconstruction model.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
+parser.add_argument('--data_parallel',
+                    action='store_true',
+                    help='Train with data parallel.')
 parser.add_argument(
     '--exit-after',
     type=int,
@@ -35,7 +40,10 @@ device = torch.device("cuda" if is_cuda else "cpu")
 t0 = time.time()
 
 # Shorthands
-out_dir = cfg['training']['out_dir']
+#out_dir = cfg['training']['out_dir']
+out_dir = os.path.join('out', cfg['data']['input_type'],
+                       os.path.basename(args.config).split('.')[0])
+cfg['training']['out_dir'] = out_dir
 batch_size = cfg['training']['batch_size']
 backup_every = cfg['training']['backup_every']
 exit_after = args.exit_after
@@ -62,8 +70,13 @@ if 'debug' in cfg['data']:
 else:
     train_shuffle = True
 
+if args.data_parallel:
+    dist_coef = 1
+else:
+    dist_coef = 1
+
 train_loader = torch.utils.data.DataLoader(train_dataset,
-                                           batch_size=batch_size,
+                                           batch_size=batch_size * dist_coef,
                                            num_workers=4,
                                            shuffle=train_shuffle,
                                            collate_fn=data.collate_remove_none,
@@ -72,37 +85,44 @@ train_loader = torch.utils.data.DataLoader(train_dataset,
 
 val_loader = torch.utils.data.DataLoader(
     val_dataset,
-    batch_size=cfg['training']['val_batch_size'],
+    batch_size=cfg['training']['val_batch_size'] * dist_coef,
     num_workers=4,
     shuffle=False,
     collate_fn=data.collate_remove_none,
     worker_init_fn=data.worker_init_fn)
 
 # For visualizations
-vis_loader = torch.utils.data.DataLoader(val_dataset,
-                                         batch_size=cfg['training'].get(
-                                             'vis_batch_size', 1),
-                                         shuffle=False,
-                                         collate_fn=data.collate_remove_none,
-                                         worker_init_fn=data.worker_init_fn)
+vis_loader = torch.utils.data.DataLoader(
+    val_dataset,
+    batch_size=cfg['training'].get('vis_batch_size', 1) * dist_coef,
+    shuffle=False,
+    collate_fn=data.collate_remove_none,
+    worker_init_fn=data.worker_init_fn)
 data_vis = next(iter(vis_loader))
 
 # Model
 model = config.get_model(cfg, device=device, dataset=train_dataset)
-
 # Intialize training
 npoints = 1000
-learning_rate = float(cfg['training'].get('learning_rate', 1e-4))
-optimizer = optim.Adam(model.parameters(), lr=learning_rate)
-# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)
-
-trainer = config.get_trainer(model, optimizer, cfg, device=device)
+learning_rate = float(cfg['training'].get('learning_rate', 1e-4)) * dist_coef
+if not args.data_parallel:
+    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
+    trainer = config.get_trainer(model, optimizer, cfg, device=device)
 
 if cfg['training'].get('skip_load_pretrained_optimizer', False):
     print('skip loading optimizer')
     checkpoint_io = CheckpointIO(out_dir, model=model)
-else:
+elif not args.data_parallel:
     checkpoint_io = CheckpointIO(out_dir, model=model, optimizer=optimizer)
+else:
+    assert args.data_parallel and not cfg['training'].get(
+        'skip_load_pretrained_optimizer', False)
+    checkpoint_io = CheckpointIO(out_dir, model=model)
+    model = torch.nn.DataParallel(model)
+    torch.backends.cudnn.benchmark = True
+    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
+    CheckpointIO(out_dir, optimizer=optimizer)
+    trainer = config.get_trainer(model, optimizer, cfg, device=device)
 try:
     load_dict = checkpoint_io.load('model.pt')
 except FileExistsError:
