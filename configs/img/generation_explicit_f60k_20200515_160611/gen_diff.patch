diff --git a/configs/default.yaml b/configs/default.yaml
index 919e7da..eaf76bf 100644
--- a/configs/default.yaml
+++ b/configs/default.yaml
@@ -62,12 +62,12 @@ test:
   eval_pointcloud: true
   eval_fscore: true
   fscore_thresholds:
-  - 0.005053668503213957
-  - 0.0107337006427915 # as bbox's side length is slightly bigger as normalization happens before meshing.
-  - 0.02021467401285583
-  - 0.050536685032139574
-  - 0.10107337006427915
-  - 0.2021467401285583
+  - 0.005
+  - 0.01
+  - 0.02
+  - 0.05
+  - 0.1
+  - 0.2
   model_file: model_best.pt
   n_points: 100000
   is_sample_from_surface: false
diff --git a/configs/img/atlasnetv2_pn30_debugged.yaml b/configs/img/atlasnetv2_pn30_debugged.yaml
index 893fef3..42f5a07 100644
--- a/configs/img/atlasnetv2_pn30_debugged.yaml
+++ b/configs/img/atlasnetv2_pn30_debugged.yaml
@@ -32,6 +32,7 @@ training:
   learning_rage_decay_at:
     - 250
     - 300
+  wandb_resume: ho0ih4zl
 trainer:
   point_scale: 2
 test:
diff --git a/configs/img/generation_explicit__20200503_222537/gen_diff.patch b/configs/img/generation_explicit__20200503_222537/gen_diff.patch
deleted file mode 100644
index f075b79..0000000
--- a/configs/img/generation_explicit__20200503_222537/gen_diff.patch
+++ /dev/null
@@ -1,3357 +0,0 @@
-diff --git a/README.md b/README.md
-index 4378f27..16febfd 100644
---- a/README.md
-+++ b/README.md
-@@ -1,169 +1,138 @@
--# Occupancy Networks
--![Example 1](img/00.gif)
--![Example 2](img/01.gif)
--![Example 3](img/02.gif)
-+# PSNet experiments
- 
--This repository contains the code to reproduce the results from the paper
--[Occupancy Networks - Learning 3D Reconstruction in Function Space](https://avg.is.tuebingen.mpg.de/publications/occupancy-networks).
-+## Directory structure
-+`external/<model of previous works>/`: Place model of external works. It needs to be modularized.
-+`out/submission/eval/`: Evaluation results are stored.
-+`paper_resources/`: Tex resources and their generation scripts are stored.
- 
--You can find detailed usage instructions for training your own models and using pretrained models below.
--
--If you find our code or paper useful, please consider citing
--
--    @inproceedings{Occupancy Networks,
--        title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
--        author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
--        booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
--        year = {2019}
--    }
--
--## Installation
--First you have to make sure that you have all dependencies in place.
--The simplest way to do so, is to use [anaconda](https://www.anaconda.com/). 
--
--You can create an anaconda environment called `mesh_funcspace` using
-+## Prerequiste
-+### venv
- ```
--conda env create -f environment.yaml
--conda activate mesh_funcspace
-+source env/bin/activate
- ```
--
--Next, compile the extension modules.
--You can do this via
-+### PYTHONPATH
- ```
--python setup.py build_ext --inplace
-+export PYTHONPATH=$PYTHONPATH:external/periodic_shapes:$PWD:external/atlasnetv2
- ```
- 
--To compile the dmc extension, you have to have a cuda enabled device set up.
--If you experience any errors, you can simply comment out the `dmc_*` dependencies in `setup.py`.
--You should then also comment out the `dmc` imports in `im2mesh/config.py`.
--
--## Demo
--![Example Input](img/example_input.png)
--![Example Output](img/example_output.gif)
--
--You can now test our code on the provided input images in the `demo` folder.
--To this end, simply run
-+## Training
-+### Single GPU
- ```
--python generate.py configs/demo.yaml
-+CUDA_VISIBLE_DEVICES=0 python3 train.py out/img/pnet.config
- ```
--This script should create a folder `demo/generation` where the output meshes are stored.
--The script will copy the inputs into the `demo/generation/inputs` folder and creates the meshes in the `demo/generation/meshes` folder.
--Moreover, the script creates a `demo/generation/vis` folder where both inputs and outputs are copied together.
--
--## Dataset
--
--To evaluate a pretrained model or train a new model from scratch, you have to obtain the dataset.
--To this end, there are two options:
--
--1. you can download our preprocessed data
--2. you can download the ShapeNet dataset and run the preprocessing pipeline yourself
--
--Take in mind that running the preprocessing pipeline yourself requires a substantial amount time and space on your hard drive.
--Unless you want to apply our method to a new dataset, we therefore recommmend to use the first option.
--
--### Preprocessed data
--You can download our preprocessed data (73.4 GB) using
--
-+### Multiple GPUs
- ```
--bash scripts/download_data.sh
-+CUDA_VISIBLE_DEVICES=0,1,2 python3 dist_train.py out/img/pnet.config
- ```
- 
--This script should download and unpack the data automatically into the `data/ShapeNet` folder.
--
--### Building the dataset
--Alternatively, you can also preprocess the dataset yourself.
--To this end, you have to follow the following steps:
--* download the [ShapeNet dataset v1](https://www.shapenet.org/) and put into `data/external/ShapeNet`. 
--* download the [renderings and voxelizations](http://3d-r2n2.stanford.edu/) from Choy et al. 2016 and unpack them in `data/external/Choy2016` 
--* build our modified version of [mesh-fusion](https://github.com/davidstutz/mesh-fusion) by following the instructions in the `external/mesh-fusion` folder
--
--You are now ready to build the dataset:
-+## Evaluation
-+### IoU
- ```
--cd scripts
--bash dataset_shapenet/build.sh
--``` 
--
--This command will build the dataset in `data/ShapeNet.build`.
--To install the dataset, run
-+CUDA_VISIBLE_DEVICES=0 python3 eval.py out/img/pnet.yamla <config overwrite options. e.g. --test.threshold 0.999999>
- ```
--bash dataset_shapenet/install.sh
-+This creates copied run directory `out/submission/eval/img/<out_dir>_<YYYYMMDD>`.
-+`<out_dir>` is written in config yaml.
-+For PSNet, `--test.threshold 0.999999` is usually used for IoU.
-+#### Generate IoU result in existing directory
-+Use `--dontcopy` option with config.yaml created by eval.py.
-+E.g.
- ```
-+CUDA_VISIBLE_DEVICES=0 python3 eval.py out/submission/eval/img/config.yaml
-+```
-+#### Generate IoU result of the run which doesn't have output dir under out/.
-+Use `--no_copy_but_create_new` option.
- 
--If everything worked out, this will copy the dataset into `data/ShapeNet`.
--
--## Usage
--When you have installed all binary dependencies and obtained the preprocessed data, you are ready to run our pretrained models and train new models from scratch.
--
--### Generation
--To generate meshes using a trained model, use
-+#### Generate IoU result for BSPNet
- ```
--python generate.py CONFIG.yaml
-+sh external/bspnet/scripts/eval_iou.sh <config path> <GPU id>
- ```
--where you replace `CONFIG.yaml` with the correct config file.
- 
--The easiest way is to use a pretrained model.
--You can do this by using one of the config files
--```
--configs/img/onet_pretrained.yaml
--configs/pointcloud/onet_pretrained.yaml
--configs/voxels/onet_pretrained.yaml
--configs/unconditional/onet_cars_pretrained.yaml
--configs/unconditional/onet_airplanes_pretrained.yaml
--configs/unconditional/onet_sofas_pretrained.yaml
--configs/unconditional/onet_chairs_pretrained.yaml
--```
--which correspond to the experiments presented in the paper.
--Our script will automatically download the model checkpoints and run the generation.
--You can find the outputs in the `out/*/*/pretrained` folders.
--
--Please note that the config files  `*_pretrained.yaml` are only for generation, not for training new models: when these configs are used for training, the model will be trained from scratch, but during inference our code will still use the pretrained model.
--
--### Evaluation
--For evaluation of the models, we provide two scripts: `eval.py` and `eval_meshes.py`.
-+### Generate Mesh
-+For PSNet and AtlasNet V2,
-+```
-+sh im2mesh/<pnet or atlasnetv2>/script/generate_mesh.sh out/submission/eval/img/<out_dir>_<YYYYMMDD>/config.yaml
-+```
-+This automatically creates config yaml:
-+```
-+im2mesh/<pnet or atlasnetv2>/script/generate_mesh.sh out/submission/eval/img/<out_dir>_<YYYYMMDD>/gen_***_<yyyymmdd>.yaml <GPU id>
-+```
- 
--The main evaluation script is `eval_meshes.py`.
--You can run it using
-+### Fscore
-+Use generated config file by mesh generation.
- ```
--python eval_meshes.py CONFIG.yaml
-+CUDA_VISIBLE_DEVICES=0 python3 eval_fscore.py out/submission/eval/img/<out_dir>_<YYYYMMDD>/gen_***_<yyyymmdd>.yaml
- ```
--The script takes the meshes generated in the previous step and evaluates them using a standardized protocol.
--The output will be written to `.pkl`/`.csv` files in the corresponding generation folder which can be processed using [pandas](https://pandas.pydata.org/).
- 
--For a quick evaluation, you can also run
-+### Chamfer distance, normal consistency
-+Use generated config file by mesh generation.
- ```
--python eval.py CONFIG.yaml
-+CUDA_VISIBLE_DEVICES=0 python3 eval_mesh.py out/submission/eval/img/<out_dir>_<YYYYMMDD>/gen_***_<yyyymmdd>.yaml
- ```
--This script will run a fast method specific evaluation to obtain some basic quantities that can be easily computed without extracting the meshes.
--This evaluation will also be conducted automatically on the validation set during training.
- 
--All results reported in the paper were obtained using the `eval_meshes.py` script.
-+## Experiment specific
-+All paper related scripts are in `paper_resources/`.
-+Generated resources for tex are stored in `paper_resources/<experiment>/resoureces`.
-+Some experiments requires to run IoU and fscore evaluation before the experiment.
-+Scripts for generating resources are stored in `paper_resources/<experiment>/scripts`.
-+### Mesh quality comparison
-+Under `paper_resources/compare_mesh_methods`.
-+Run `paper_resources/compare_mesh_methods/scripts/mesh_quality_comparison.ipynb`.
- 
--### Training
--Finally, to train a new network from scratch, run
-+### SVR Pix3D evaluation
-+Under `paper_resources/pix3d_comparison`
-+Fist, You need to create OccNet experiment environment compatible Pix3D dataset.
-+Run scripts in following steps to create the Dataset.
-+1. Compute stats of ShapeNet renderings margin.
- ```
--python train.py CONFIG.yaml
-+python3 paper_resources/pix3d_comparison/scripts/calc_stats_of_margin_of_rendering.py
- ```
--where you replace `CONFIG.yaml` with the name of the configuration file you want to use.
--
--You can monitor on <http://localhost:6006> the training process using [tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard):
-+2. Create the compatible renderings of Pix3D.
-+```
-+python3 paper_resources/pix3d_comparison/scripts/convert_pix3d_images.py
-+```
-+3. Create the surface mesh.
-+```
-+python3 paper_resources/pix3d_comparison/scripts/convert_pix3d_models.py
- ```
--cd OUTPUT_DIR
--tensorboard --logdir ./logs --port 6006
-+4. Create sample lists.
- ```
--where you replace `OUTPUT_DIR` with the respective output directory.
-+python3 paper_resources/pix3d_comparison/scripts/generate_list.py
-+```
-+After finising Pix3D dataset conversion, generate resoureces.
-+1. Generate mesh for Pix3D for PSNet (for atlasnetv2, TBD)
-+```
-+sh paper_resources/pix3d_comparison/scripts/generate_explicit_pix3d_mesh.sh  out/submission/eval/img/<out_dir>_<YYYYMMDD>/config.yaml <GPU id>
-+```
-+3. Generate Fscore
-+2. Generate resoureces
-+```
-+CUDA_VISIBLE_DEVICES=0 python3 paper_resources/pix3d_comparison/scripts/generate_pix3d_table_and_generate_renderings.py
-+```
-+### SVR ShapeNet evaluation.
-+Under `paper_resources/shapenet_svr_comparison`.
-+1. Generate IoU, fscore, CD1 metrics.
-+2. Prep csv tables of metrics of previous works under `paper_resources/shapenet_svr_comparison/csv`.
-+3. Run `paper_resources/shapenet_svr_comparison/scripts/generate_svr_table.ipynb`.
-+4. Tex table is generated in `paper_resources/shapenet_svr_comparison/table.txt`.
- 
--For available training options, please take a look at `configs/default.yaml`.
-+### Primitive visualization
-+This experiment render primitives and evaluate part semseg.
-+For visualize primitives,
-+```
-+CUDA_VISIBLE_DEVICES=0 python3 paper_resources/primitive_visualization/scripts/render_primitives.py
-+```
-+For evaluate part semseg,
-+1. Generate semseg data.
-+```
-+python3 paper_resources/primitive_visualization/scripts/generate_semseg_data.py
-+``` 
-+2. TBD
- 
--# Notes
--* In our paper we used random crops and scaling to augment the input images. 
--  However, we later found that this image augmentation decreases performance on the ShapeNet test set.
--  The pretrained model that is loaded in `configs/img/onet_pretrained.yaml` was hence trained without data augmentation and has slightly better performance than the model from the paper. The updated table looks a follows:
--  ![Updated table for single view 3D reconstruction experiment](img/table_img2mesh.png)
--  For completeness, we also provide the trained weights for the model which was used in the paper in  `configs/img/onet_legacy_pretrained.yaml`.
--* Note that training and evaluation of both our model and the baselines is performed with respect to the *watertight models*, but that normalization into the unit cube is performed with respect to the *non-watertight meshes* (to be consistent with the voxelizations from Choy et al.). As a result, the bounding box of the sampled point cloud is usually slightly bigger than the unit cube and may differ a little bit from a point cloud that was sampled from the original ShapeNet mesh.
-+## Other notes
-+### Rendering
-+Rendering script is in `/home/mil/kawana/workspace/RenderForCNN`.
-+Wrapper for this project is in `scripts/render_3dobj.sh`.
- 
--# Futher Information
--Please also check out the following concurrent papers that have proposed similar ideas:
--* [Park et al. - DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation (2019)](https://arxiv.org/abs/1901.05103)
--* [Chen et al. - Learning Implicit Fields for Generative Shape Modeling (2019)](https://arxiv.org/abs/1812.02822)
--* [Michalkiewicz et al. - Deep Level Sets: Implicit Surface Representations for 3D Shape Inference (2019)](https://arxiv.org/abs/1901.06802)
-+### Visualization
-+Simple mesh visualization by trimesh, see `scripts/show_mesh.py`.
-+Point cloud visualization by plotly, see `external/periodic_shapes/periodic_shapes/visualize/plot.py`.
-diff --git a/configs/default.yaml b/configs/default.yaml
-index 52c7f35..919e7da 100644
---- a/configs/default.yaml
-+++ b/configs/default.yaml
-@@ -61,8 +61,17 @@ test:
-   eval_mesh: true
-   eval_pointcloud: true
-   eval_fscore: true
--  fscore_thresholds: [0.005, 0.01, 0.0107337006427915, 0.02, 0.05, 0.1, 0.2]
-+  fscore_thresholds:
-+  - 0.005053668503213957
-+  - 0.0107337006427915 # as bbox's side length is slightly bigger as normalization happens before meshing.
-+  - 0.02021467401285583
-+  - 0.050536685032139574
-+  - 0.10107337006427915
-+  - 0.2021467401285583
-   model_file: model_best.pt
-+  n_points: 100000
-+  is_sample_from_surface: false
-+  is_normalize_by_side_length: false
- generation:
-   batch_size: 100000
-   refinement_step: 0
-diff --git a/configs/img/debug_atlasnetv2.yaml b/configs/img/debug_atlasnetv2.yaml
-index 2906cdc..9a3bfe3 100644
---- a/configs/img/debug_atlasnetv2.yaml
-+++ b/configs/img/debug_atlasnetv2.yaml
-@@ -16,14 +16,15 @@ model:
-   encoder_latent: null
-   decoder: atlasnetv2decoder 
-   encoder: resnet18
--  c_dim: 1024
-+  c_dim: 256
-   z_dim: 0
-   decoder_kwargs:
-     npatch: 10
-     patchDim: 2
-     patchDeformDim: 3
-+    hidden_size: 256
- training:
--  out_dir:  out/img/debug_atlasnetv2-2
-+  out_dir:  out/img/debug_atlasnetv2
-   batch_size: 20
-   val_batch_size: 2
-   model_selection_metric: cd1
-@@ -36,6 +37,8 @@ training:
-   learning_rage_decay_at:
-     - 250
-     - 300
-+trainer:
-+  point_scale: 2
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/debug_atlasnetv2_pointnet.yaml b/configs/img/debug_atlasnetv2_pointnet.yaml
-deleted file mode 100644
-index 9fda6fa..0000000
---- a/configs/img/debug_atlasnetv2_pointnet.yaml
-+++ /dev/null
-@@ -1,56 +0,0 @@
--method: atlasnetv2
--data:
--  path: data/ShapeNet
--  train_split: train # Overfit to train data, see if training is working
--  val_split: train
--  test_split: train
--  classes: ['02691156'] # plane
--  img_folder: img_choy2016
--  img_size: 224 
--  input_type: pointcloud
--  pointcloud_n: 2500
--  #pointcloud_noise: 0.000
--  pointcloud_target_n: 4096
--  patch_side_length: 16 # will be powered by dim - 1, if dim == 3, then 900
--  is_generate_mesh: false
--  debug:
--    sample_n: 50
--model:
--  encoder_latent: null
--  decoder: atlasnetv2decoder 
--  encoder: pointnet_atlasnetv2
--  encoder_kwargs:
--    hidden_dim: 1024
--  c_dim: 2500
--  z_dim: 0
--  decoder_kwargs:
--    npatch: 10
--    patchDim: 2
--    patchDeformDim: 3
--    hidden_size: 1024
--training:
--  out_dir:  out/img/debug_atlasnetv2_pointnet
--  batch_size: 10
--  val_batch_size: 2
--  model_selection_metric: cd1
--  model_selection_mode: minimize
--  visualize_every: 500
--  validate_every: 1000
--  checkpoint_every: 100000
--  skip_load_pretrained_optimizer: false
--  learning_rate: 1e-3
--  learning_rage_decay_at:
--    - 250
--    - 300
--test:
--  threshold: 0.5
--  eval_mesh: true
--  eval_pointcloud: false
--generation:
--  batch_size: 100000
--  refine: false
--  n_x: 128
--  n_z: 1
--  resolution_0: 32 
--  upsampling_steps: 2
--
-diff --git a/configs/img/debug_onet.yaml b/configs/img/debug_onet.yaml
-index b444c37..1a68a93 100644
---- a/configs/img/debug_onet.yaml
-+++ b/configs/img/debug_onet.yaml
-@@ -1,3 +1,5 @@
- inherit_from: configs/img/onet.yaml
- data:
-   classes: ['02691156'] # plane
-+training:
-+    out_dir: out/img/debug_onet_eval
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml
-index 3503f68..ffe24c1 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_cceff0.yaml
-@@ -42,6 +42,7 @@ trainer:
-   self_overlap_reg_threshold: 0.1
-   is_normal_loss: false
-   normal_loss_coef: 3
-+  is_cvx_net_merged_loss: true
-   cvx_net_merged_loss_coef: 100
-   cvx_net_merged_loss_topk_samples: 10
- training:
-@@ -55,6 +56,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: cffp5ly9
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml
-index 99ac22c..b188ba7 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: qguu0ndb
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml
-index 95164fe..3ae1e1d 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: biibxzh2
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
-index 3455101..44242d0 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: o8pj91rl
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml
-index c049e61..25f8f2c 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn4_target_n_4096_no_overlap_reg.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: 3cagj5tz
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml
-index 6354414..7c32328 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn9_target_n_4096_no_overlap_reg.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: dqavijvk
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/eval.py b/eval.py
-index 4f6589e..8a29102 100644
---- a/eval.py
-+++ b/eval.py
-@@ -12,6 +12,10 @@ from im2mesh.checkpoints import CheckpointIO
- import shutil
- import yaml
- from collections import OrderedDict
-+import eval_utils
-+from datetime import datetime
-+
-+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
- 
- 
- def represent_odict(dumper, instance):
-@@ -30,57 +34,77 @@ yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
- parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
-+parser.add_argument('--dontcopy', action='store_true', help='Do not use cuda.')
-+parser.add_argument('--no_copy_but_create_new',
-+                    action='store_true',
-+                    help='Do not use cuda.')
-+parser.add_argument('--use_config_in_eval_dir',
-+                    action='store_true',
-+                    help='Do not use cuda.')
- 
- # Get configuration and basic arguments
- args, unknown_args = parser.parse_known_args()
- cfg = config.load_config(args.config, 'configs/default.yaml')
--for idx, arg in enumerate(unknown_args):
--    if arg.startswith('--'):
--        arg = arg.replace('--', '')
--        value = unknown_args[idx + 1]
--        keys = arg.split('.')
--        if keys[0] not in cfg:
--            cfg[keys[0]] = {}
--        child_cfg = cfg.get(keys[0], {})
--        for key in keys[1:]:
--            item = child_cfg.get(key, None)
--            if isinstance(item, dict):
--                child_cfg = item
--            elif item is None:
--                child_cfg[key] = value
--            else:
--                child_cfg[key] = type(item)(value)
-+
-+eval_utils.update_dict_with_options(cfg, unknown_args)
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
- # Shorthands
--if '--dontcopy' in unknown_args:
-+
-+if args.dontcopy:
-     out_dir = cfg['training']['out_dir']
-+elif args.use_config_in_eval_dir:
-+    out_dir = os.path.dirname(args.config)
- else:
-+    out_dir = os.path.join('out', cfg['data']['input_type'],
-+                           os.path.basename(args.config).split('.')[0])
-+    cfg['training']['out_dir'] = out_dir
-     base_out_dir = cfg['training']['out_dir']
-     out_dir = os.path.join(
-         os.path.dirname(base_out_dir).replace('out', 'out/submission/eval'),
-         os.path.basename(base_out_dir)) + '_' + datetime.now().strftime(
-             ('%Y%m%d_%H%M%S'))
- print('out dir for eval: ', out_dir)
--if not '--dontcopy' in unknown_args:
-+if not (args.dontcopy or args.use_config_in_eval_dir):
-     if not os.path.exists(out_dir):
--        shutil.copytree(base_out_dir, out_dir)
-+        if args.no_copy_but_create_new:
-+            os.makedirs(out_dir)
-+        else:
-+            #shutil.copytree(base_out_dir, out_dir)
-+            os.makedirs(out_dir)
-+            best_file = cfg['test']['model_file']
-+            best_path = os.path.join(base_out_dir, best_file)
-+            shutil.copy2(best_path, out_dir)
-     else:
-         raise ValueError('out dir already exists')
- 
--if not '--dontcopy' in unknown_args:
-+threshold_txt_path = os.path.join(out_dir, 'threshold')
-+if os.path.exists(threshold_txt_path):
-+    with open(threshold_txt_path) as f:
-+        threshold = float(f.readlines()[0].strip())
-+        print('Use threshold in dir', threshold)
-+        cfg['test']['threshold'] = threshold
-+
-+if not (args.dontcopy or args.use_config_in_eval_dir):
-     patch_path = os.path.join(out_dir, 'diff.patch')
-     subprocess.run('git diff > {}'.format(patch_path), shell=True)
--    weight_path = os.path.join(out_dir, cfg['test']['model_file'])
--    with open(weight_path, 'rb') as f:
--        md5 = hashlib.md5(f.read()).hexdigest()
--    cfg['test']['model_file_hash'] = md5
--    yaml.dump(cfg, open(os.path.join(out_dir, 'config.yaml'), 'w'))
-+    if not cfg['test']['model_file'].startswith('http'):
-+        weight_path = os.path.join(out_dir, cfg['test']['model_file'])
-+        with open(weight_path, 'rb') as f:
-+            md5 = hashlib.md5(f.read()).hexdigest()
-+        cfg['test']['model_file_hash'] = md5
-+yaml.dump(
-+    cfg,
-+    open(os.path.join(out_dir, 'eval_config_{}.yaml'.format(date_str)), 'w'))
- 
- out_file = os.path.join(out_dir, 'eval_full.pkl')
- out_file_class = os.path.join(out_dir, 'eval.csv')
- 
-+if (args.dontcopy or args.use_config_in_eval_dir):
-+    t = datetime.now().strftime('%Y%m%d_%H%M%S')
-+    out_file = out_file.replace('.pkl', '_' + t + '.pkl')
-+    out_file_class = out_file.replace('.csv', '_' + t + '.csv')
- # Dataset
- dataset = config.get_dataset('test', cfg, return_idx=True)
- model = config.get_model(cfg, device=device, dataset=dataset)
-@@ -97,7 +121,6 @@ trainer = config.get_trainer(model, None, cfg, device=device)
- 
- # Print model
- nparameters = sum(p.numel() for p in model.parameters())
--print(model)
- print('Total number of parameters: %d' % nparameters)
- 
- # Evaluate
-@@ -139,17 +162,6 @@ for it, data in enumerate(tqdm(test_loader)):
-         'modelname': modelname,
-     }
- 
--    inputs = data.get('inputs', torch.empty(points.size(0), 0)).to(device)
--    angles = data.get('angles').to(device)
--    points = data.get('points').to(device)
--
--    feature = model.encode_inputs(inputs)
--
--    kwargs = {}
--    scaled_coord = points * cfg['trainer']['pnet_point_scale']
--    output = model.decode(scaled_coord, None, feature, angles=angles, **kwargs)
--    super_shape_point, surface_mask, sgn, sgn_BxNxNP, radius = output
--
-     eval_dicts.append(eval_dict)
-     eval_data = trainer.eval_step(data)
-     eval_dict.update(eval_data)
-diff --git a/eval_fscore.py b/eval_fscore.py
-index 593d769..de3545c 100644
---- a/eval_fscore.py
-+++ b/eval_fscore.py
-@@ -10,6 +10,10 @@ from im2mesh import config, data
- from im2mesh.eval import MeshEvaluator
- from im2mesh.utils.io import load_pointcloud
- import numpy as np
-+from datetime import datetime
-+import yaml
-+import eval_utils
-+
- parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
-@@ -17,8 +21,17 @@ parser.add_argument('--eval_input',
-                     action='store_true',
-                     help='Evaluate inputs instead.')
- 
--args = parser.parse_args()
-+parser.add_argument('--unique_name',
-+                    default='',
-+                    type=str,
-+                    help='String name for generation.')
-+args, unknown_args = parser.parse_known_args()
- cfg = config.load_config(args.config, 'configs/default.yaml')
-+eval_utils.update_dict_with_options(cfg, unknown_args)
-+
-+if cfg['test']['is_normalize_by_side_length']:
-+    cfg['test']['fscore_thresholds'] = [0.005, 0.01, 0.02, 0.1, 0.2]
-+
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
-@@ -26,22 +39,27 @@ is_eval_explicit_mesh = cfg['test'].get('is_eval_explicit_mesh', False)
- # Shorthands
- #out_dir = cfg['training']['out_dir']
- out_dir = os.path.dirname(args.config)
--generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
-+generation_dir = os.path.dirname(args.config)
-+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
-+assert generation_dir.endswith(cfg['generation']['generation_dir'])
- if not args.eval_input:
-     out_file = os.path.join(
--        generation_dir, 'eval_fscore_from_meshes_full{}.pkl'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_fscore_from_meshes_full_{}_{}_{}.pkl'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
-     out_file_class = os.path.join(
--        generation_dir, 'eval_fscore_from_meshes{}.csv'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_fscore_from_meshes_{}_{}_{}.csv'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
- else:
-     out_file = os.path.join(
--        generation_dir, 'eval_fscore_from_input_full{}.pkl'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_fscore_from_input_full_{}_{}_{}.pkl'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
-     out_file_class = os.path.join(
--        generation_dir, 'eval_fscore_from_meshes_input{}.csv'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
--
-+        generation_dir, 'eval_fscore_from_meshes_input_{}_{}_{}.csv'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
- # Dataset
- points_field = data.PointsField(
-     cfg['data']['points_iou_file'],
-@@ -66,8 +84,19 @@ if 'debug' in cfg['data']:
-     dataset = torch_data.Subset(dataset,
-                                 range(cfg['data']['debug']['sample_n']))
- 
-+yaml.dump(
-+    cfg,
-+    open(
-+        os.path.join(
-+            generation_dir,
-+            'fscore_config_{}_{}.yaml'.format(args.unique_name, date_str)),
-+        'w'))
-+
- # Evaluator
--evaluator = MeshEvaluator(n_points=100000)
-+evaluator = MeshEvaluator(
-+    n_points=cfg['test']['n_points'],
-+    is_sample_from_surface=cfg['test']['is_sample_from_surface'],
-+    is_normalize_by_side_length=cfg['test']['is_normalize_by_side_length'])
- 
- # Loader
- test_loader = torch.utils.data.DataLoader(dataset,
-@@ -129,30 +158,67 @@ for it, data in enumerate(tqdm(test_loader)):
- 
-     # Evaluate mesh
-     if cfg['test']['eval_fscore']:
--        mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+        vertex_visibility = None
-+        if cfg['method'] == 'pnet':
-+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
- 
--        if is_eval_explicit_mesh:
--            visbility_file = os.path.join(
--                mesh_dir, '%s_vertex_visbility.npz' % modelname)
--        if os.path.exists(mesh_file):
--            mesh = trimesh.load(mesh_file, process=False)
-+            if os.path.exists(mesh_file):
-+                mesh = trimesh.load(mesh_file, process=False)
-+            else:
-+                print('Warning: mesh file does not exist: %s' % mesh_file)
-+                continue
-             if is_eval_explicit_mesh:
--                vertex_visibility = np.load(
--                    visbility_file)['vertex_visibility']
-+                visbility_file = os.path.join(
-+                    mesh_dir, '%s_vertex_visbility.npz' % modelname)
-+                if os.path.exists(visbility_file):
-+                    vertex_visibility = np.load(
-+                        visbility_file)['vertex_visibility']
-+                else:
-+                    print('Warning: vibility file does not exist: %s' %
-+                          visbility_file)
-+                    continue
-+        elif cfg['method'] == 'bspnet':
-+            vertex_file = os.path.join(mesh_dir,
-+                                       '%s_vertex_attributes.npz' % modelname)
-+            is_eval_explicit_mesh = True
-+            if os.path.exists(vertex_file):
-+                try:
-+                    vertex_attributes = np.load(vertex_file)
-+                except:
-+                    print('Error in bspnet loading vertex')
-+                    continue
-+                mesh = trimesh.Trimesh(vertex_attributes['vertices'])
-+                vertex_visibility = vertex_attributes['vertex_visibility']
-+            else:
-+                print('Warning: vertex file does not exist: %s' %
-+                      visbility_file)
-+                continue
-+        elif cfg['method'] == 'atlasnetv2':
-+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+
-+            if os.path.exists(mesh_file):
-+                mesh = trimesh.load(mesh_file, process=False)
-             else:
--                vertex_visibility = None
--
--            eval_dict_mesh = evaluator.eval_fscore_from_mesh(
--                mesh,
--                pointcloud_tgt,
--                cfg['test']['fscore_thresholds'],
--                is_eval_explicit_mesh=is_eval_explicit_mesh,
--                vertex_visibility=vertex_visibility)
--            if eval_dict_mesh is not None:
--                for k, v in eval_dict_mesh.items():
--                    eval_dict[k + ' (mesh)'] = v
-+                print('Warning: mesh file does not exist: %s' % mesh_file)
-+                continue
-         else:
--            print('Warning: mesh does not exist: %s' % mesh_file)
-+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+
-+            if os.path.exists(mesh_file):
-+                mesh = trimesh.load(mesh_file, process=False)
-+            else:
-+                print('Warning: mesh file does not exist: %s' % mesh_file)
-+                continue
-+
-+        eval_dict_mesh = evaluator.eval_fscore_from_mesh(
-+            mesh,
-+            pointcloud_tgt,
-+            cfg['test']['fscore_thresholds'],
-+            is_eval_explicit_mesh=is_eval_explicit_mesh,
-+            vertex_visibility=vertex_visibility)
-+        if eval_dict_mesh is not None:
-+            for k, v in eval_dict_mesh.items():
-+                eval_dict[k + ' (mesh)'] = v
- 
- # Create pandas dataframe and save
- eval_df = pd.DataFrame(eval_dicts)
-diff --git a/eval_meshes.py b/eval_meshes.py
-index 44e5d88..2289e02 100644
---- a/eval_meshes.py
-+++ b/eval_meshes.py
-@@ -10,6 +10,10 @@ from im2mesh import config, data
- from im2mesh.eval import MeshEvaluator
- from im2mesh.utils.io import load_pointcloud
- import numpy as np
-+from datetime import datetime
-+import yaml
-+import eval_utils
-+
- parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
-@@ -17,31 +21,42 @@ parser.add_argument('--eval_input',
-                     action='store_true',
-                     help='Evaluate inputs instead.')
- 
--args = parser.parse_args()
-+parser.add_argument('--unique_name',
-+                    default='',
-+                    type=str,
-+                    help='String name for generation.')
-+args, unknown_args = parser.parse_known_args()
- cfg = config.load_config(args.config, 'configs/default.yaml')
-+eval_utils.update_dict_with_options(cfg, unknown_args)
-+
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
- is_eval_explicit_mesh = cfg['test'].get('is_eval_explicit_mesh', False)
--
- # Shorthands
-+
- out_dir = os.path.dirname(args.config)
--#out_dir = cfg['training']['out_dir']
--generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
-+generation_dir = os.path.dirname(args.config)
-+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
-+assert generation_dir.endswith(cfg['generation']['generation_dir'])
- if not args.eval_input:
-     out_file = os.path.join(
--        generation_dir, 'eval_meshes_full{}.pkl'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_meshes_full_{}_{}_{}.pkl'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
-     out_file_class = os.path.join(
--        generation_dir, 'eval_meshes{}.csv'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_meshes_{}_{}_{}.csv'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
- else:
-     out_file = os.path.join(
--        generation_dir, 'eval_input_full{}.pkl'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_input_full_{}_{}_{}.pkl'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
-     out_file_class = os.path.join(
--        generation_dir, 'eval_input{}.csv'.format(
--            '_explicit' if is_eval_explicit_mesh else ''))
-+        generation_dir, 'eval_input_{}_{}_{}.csv'.format(
-+            args.unique_name, '_explicit' if is_eval_explicit_mesh else '',
-+            date_str))
- 
- # Dataset
- points_field = data.PointsField(
-@@ -66,9 +81,19 @@ dataset = data.Shapes3dDataset(dataset_folder,
- if 'debug' in cfg['data']:
-     dataset = torch_data.Subset(dataset,
-                                 range(cfg['data']['debug']['sample_n']))
-+yaml.dump(
-+    cfg,
-+    open(
-+        os.path.join(
-+            generation_dir,
-+            'eval_mesh_config_{}_{}.yaml'.format(args.unique_name, date_str)),
-+        'w'))
- 
- # Evaluator
--evaluator = MeshEvaluator(n_points=100000)
-+evaluator = MeshEvaluator(
-+    n_points=cfg['test']['n_points'],
-+    is_sample_from_surface=cfg['test']['is_sample_from_surface'],
-+    is_normalize_by_side_length=cfg['test']['is_normalize_by_side_length'])
- 
- # Loader
- test_loader = torch.utils.data.DataLoader(dataset,
-@@ -130,30 +155,73 @@ for it, data in enumerate(tqdm(test_loader)):
- 
-     # Evaluate mesh
-     if cfg['test']['eval_mesh']:
--        mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
- 
--        if is_eval_explicit_mesh:
--            visbility_file = os.path.join(
--                mesh_dir, '%s_vertex_visbility.npz' % modelname)
--        if os.path.exists(mesh_file):
--            mesh = trimesh.load(mesh_file, process=False)
-+        vertex_visibility = None
-+        if cfg['method'] == 'pnet':
-+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+
-+            if os.path.exists(mesh_file):
-+                mesh = trimesh.load(mesh_file, process=False)
-+            else:
-+                print('Warning: mesh file does not exist: %s' % mesh_file)
-+                continue
-             if is_eval_explicit_mesh:
--                vertex_visibility = np.load(
--                    visbility_file)['vertex_visibility']
-+                visbility_file = os.path.join(
-+                    mesh_dir, '%s_vertex_visbility.npz' % modelname)
-+                if os.path.exists(visbility_file):
-+                    vertex_visibility = np.load(
-+                        visbility_file)['vertex_visibility']
-+                else:
-+                    print('Warning: vibility file does not exist: %s' %
-+                          visbility_file)
-+                    continue
-+        elif cfg['method'] == 'bspnet':
-+            vertex_file = os.path.join(mesh_dir,
-+                                       '%s_vertex_attributes.npz' % modelname)
-+            is_eval_explicit_mesh = True
-+            if os.path.exists(vertex_file):
-+                try:
-+                    vertex_attributes = np.load(vertex_file)
-+                except:
-+                    print('Error in bspnet loading vertex')
-+                    continue
-+                mesh = trimesh.Trimesh(
-+                    vertex_attributes['vertices'],
-+                    vertex_normals=vertex_attributes['normals'])
-+                vertex_visibility = vertex_attributes['vertex_visibility']
-+            else:
-+                print('Warning: vertex file does not exist: %s' %
-+                      visbility_file)
-+                continue
-+        elif cfg['method'] == 'atlasnetv2':
-+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+            is_eval_explicit_mesh = False
-+
-+            if os.path.exists(mesh_file):
-+                mesh = trimesh.load(mesh_file, process=False)
-             else:
--                vertex_visibility = None
--            eval_dict_mesh = evaluator.eval_mesh(
--                mesh,
--                pointcloud_tgt,
--                normals_tgt,
--                points_tgt,
--                occ_tgt,
--                is_eval_explicit_mesh=is_eval_explicit_mesh,
--                vertex_visibility=vertex_visibility)
--            for k, v in eval_dict_mesh.items():
--                eval_dict[k + ' (mesh)'] = v
-+                print('Warning: mesh file does not exist: %s' % mesh_file)
-+                continue
-         else:
--            print('Warning: mesh does not exist: %s' % mesh_file)
-+            mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+
-+            if os.path.exists(mesh_file):
-+                mesh = trimesh.load(mesh_file, process=False)
-+            else:
-+                print('Warning: mesh file does not exist: %s' % mesh_file)
-+                continue
-+
-+        eval_dict_mesh = evaluator.eval_mesh(
-+            mesh,
-+            pointcloud_tgt,
-+            normals_tgt,
-+            points_tgt,
-+            occ_tgt,
-+            is_eval_explicit_mesh=is_eval_explicit_mesh,
-+            vertex_visibility=vertex_visibility,
-+            skip_iou=(cfg['method'] == 'atlasnetv2'))
-+        for k, v in eval_dict_mesh.items():
-+            eval_dict[k + ' (mesh)'] = v
- 
-     # Evaluate point cloud
-     if cfg['test']['eval_pointcloud']:
-diff --git a/external/atlasnetv2/atlasnetv2 b/external/atlasnetv2/atlasnetv2
---- a/external/atlasnetv2/atlasnetv2
-+++ b/external/atlasnetv2/atlasnetv2
-@@ -1 +1 @@
--Subproject commit 09739b6328f969a6de084376696a5b57890494e8
-+Subproject commit 09739b6328f969a6de084376696a5b57890494e8-dirty
-diff --git a/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py b/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py
-index bf2958a..6eb8fc2 100644
---- a/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py
-+++ b/external/periodic_shapes/periodic_shapes/layers/primitive_wise_layers.py
-@@ -79,6 +79,5 @@ class PrimitiveWiseLinear(nn.Module):
-         """
- 
-         B, N, D, P = input_data.shape
--        return self.main(input_data.view(B, N * D,
--                                         P)).view(B, N, self.output_channels,
--                                                  P)
-+        out = self.main(input_data.view(B, N * D, P))
-+        return out.view(B, N, self.output_channels, P)
-diff --git a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
-index 89672aa..a17e6c0 100644
---- a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
-+++ b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
-@@ -14,6 +14,7 @@ class BaseShapeSampler(nn.Module):
-                  learn_pose=True,
-                  linear_scaling=True,
-                  disable_learn_pose_but_transition=False,
-+                 extract_surface_point_by_max=False,
-                  dim=2):
-         """Intitialize SuperShapeSampler.
- 
-@@ -28,6 +29,7 @@ class BaseShapeSampler(nn.Module):
-         self.learn_pose = learn_pose
-         self.linear_scaling = linear_scaling
-         self.disable_learn_pose_but_transition = disable_learn_pose_but_transition
-+        self.extract_surface_point_by_max = extract_surface_point_by_max
- 
-         self.dim = dim
-         if not self.dim in [2, 3]:
-@@ -276,9 +278,14 @@ class BaseShapeSampler(nn.Module):
-         P = NP // N
-         output_sgn = output_sgn_BxNxNP.view(B, self.n_primitives,
-                                             self.n_primitives, P)
--        sgn_p_BxPsN = nn.functional.relu(output_sgn).sum(1).view(
--            B, self.n_primitives, P)
--        surface_mask = (sgn_p_BxPsN <= 1e-1)
-+        if self.extract_surface_point_by_max:
-+            sgn_p_BxPsN = torch.relu(output_sgn.max(1)[0]).view(
-+                B, self.n_primitives, P)
-+            surface_mask = (sgn_p_BxPsN <= 1e-4)
-+        else:
-+            sgn_p_BxPsN = nn.functional.relu(output_sgn).sum(1).view(
-+                B, self.n_primitives, P)
-+            surface_mask = (sgn_p_BxPsN <= 1e-1)
-         return surface_mask
- 
-     def extract_surface_point_std(self, super_shape_point, primitive_params,
-diff --git a/external/periodic_shapes/periodic_shapes/models/decoder.py b/external/periodic_shapes/periodic_shapes/models/decoder.py
-index a1dc597..028645d 100644
---- a/external/periodic_shapes/periodic_shapes/models/decoder.py
-+++ b/external/periodic_shapes/periodic_shapes/models/decoder.py
-@@ -118,7 +118,6 @@ class ShapeDecoderCBatchNorm(nn.Module):
-                           P).transpose(2, 3).contiguous()
- 
-         # B, n_primitives, P, self.label_num = 1
--        print('radius', radius.mean().item())
-         return radius
- 
- 
-diff --git a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
-index 084e5da..ccf971e 100644
---- a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
-+++ b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
-@@ -26,12 +26,14 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-                  is_feature_radius=True,
-                  no_last_bias=False,
-                  return_sdf=False,
-+                 is_infer_r1r2=False,
-                  **kwargs):
-         super().__init__(*args, **kwargs)
-         self.clamp = True
-         self.factor = factor
-         self.num_points = num_points
--        self.num_labels = 1  # Only infer r2 for 3D
-+        self.num_labels = (1 if not is_infer_r1r2 or self.dim == 2 else 2
-+                           )  # Only infer r2 for 3D
-         self.theta_dim = 2 if self.dim == 2 else 4
-         self.last_scale = last_scale
- 
-@@ -40,6 +42,9 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-         self.is_shape_sampler_sphere = is_shape_sampler_sphere
-         self.spherical_angles = spherical_angles
-         self.return_sdf = return_sdf
-+        self.is_infer_r1r2 = is_infer_r1r2
-+        if self.is_infer_r1r2:
-+            assert not self.is_shape_sampler_sphere and not self.spherical_angles
- 
-         c64 = 64 // self.factor
-         self.encoder_dim = c64 * 2
-@@ -101,6 +106,9 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-             #print('mean r1 in points', r[..., 0].mean())
-             final_r[..., 0] = r[..., 0] + periodic_net_r.squeeze(-1)
-             #print('mean final r in points', final_r[..., 0].mean())
-+
-+        elif self.is_infer_r1r2:
-+            final_r = r + periodic_net_r
-         else:
-             #print('mean r1 in points', r[..., -1].mean())
-             final_r[..., -1] = r[..., -1] + periodic_net_r.squeeze(-1)
-@@ -110,11 +118,11 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-             final_r = final_r.clamp(min=EPS)
-         else:
-             final_r = torch.relu(final_r) + EPS
--        print('r in ss stats',
--              final_r.mean().item(),
--              final_r.max().item(),
--              final_r.min().item(),
--              final_r.std().item())
-+        #print('r in ss stats',
-+        #      final_r.mean().item(),
-+        #      final_r.max().item(),
-+        #      final_r.min().item(),
-+        #      final_r.std().item())
- 
-         # B, n_primitives, P, dim
-         if self.is_shape_sampler_sphere and self.spherical_angles:
-@@ -221,11 +229,21 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
- 
-         else:
-             if is3d:
--                r2 = r2 + rp.squeeze(-1)
--                if self.clamp:
--                    r2 = r2.clamp(min=EPS)
-+                if self.is_infer_r1r2:
-+                    r1 = r1 + rp[..., 0]
-+                    r2 = r2 + rp[..., -1]
-+                    if self.clamp:
-+                        r1 = r1.clamp(min=EPS)
-+                        r2 = r2.clamp(min=EPS)
-+                    else:
-+                        r1 = nn.functional.relu(r1) + EPS
-+                        r2 = nn.functional.relu(r2) + EPS
-                 else:
--                    r2 = nn.functional.relu(r2) + EPS
-+                    r2 = r2 + rp.squeeze(-1)
-+                    if self.clamp:
-+                        r2 = r2.clamp(min=EPS)
-+                    else:
-+                        r2 = nn.functional.relu(r2) + EPS
-             else:
-                 r1 = r1 + rp.squeeze(-1)
-                 if self.clamp:
-diff --git a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
-index 4a46eae..8affe18 100644
---- a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
-+++ b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
-@@ -782,11 +782,12 @@ def test_decoder_consistency_OtherDecoders():
-         #decoder_class='PrimitiveWiseGroupConvDecoderLegacy',
-         last_scale=10,
-         no_encoder=True,
--        is_shape_sampler_sphere=True,
--        spherical_angles=True,
--        is_feature_coord=True,
--        is_feature_angles=False,
-+        is_shape_sampler_sphere=False,
-+        spherical_angles=False,
-+        is_feature_coord=False,
-+        is_feature_angles=True,
-         is_feature_radius=False,
-+        is_infer_r1r2=False,
-         dim=dim)
-     preset_params = utils.generate_multiple_primitive_params(
-         m,
-diff --git a/generate.py b/generate.py
-index bbf4e8d..9866c3c 100644
---- a/generate.py
-+++ b/generate.py
-@@ -1,6 +1,8 @@
--import torch
- # import torch.distributions as dist
- import os
-+os.environ['CUDA_PATH'] = '/usr/local/cuda-10.0'
-+
-+import torch
- import shutil
- import argparse
- from tqdm import tqdm
-@@ -18,6 +20,7 @@ import subprocess
- import yaml
- from datetime import datetime
- import subprocess
-+import eval_utils
- 
- 
- def represent_odict(dumper, instance):
-@@ -45,69 +48,66 @@ parser.add_argument('--unique_name',
-                     default='',
-                     type=str,
-                     help='String name for generation.')
-+parser.add_argument('--resume_generation_dir',
-+                    default=None,
-+                    type=str,
-+                    help='String name for generation.')
- 
- args, unknown_args = parser.parse_known_args()
-+if args.resume_generation_dir is not None:
-+    assert os.path.isabs(args.resume_generation_dir)
- 
- cfg = config.load_config(args.config, 'configs/default.yaml')
- 
--for idx, arg in enumerate(unknown_args):
--    if arg.startswith('--'):
--        arg = arg.replace('--', '')
--        value = unknown_args[idx + 1]
--        keys = arg.split('.')
--        if keys[0] not in cfg:
--            cfg[keys[0]] = {}
--        child_cfg = cfg.get(keys[0], {})
--        for key in keys[1:]:
--            item = child_cfg.get(key, None)
--            if isinstance(item, dict):
--                child_cfg = item
--            elif item is None:
--                if value == 'true':
--                    value = True
--                if value == 'false':
--                    value = False
--                if value == 'null':
--                    value = None
--                if isinstance(value, str) and value.isdigit():
--                    value = float(value)
--                child_cfg[key] = value
--            else:
--                child_cfg[key] = type(item)(value)
-+eval_utils.update_dict_with_options(cfg, unknown_args)
-+
- date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
- if args.explicit:
--    assert cfg['data'].get('is_normal_icosahedron', False) or cfg['data'].get(
--        'is_normal_uv_sphere', False)
-+    if cfg['method'] == 'pnet':
-+        assert cfg['data'].get('is_normal_icosahedron',
-+                               False) or cfg['data'].get(
-+                                   'is_normal_uv_sphere', False)
-+    elif cfg['method'] == 'atlasnetv2':
-+        assert cfg['data'].get('is_generate_mesh', False)
-+
-     cfg['generation']['is_explicit_mesh'] = True
-     cfg['test']['is_eval_explicit_mesh'] = True
--if args.explicit:
-     cfg['generation']['generation_dir'] += '_explicit'
--cfg['generation']['generation_dir'] += ('_' + date_str)
-+
-+cfg['generation']['generation_dir'] += ('_' + args.unique_name + '_' +
-+                                        date_str)
- 
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
--out_dir = os.path.dirname(args.config)
-+if args.resume_generation_dir is None:
-+    out_dir = os.path.dirname(args.config)
-+    generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
-+else:
-+    out_dir = os.path.dirname(args.resume_generation_dir)
-+    generation_dir = args.resume_generation_dir
- 
--generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
--if not os.path.exists(generation_dir):
-+if not os.path.exists(generation_dir) and args.resume_generation_dir is None:
-     os.makedirs(generation_dir)
--out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')
--out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')
- 
--patch_path = os.path.join(generation_dir, 'gen_diff.patch')
--subprocess.run('git diff > {}'.format(patch_path), shell=True)
--weight_path = os.path.join(out_dir, cfg['test']['model_file'])
--with open(weight_path, 'rb') as f:
--    md5 = hashlib.md5(f.read()).hexdigest()
--cfg['test']['model_file_hash'] = md5
--yaml.dump(
--    cfg,
--    open(
--        os.path.join(
--            out_dir, 'gen_config_{}_{}.yaml'.format(args.unique_name,
--                                                    date_str)), 'w'))
-+if args.resume_generation_dir is None:
-+    patch_path = os.path.join(generation_dir, 'gen_diff.patch')
-+    subprocess.run('git diff > {}'.format(patch_path), shell=True)
-+    if not cfg['test']['model_file'].startswith('http'):
-+        weight_path = os.path.join(out_dir, cfg['test']['model_file'])
-+        with open(weight_path, 'rb') as f:
-+            md5 = hashlib.md5(f.read()).hexdigest()
-+        cfg['test']['model_file_hash'] = md5
-+    yaml.dump(
-+        cfg,
-+        open(
-+            os.path.join(
-+                generation_dir,
-+                'gen_config_{}_{}.yaml'.format(args.unique_name, date_str)),
-+            'w'))
- 
-+out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')
-+out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')
- batch_size = cfg['generation']['batch_size']
- input_type = cfg['data']['input_type']
- vis_n_outputs = cfg['generation']['vis_n_outputs']
-@@ -121,7 +121,7 @@ dataset = config.get_dataset('test', cfg, return_idx=True)
- model = config.get_model(cfg, device=device, dataset=dataset)
- 
- checkpoint_io = CheckpointIO(out_dir, model=model)
--checkpoint_io.load(cfg['test']['model_file'])
-+checkpoint_io.load(cfg['test']['model_file'], device=device)
- 
- # Generator
- generator = config.get_generator(model, cfg, device=device)
-@@ -222,28 +222,76 @@ for it, data in enumerate(tqdm(test_loader)):
-         out_file_dict['gt'] = modelpath
- 
-     if generate_mesh:
-+        # Checkfile exists
-+        is_input_file_exists = False
-+        if input_type == 'img':
-+            inputs_path = os.path.join(in_dir, '%s.jpg' % modelname)
-+        elif input_type == 'voxels':
-+            inputs_path = os.path.join(in_dir, '%s.off' % modelname)
-+        elif input_type == 'pointcloud':
-+            inputs_path = os.path.join(in_dir, '%s.ply' % modelname)
-+        if os.path.exists(input_type):
-+            is_input_file_exists = True
-+
-+        # Write output
-+        is_mesh_file_exists = False
-+        mesh_out_file = os.path.join(mesh_dir, '%s.off' % modelname)
-+        if os.path.exists(mesh_out_file):
-+            is_mesh_file_exists = True
-+
-+        is_vertex_attribute_file_exists = False
-+        visibility_out_file = ''
-+        if cfg['generation'].get('is_explicit_mesh',
-+                                 False) and cfg['method'] == 'pnet':
-+            visibility_out_file = os.path.join(
-+                mesh_dir, '%s_vertex_visbility.npz' % modelname)
-+        elif cfg['method'] == 'bspnet':
-+            visibility_out_file = os.path.join(
-+                mesh_dir, '%s_vertex_attributes.npz' % modelname)
-+        if os.path.exists(visibility_out_file):
-+            is_vertex_attribute_file_exists = True
-+
-+        if is_input_file_exists and is_mesh_file_exists and is_vertex_attribute_file_exists and args.resume_generation_dir is not None:
-+            print('pass', category_id, modelname)
-+            continue
-+
-         t0 = time.time()
-         out = generator.generate_mesh(data)
-         time_dict['mesh'] = time.time() - t0
- 
-         # Get statistics
--        try:
--            mesh, stats_dict = out
--        except TypeError:
--            mesh, stats_dict = out, {}
-+        if cfg['method'] == 'bspnet':
-+            try:
-+                mesh, stats_dict, vertices, normals, visibility = out
-+            except TypeError:
-+                mesh, vertices, normals, visibility = out
-+                stats_dict = {}
-+            if mesh is None:
-+                continue
-+        else:
-+            try:
-+                mesh, stats_dict = out
-+            except TypeError:
-+                mesh, stats_dict = out, {}
-         time_dict.update(stats_dict)
- 
-         # Write output
--        mesh_out_file = os.path.join(mesh_dir, '%s.off' % modelname)
-         mesh.export(mesh_out_file)
-         out_file_dict['mesh'] = mesh_out_file
--        if cfg['generation'].get('is_explicit_mesh', False):
-+        if cfg['generation'].get('is_explicit_mesh',
-+                                 False) and cfg['method'] == 'pnet':
-             visibility = mesh.vertex_attributes['vertex_visibility']
--            visibility_out_file = os.path.join(
--                mesh_dir, '%s_vertex_visbility.npz' % modelname)
-+
-             np.savez(visibility_out_file, vertex_visibility=visibility)
-             out_file_dict['vertex_visibility'] = visibility_out_file
- 
-+        elif cfg['method'] == 'bspnet':
-+            np.savez(visibility_out_file,
-+                     vertex_visibility=visibility,
-+                     vertices=vertices,
-+                     normals=normals)
-+            out_file_dict['vertex_attributes'] = visibility_out_file
-+
-     if generate_pointcloud:
-         t0 = time.time()
-         pointcloud = generator.generate_pointcloud(data)
-@@ -257,7 +305,10 @@ for it, data in enumerate(tqdm(test_loader)):
-         # Save inputs
-         if input_type == 'img':
-             inputs_path = os.path.join(in_dir, '%s.jpg' % modelname)
--            inputs = data['inputs'].squeeze(0).cpu()
-+            if cfg['method'] == 'bspnet':
-+                inputs = data['inputs'].squeeze(0).expand(3, -1, -1).cpu()
-+            else:
-+                inputs = data['inputs'].squeeze(0).cpu()
-             visualize_data(inputs, 'img', inputs_path)
-             out_file_dict['in'] = inputs_path
-         elif input_type == 'voxels':
-diff --git a/im2mesh/atlasnetv2/config.py b/im2mesh/atlasnetv2/config.py
-index cea82f8..eee4061 100644
---- a/im2mesh/atlasnetv2/config.py
-+++ b/im2mesh/atlasnetv2/config.py
-@@ -6,6 +6,7 @@ from im2mesh.encoder import encoder_dict
- from im2mesh.atlasnetv2 import models, training, generation
- from im2mesh import data
- from im2mesh import config
-+from atlasnetv2.auxiliary.utils import weights_init
- 
- 
- def get_model(cfg, device=None, dataset=None, **kwargs):
-@@ -50,7 +51,8 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
-                               encoder_latent,
-                               p0_z,
-                               device=device)
--
-+    if cfg['data']['input_type'] == 'pointcloud':
-+        model.apply(weights_init)
-     return model
- 
- 
-@@ -68,15 +70,15 @@ def get_trainer(model, optimizer, cfg, device, **kwargs):
-     vis_dir = os.path.join(out_dir, 'vis')
-     input_type = cfg['data']['input_type']
- 
--    trainer = training.Trainer(
--        model,
--        optimizer,
--        device=device,
--        input_type=input_type,
--        vis_dir=vis_dir,
--        threshold=threshold,
--        eval_sample=cfg['training']['eval_sample'],
--    )
-+    trainer = training.Trainer(model,
-+                               optimizer,
-+                               device=device,
-+                               input_type=input_type,
-+                               vis_dir=vis_dir,
-+                               threshold=threshold,
-+                               eval_sample=cfg['training']['eval_sample'],
-+                               debugged=cfg['training'].get('debugged', False),
-+                               **cfg['trainer'])
- 
-     return trainer
- 
-@@ -101,7 +103,8 @@ def get_generator(model, cfg, device, **kwargs):
-         refinement_step=cfg['generation']['refinement_step'],
-         simplify_nfaces=cfg['generation']['simplify_nfaces'],
-         preprocessor=preprocessor,
--    )
-+        debugged=cfg['training'].get('debugged', False),
-+        point_scale=cfg['trainer']['point_scale'])
-     return generator
- 
- 
-diff --git a/im2mesh/atlasnetv2/generation.py b/im2mesh/atlasnetv2/generation.py
-index 49786de..422aae4 100644
---- a/im2mesh/atlasnetv2/generation.py
-+++ b/im2mesh/atlasnetv2/generation.py
-@@ -8,6 +8,7 @@ from im2mesh.utils import libmcubes
- from im2mesh.common import make_3d_grid
- from im2mesh.utils.libsimplify import simplify_mesh
- from im2mesh.utils.libmise import MISE
-+from periodic_shapes.models import model_utils
- import time
- 
- 
-@@ -30,13 +31,22 @@ class Generator3D(object):
-         simplify_nfaces (int): number of faces the mesh should be simplified to
-         preprocessor (nn.Module): preprocessor for inputs
-     '''
--
--    def __init__(self, model, points_batch_size=100000,
--                 threshold=0.5, refinement_step=0, device=None,
--                 resolution0=16, upsampling_steps=3,
--                 with_normals=False, padding=0.1, sample=False,
-+    def __init__(self,
-+                 model,
-+                 points_batch_size=100000,
-+                 threshold=0.5,
-+                 refinement_step=0,
-+                 device=None,
-+                 resolution0=16,
-+                 upsampling_steps=3,
-+                 with_normals=False,
-+                 padding=0.1,
-+                 sample=False,
-                  simplify_nfaces=None,
--                 preprocessor=None):
-+                 preprocessor=None,
-+                 point_scale=1,
-+                 debugged=False,
-+                 **kwargs):
-         self.model = model.to(device)
-         self.points_batch_size = points_batch_size
-         self.refinement_step = refinement_step
-@@ -49,6 +59,8 @@ class Generator3D(object):
-         self.sample = sample
-         self.simplify_nfaces = simplify_nfaces
-         self.preprocessor = preprocessor
-+        self.point_scale = point_scale
-+        self.debugged = debugged
- 
-     def generate_mesh(self, data, return_stats=True):
-         ''' Generates the output mesh.
-@@ -61,7 +73,8 @@ class Generator3D(object):
-         device = self.device
-         stats_dict = {}
- 
--        inputs = data.get('inputs', torch.empty(1, 0)).to(device)
-+        inputs = data.get('inputs', torch.empty(
-+            1, 0)).to(device) * (1 if self.debugged else self.point_scale)
-         kwargs = {}
- 
-         # Preprocess if requires
-@@ -77,15 +90,24 @@ class Generator3D(object):
-             c = self.model.encode_inputs(inputs)
-         stats_dict['time (encode inputs)'] = time.time() - t0
- 
--        z = self.model.get_z_from_prior((1,), sample=self.sample).to(device)
--        mesh = self.generate_from_latent(z, c, stats_dict=stats_dict, **kwargs)
-+        z = self.model.get_z_from_prior((1, ), sample=self.sample).to(device)
-+        mesh = self.generate_from_latent(z,
-+                                         c,
-+                                         stats_dict=stats_dict,
-+                                         data=data,
-+                                         **kwargs)
- 
-         if return_stats:
-             return mesh, stats_dict
-         else:
-             return mesh
- 
--    def generate_from_latent(self, z, c=None, stats_dict={}, **kwargs):
-+    def generate_from_latent(self,
-+                             z,
-+                             c=None,
-+                             stats_dict={},
-+                             data=None,
-+                             **kwargs):
-         ''' Generates mesh from latent.
- 
-         Args:
-@@ -93,48 +115,31 @@ class Generator3D(object):
-             c (tensor): latent conditioned code c
-             stats_dict (dict): stats dictionary
-         '''
--        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-+        assert data is not None
- 
-+        faces = data.get('patch.mesh_faces').to(self.device)
-+        vertices = data.get('patch.mesh_vertices').to(self.device)
-+        patch = data.get('patch').to(self.device)
-         t0 = time.time()
--        # Compute bounding box size
--        box_size = 1 + self.padding
--
--        # Shortcut
--        if self.upsampling_steps == 0:
--            nx = self.resolution0
--            pointsf = box_size * make_3d_grid(
--                (-0.5,)*3, (0.5,)*3, (nx,)*3
--            )
--            values = self.eval_points(pointsf, z, c, **kwargs).cpu().numpy()
--            value_grid = values.reshape(nx, nx, nx)
--        else:
--            mesh_extractor = MISE(
--                self.resolution0, self.upsampling_steps, threshold)
--
--            points = mesh_extractor.query()
--
--            while points.shape[0] != 0:
--                # Query points
--                pointsf = torch.FloatTensor(points).to(self.device)
--                # Normalize to bounding box
--                pointsf = pointsf / mesh_extractor.resolution
--                pointsf = box_size * (pointsf - 0.5)
--                # Evaluate model and update
--                values = self.eval_points(
--                    pointsf, z, c, **kwargs).cpu().numpy()
--                values = values.astype(np.float64)
--                mesh_extractor.update(points, values)
--                points = mesh_extractor.query()
--
--            value_grid = mesh_extractor.to_dense()
--
--        # Extract mesh
-+        predicted_vertices = self.model.decode(
-+            None, z, c, grid=patch, **kwargs) / self.point_scale
-         stats_dict['time (eval points)'] = time.time() - t0
- 
--        mesh = self.extract_mesh(value_grid, z, c, stats_dict=stats_dict)
-+        t0 = time.time()
-+        B, N, P, D = predicted_vertices.shape
-+        faces_all = torch.cat([(faces + idx * P) for idx in range(N)], axis=1)
-+        assert B == 1
-+        mem_t = time.time()
-+        npverts = predicted_vertices.view(N * P, D).to('cpu').detach().numpy()
-+        npfaces = faces_all.view(-1, 3).to('cpu').detach().numpy()
-+        skip_t = time.time() - mem_t
-+
-+        mesh = trimesh.Trimesh(npverts, npfaces, process=False)
-+        stats_dict['time (copy to trimesh)'] = time.time() - t0 - skip_t
-+
-         return mesh
- 
--    def eval_points(self, p, z, c=None, **kwargs):
-+    def eval_points(self, p, z, c=None, data=None, **kwargs):
-         ''' Evaluates the occupancy values for the points.
- 
-         Args:
-@@ -142,13 +147,26 @@ class Generator3D(object):
-             z (tensor): latent code z
-             c (tensor): latent conditioned code c
-         '''
-+        assert data is not None
-         p_split = torch.split(p, self.points_batch_size)
-+
-+        angles = data.get('angles').to(self.device)
-         occ_hats = []
- 
-         for pi in p_split:
-             pi = pi.unsqueeze(0).to(self.device)
-+            an = angles.to(self.device)
-             with torch.no_grad():
--                occ_hat = self.model.decode(pi, z, c, **kwargs).logits
-+                #_, _, sgn, _ = self.model.decode(pi, z, c, **kwargs).logits
-+                _, _, sgn, _, _ = self.model.decode(pi * self.pnet_point_scale,
-+                                                    z,
-+                                                    c,
-+                                                    angles=an,
-+                                                    **kwargs)
-+
-+                occ_hat = (
-+                    model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1) >
-+                    self.threshold).float()
- 
-             occ_hats.append(occ_hat.squeeze(0).detach().cpu())
- 
-@@ -168,20 +186,20 @@ class Generator3D(object):
-         # Some short hands
-         n_x, n_y, n_z = occ_hat.shape
-         box_size = 1 + self.padding
--        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-+        threshold = 0.
-+        #threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-         # Make sure that mesh is watertight
-         t0 = time.time()
--        occ_hat_padded = np.pad(
--            occ_hat, 1, 'constant', constant_values=-1e6)
--        vertices, triangles = libmcubes.marching_cubes(
--            occ_hat_padded, threshold)
-+        occ_hat_padded = np.pad(occ_hat, 1, 'constant', constant_values=-1e6)
-+        vertices, triangles = libmcubes.marching_cubes(occ_hat_padded,
-+                                                       threshold)
-         stats_dict['time (marching cubes)'] = time.time() - t0
-         # Strange behaviour in libmcubes: vertices are shifted by 0.5
-         vertices -= 0.5
-         # Undo padding
-         vertices -= 1
-         # Normalize to bounding box
--        vertices /= np.array([n_x-1, n_y-1, n_z-1])
-+        vertices /= np.array([n_x - 1, n_y - 1, n_z - 1])
-         vertices = box_size * (vertices - 0.5)
- 
-         # mesh_pymesh = pymesh.form_mesh(vertices, triangles)
-@@ -197,7 +215,8 @@ class Generator3D(object):
-             normals = None
- 
-         # Create mesh
--        mesh = trimesh.Trimesh(vertices, triangles,
-+        mesh = trimesh.Trimesh(vertices,
-+                               triangles,
-                                vertex_normals=normals,
-                                process=False)
- 
-@@ -261,7 +280,7 @@ class Generator3D(object):
- 
-         # Some shorthands
-         n_x, n_y, n_z = occ_hat.shape
--        assert(n_x == n_y == n_z)
-+        assert (n_x == n_y == n_z)
-         # threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-         threshold = self.threshold
- 
-@@ -290,10 +309,9 @@ class Generator3D(object):
-             face_normal = face_normal / \
-                 (face_normal.norm(dim=1, keepdim=True) + 1e-10)
-             face_value = torch.sigmoid(
--                self.model.decode(face_point.unsqueeze(0), z, c).logits
--            )
--            normal_target = -autograd.grad(
--                [face_value.sum()], [face_point], create_graph=True)[0]
-+                self.model.decode(face_point.unsqueeze(0), z, c).logits)
-+            normal_target = -autograd.grad([face_value.sum()], [face_point],
-+                                           create_graph=True)[0]
- 
-             normal_target = \
-                 normal_target / \
-diff --git a/im2mesh/atlasnetv2/models/decoder.py b/im2mesh/atlasnetv2/models/decoder.py
-index e35f724..0aeefc6 100644
---- a/im2mesh/atlasnetv2/models/decoder.py
-+++ b/im2mesh/atlasnetv2/models/decoder.py
-@@ -5,7 +5,8 @@ from im2mesh.layers import (ResnetBlockFC, CResnetBlockConv1d, CBatchNorm1d,
-                             CBatchNorm1d_legacy, ResnetBlockConv1d)
- 
- from atlasnetv2.auxiliary.model import mlpAdj, patchDeformationMLP
--from atlasnetv2.auxiliary.utils import weights_init
-+
-+from periodic_shapes.layers import primitive_wise_layers
- 
- 
- class AtlasNetV2Decoder(nn.Module):
-@@ -32,18 +33,16 @@ class AtlasNetV2Decoder(nn.Module):
-         self.c_dim = c_dim
-         decoder_kwargs['nlatent'] = hidden_size
-         self.options = type('', (), decoder_kwargs)
-+        #self.decoder = PatchDeformGroupWiseMLPAdjInOcc(self.options)
-         self.decoder = PatchDeformMLPAdjInOcc(self.options)
--        self.decoder.apply(weights_init)
- 
-     def forward(self, p, z, color_feature, grid=None, **kwargs):
-         # grid (B, P, dim) -> (B, dim, P)
-         transposed_grid = grid[:, 0, :, :].transpose(-1, -2)
-         out, _ = self.decoder(color_feature, transposed_grid)
--
-         #
-         B = transposed_grid.shape[0]
--        out = out.transpose(-1, -2).contiguous().view(B, self.options.npatch,
--                                                      -1, 3)
-+        out = out.view(B, self.options.npatch, -1, 3)
-         return out
- 
- 
-@@ -104,3 +103,134 @@ class PatchDeformMLPAdjInOcc(nn.Module):
-             #==========================================================================
- 
-         return torch.cat(outs, 2).transpose(2, 1).contiguous(), patches
-+
-+
-+class PatchDeformGroupWiseMLPAdjInOcc(nn.Module):
-+    """Atlas net auto encoder"""
-+    def __init__(self, options):
-+
-+        super(PatchDeformGroupWiseMLPAdjInOcc, self).__init__()
-+
-+        self.npatch = options.npatch
-+        self.nlatent = options.nlatent
-+        self.patchDim = options.patchDim
-+        assert self.patchDim == 2
-+        self.patchDeformDim = options.patchDeformDim
-+
-+        #encoder decoder and patch deformation module
-+        #==============================================================================
-+        self.decoder = GroupWisemlpAdj(nlatent=self.patchDeformDim +
-+                                       self.nlatent,
-+                                       npatch=self.npatch)
-+        self.patchDeformation = patchDeformationGroupWiseMLP(
-+            patchDim=self.patchDim,
-+            patchDeformDim=self.patchDeformDim,
-+            npatch=self.npatch)
-+        #==============================================================================
-+
-+    def forward(self, x, grid):
-+
-+        #encoder
-+        #==============================================================================
-+        #x = self.encoder(x.transpose(2, 1).contiguous())
-+        #==============================================================================
-+
-+        # B, P, dims
-+        grid[:, 2:, :] = 0
-+        # B, N, dims, P
-+        rand_grid1 = grid.unsqueeze(1).expand(-1, self.npatch, -1,
-+                                              -1).contiguous()
-+
-+        #random planar patch
-+        #==========================================================================
-+        rand_grid2 = self.patchDeformation(rand_grid1)
-+        #==========================================================================
-+
-+        #cat with latent vector and decode
-+        #==========================================================================
-+        # B, nlatent -> B, N, nlatent, P
-+        y1 = x.view(x.size(0), 1, x.size(1), 1).expand(-1, self.npatch, -1,
-+                                                       rand_grid2.size(3))
-+        y2 = torch.cat([y1, rand_grid2], axis=2).contiguous()
-+
-+        # B, N, defromdim, P
-+        out = self.decoder(y2)
-+        #==========================================================================
-+
-+        #B, N, P, deformdim
-+        return out.transpose(3, 2).contiguous(), None
-+
-+
-+class patchDeformationGroupWiseMLP(nn.Module):
-+    """deformation of a 2D patch into a 3D surface"""
-+    def __init__(self, patchDim=2, patchDeformDim=3, npatch=16, tanh=True):
-+
-+        super(patchDeformationGroupWiseMLP, self).__init__()
-+        layer_size = 128
-+        self.tanh = tanh
-+        #self.conv1 = torch.nn.Conv1d(patchDim, layer_size, 1)
-+        #self.conv2 = torch.nn.Conv1d(layer_size, layer_size, 1)
-+        #self.conv3 = torch.nn.Conv1d(layer_size, patchDeformDim, 1)
-+        self.bn1 = torch.nn.BatchNorm1d(layer_size * npatch)
-+        self.bn2 = torch.nn.BatchNorm1d(layer_size * npatch)
-+
-+        self.conv1 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
-+                                                               patchDim,
-+                                                               layer_size,
-+                                                               act='none')
-+        self.conv2 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
-+                                                               layer_size,
-+                                                               layer_size,
-+                                                               act='none')
-+        self.conv3 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
-+                                                               layer_size,
-+                                                               patchDeformDim,
-+                                                               act='none')
-+        self.th = nn.Tanh()
-+
-+    def forward(self, x):
-+        B, N, _, P = x.shape
-+        x = F.relu(self.bn1(self.conv1(x).view(B, -1, P))).view(B, N, -1, P)
-+        x = F.relu(self.bn2(self.conv2(x).view(B, -1, P))).view(B, N, -1, P)
-+        if self.tanh:
-+            x = self.th(self.conv3(x))
-+        else:
-+            x = self.conv3(x)
-+        return x
-+
-+
-+class GroupWisemlpAdj(nn.Module):
-+    def __init__(self, nlatent=1024, npatch=16):
-+        """Atlas decoder"""
-+
-+        super(GroupWisemlpAdj, self).__init__()
-+        self.nlatent = nlatent
-+        self.conv1 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
-+                                                               self.nlatent,
-+                                                               self.nlatent,
-+                                                               act='none')
-+        self.conv2 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
-+                                                               self.nlatent,
-+                                                               self.nlatent //
-+                                                               2,
-+                                                               act='none')
-+        self.conv3 = primitive_wise_layers.PrimitiveWiseLinear(
-+            npatch, self.nlatent // 2, self.nlatent // 4, act='none')
-+        self.conv4 = primitive_wise_layers.PrimitiveWiseLinear(npatch,
-+                                                               self.nlatent //
-+                                                               4,
-+                                                               3,
-+                                                               act='none')
-+
-+        self.th = nn.Tanh()
-+        self.bn1 = torch.nn.BatchNorm1d(self.nlatent * npatch)
-+        self.bn2 = torch.nn.BatchNorm1d(self.nlatent // 2 * npatch)
-+        self.bn3 = torch.nn.BatchNorm1d(self.nlatent // 4 * npatch)
-+
-+    def forward(self, x):
-+        B, N, _, P = x.shape
-+        x = F.relu(self.bn1(self.conv1(x).view(B, -1, P))).view(B, N, -1, P)
-+        x = F.relu(self.bn2(self.conv2(x).view(B, -1, P))).view(B, N, -1, P)
-+        x = F.relu(self.bn3(self.conv3(x).view(B, -1, P))).view(B, N, -1, P)
-+        x = self.th(self.conv4(x).view(B, -1, P)).view(B, N, -1, P)
-+        return x
-diff --git a/im2mesh/atlasnetv2/training.py b/im2mesh/atlasnetv2/training.py
-index 208c3e0..e109a38 100644
---- a/im2mesh/atlasnetv2/training.py
-+++ b/im2mesh/atlasnetv2/training.py
-@@ -31,6 +31,9 @@ class Trainer(BaseTrainer):
-                  input_type='img',
-                  vis_dir=None,
-                  threshold=0.5,
-+                 point_scale=1,
-+                 is_pykeops_loss=True,
-+                 debugged=False,
-                  eval_sample=False):
-         self.model = model
-         self.optimizer = optimizer
-@@ -39,6 +42,9 @@ class Trainer(BaseTrainer):
-         self.vis_dir = vis_dir
-         self.threshold = threshold
-         self.eval_sample = eval_sample
-+        self.point_scale = point_scale
-+        self.is_pykeops_loss = is_pykeops_loss
-+        self.debugged = debugged
- 
-         self.distChamferL2 = dist_chamfer.chamferDist()
- 
-@@ -67,10 +73,11 @@ class Trainer(BaseTrainer):
-         self.model.eval()
- 
-         device = self.device
--        pointcloud = data.get('pointcloud').to(device)
-+        pointcloud = data.get('pointcloud').to(device) * self.point_scale
-         patch = data.get('patch').to(device)
--        inputs = data.get('inputs', torch.empty(pointcloud.size(0),
--                                                0)).to(device)
-+        inputs = data.get('inputs', torch.empty(
-+            pointcloud.size(0),
-+            0)).to(device) * (1 if self.debugged else self.point_scale)
- 
-         feature = self.model.encode_inputs(inputs)
- 
-@@ -99,10 +106,11 @@ class Trainer(BaseTrainer):
-         '''
-         device = self.device
- 
--        pointcloud = data.get('pointcloud').to(device)
-+        pointcloud = data.get('pointcloud').to(device) * self.point_scale
-         patch = data.get('patch').to(device)
--        inputs = data.get('inputs', torch.empty(pointcloud.size(0),
--                                                0)).to(device)
-+        inputs = data.get('inputs', torch.empty(
-+            pointcloud.size(0),
-+            0)).to(device) * (1 if self.debugged else self.point_scale)
- 
-         feature = self.model.encode_inputs(inputs)
- 
-@@ -122,17 +130,17 @@ class Trainer(BaseTrainer):
-         for i in trange(B):
-             if not inputs.ndim == 1:  # no input image
-                 input_img_path = os.path.join(self.vis_dir, '%03d_in.png' % i)
--                plot = vis.visualize_data(inputs[i].cpu(),
-+                plot = vis.visualize_data(inputs[i].cpu() / self.point_scale,
-                                           self.input_type,
-                                           input_img_path,
-                                           return_plot=True)
-                 input_images.append(
-                     wandb.Image(plot, caption='input image {}'.format(i)))
--            plot = vis.visualize_pointcloud(coords[i].cpu().view(N * P, dims),
--                                            normals=None,
--                                            out_file=os.path.join(
--                                                self.vis_dir, '%03d.png' % i),
--                                            return_plot=True)
-+            plot = vis.visualize_pointcloud(
-+                coords[i].cpu().view(N * P, dims) / self.point_scale,
-+                normals=None,
-+                out_file=os.path.join(self.vis_dir, '%03d.png' % i),
-+                return_plot=True)
-             voxels_images.append(
-                 wandb.Image(plot, caption='voxel {}'.format(i)))
-         if not inputs.ndim == 1:  # no input image
-@@ -146,10 +154,11 @@ class Trainer(BaseTrainer):
-             data (dict): data dictionary
-         '''
-         device = self.device
--        pointcloud = data.get('pointcloud').to(device)
-+        pointcloud = data.get('pointcloud').to(device) * self.point_scale
-         patch = data.get('patch').to(device)
--        inputs = data.get('inputs', torch.empty(pointcloud.size(0),
--                                                0)).to(device)
-+        inputs = data.get('inputs', torch.empty(
-+            pointcloud.size(0),
-+            0)).to(device) * (1 if self.debugged else self.point_scale)
- 
-         kwargs = {}
- 
-@@ -162,11 +171,14 @@ class Trainer(BaseTrainer):
-                                    grid=patch,
-                                    **kwargs)
-         B, N, P, dims = coords.shape
--        """
--        dist1, dist2 = self.distChamferL2(coords.view(B, N * P, dims), pointcloud)
--        loss = torch.mean(dist1) + torch.mean(dist2)
--        """
- 
--        loss = atv2_utils.chamfer_loss(coords.view(B, N * P, dims), pointcloud)
-+        if self.is_pykeops_loss:
-+            loss = atv2_utils.chamfer_loss(coords.view(B, N * P, dims),
-+                                           pointcloud)
-+        else:
-+            dist1, dist2 = self.distChamferL2(coords.view(B, N * P, dims),
-+                                              pointcloud)
-+            loss = torch.mean(dist1) + torch.mean(dist2)
-+
-         losses = {'total_loss': loss}
-         return losses
-diff --git a/im2mesh/checkpoints.py b/im2mesh/checkpoints.py
-index 75ff6ab..709646b 100644
---- a/im2mesh/checkpoints.py
-+++ b/im2mesh/checkpoints.py
-@@ -41,18 +41,18 @@ class CheckpointIO(object):
- 
-         wandb.save(filename)
- 
--    def load(self, filename):
-+    def load(self, filename, device=None):
-         '''Loads a module dictionary from local file or url.
-         
-         Args:
-             filename (str): name of saved module dictionary
-         '''
-         if is_url(filename):
--            return self.load_url(filename)
-+            return self.load_url(filename, device=device)
-         else:
--            return self.load_file(filename)
-+            return self.load_file(filename, device=device)
- 
--    def load_file(self, filename):
-+    def load_file(self, filename, device=None):
-         '''Loads a module dictionary from file.
-         
-         Args:
-@@ -66,12 +66,15 @@ class CheckpointIO(object):
-             print(filename)
-             print('=> Loading checkpoint from local file...')
-             state_dict = torch.load(filename)
--            scalars = self.parse_state_dict(state_dict)
-+            if 'model' not in state_dict:
-+                print('Detect weight file trained outside of occ env.')
-+                state_dict = {'model': state_dict}
-+            scalars = self.parse_state_dict(state_dict, device=device)
-             return scalars
-         else:
-             raise FileExistsError
- 
--    def load_url(self, url):
-+    def load_url(self, url, device=None):
-         '''Load a module dictionary from url.
-         
-         Args:
-@@ -80,10 +83,10 @@ class CheckpointIO(object):
-         print(url)
-         print('=> Loading checkpoint from url...')
-         state_dict = model_zoo.load_url(url, progress=True)
--        scalars = self.parse_state_dict(state_dict)
-+        scalars = self.parse_state_dict(state_dict, device=device)
-         return scalars
- 
--    def parse_state_dict(self, state_dict):
-+    def parse_state_dict(self, state_dict, device=None):
-         '''Parse state_dict of model and return scalars.
-         
-         Args:
-@@ -98,6 +101,7 @@ class CheckpointIO(object):
-         """
-         for k, v in self.module_dict.items():
-             if k in state_dict:
-+                print('load parameter')
-                 #if False:
-                 if k == 'model':
-                     pretrained_dict = state_dict[k]
-@@ -122,13 +126,18 @@ class CheckpointIO(object):
-                     for key in pretrained_dict_new_param:
-                         print(key, pretrained_dict_new_param[key].shape)
-                     new_pretrained_dict.update(pretrained_dict_new_param)
--                    v.load_state_dict(new_pretrained_dict)
-+                    if device is not None:
-+                        v.load_state_dict(new_pretrained_dict)
-+                    else:
-+                        v.load_state_dict(new_pretrained_dict)
-+
-                 else:
-                     v.load_state_dict(state_dict[k])
-         scalars = {
-             k: v
-             for k, v in state_dict.items() if k not in self.module_dict
-         }
-+        print('load done')
-         return scalars
- 
- 
-diff --git a/im2mesh/config.py b/im2mesh/config.py
-index 00e3fc4..eb843e7 100644
---- a/im2mesh/config.py
-+++ b/im2mesh/config.py
-@@ -1,7 +1,7 @@
- import yaml
- from torchvision import transforms
- from im2mesh import data
--from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet, atlasnetv2
-+from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet, atlasnetv2, bspnet
- from im2mesh import preprocess
- from torch.utils import data as torch_data
- 
-@@ -12,7 +12,8 @@ method_dict = {
-     'pix2mesh': pix2mesh,
-     'dmc': dmc,
-     'pnet': pnet,
--    'atlasnetv2': atlasnetv2
-+    'atlasnetv2': atlasnetv2,
-+    'bspnet': bspnet
- }
- 
- 
-@@ -149,12 +150,11 @@ def get_dataset(mode, cfg, return_idx=False, return_category=False):
-         if return_category:
-             fields['category'] = data.CategoryField()
- 
--        dataset = data.Shapes3dDataset(
--            dataset_folder,
--            fields,
--            split=split,
--            categories=categories,
--        )
-+        dataset = data.Shapes3dDataset(dataset_folder,
-+                                       fields,
-+                                       split=split,
-+                                       categories=categories,
-+                                       cfg=cfg)
-     elif dataset_type == 'kitti':
-         dataset = data.KittiDataset(dataset_folder,
-                                     img_size=cfg['data']['img_size'],
-@@ -215,6 +215,9 @@ def get_inputs_field(mode, cfg):
- 
-         inputs_field = data.ImagesField(cfg['data']['img_folder'],
-                                         transform,
-+                                        cfg,
-+                                        extension=cfg['data'].get(
-+                                            'img_extension', 'jpg'),
-                                         with_camera=with_camera,
-                                         random_view=random_view)
-     elif input_type == 'pointcloud':
-diff --git a/im2mesh/data/__init__.py b/im2mesh/data/__init__.py
-index 870a44f..50b1a6d 100644
---- a/im2mesh/data/__init__.py
-+++ b/im2mesh/data/__init__.py
-@@ -4,7 +4,7 @@ from im2mesh.data.core import (
- )
- from im2mesh.data.fields import (
-     IndexField, CategoryField, ImagesField, PointsField,
--    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField, RawIDField, SDFPointsField, PlanarPatchField
-+    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField, RawIDField, SDFPointsField, PlanarPatchField, PartLabeledPointCloudField
- )
- from im2mesh.data.transforms import (
-     PointcloudNoise, SubsamplePointcloud,
-@@ -41,4 +41,5 @@ __all__ = [
-     KittiDataset,
-     OnlineProductDataset,
-     ImageDataset,
-+    PartLabeledPointCloudField,
- ]
-diff --git a/im2mesh/data/core.py b/im2mesh/data/core.py
-index a224c72..1f2f128 100644
---- a/im2mesh/data/core.py
-+++ b/im2mesh/data/core.py
-@@ -39,7 +39,8 @@ class Shapes3dDataset(data.Dataset):
-                  split=None,
-                  categories=None,
-                  no_except=True,
--                 transform=None):
-+                 transform=None,
-+                 cfg=None):
-         ''' Initialization of the the 3D shape dataset.
- 
-         Args:
-@@ -84,6 +85,12 @@ class Shapes3dDataset(data.Dataset):
-             if not os.path.isdir(subpath):
-                 logger.warning('Category %s does not exist in dataset.' % c)
- 
-+            if cfg is not None and 'semseg_path' in cfg['data']:
-+                subpath = subpath.replace(cfg['data']['path'],
-+                                          cfg['data']['semseg_path'])
-+            elif cfg is not None and 'bspnet' in cfg['data']:
-+                subpath = subpath.replace(cfg['data']['path'],
-+                                          cfg['data']['bspnet']['path'])
-             split_file = os.path.join(subpath, split + '.lst')
-             with open(split_file, 'r') as f:
-                 models_c = f.read().split('\n')
-diff --git a/im2mesh/data/fields.py b/im2mesh/data/fields.py
-index 64f1ae8..18e148c 100644
---- a/im2mesh/data/fields.py
-+++ b/im2mesh/data/fields.py
-@@ -68,6 +68,7 @@ class ImagesField(Field):
-     def __init__(self,
-                  folder_name,
-                  transform=None,
-+                 cfg=None,
-                  extension='jpg',
-                  random_view=True,
-                  with_camera=False):
-@@ -76,6 +77,19 @@ class ImagesField(Field):
-         self.extension = extension
-         self.random_view = random_view
-         self.with_camera = with_camera
-+        self.cfg = cfg
-+
-+        self.is_bspnet = False
-+        if cfg is not None and self.cfg['method'] == 'bspnet':
-+            self.is_bspnet = True
-+            assert 'bspnet' in self.cfg['data']
-+            bspnet_config = self.cfg['data']['bspnet']
-+            assert bspnet_config['path'].startswith('data')
-+
-+            assert not self.with_camera
-+
-+            self.extension = bspnet_config['extension']
-+            self.folder_name = bspnet_config['img_folder']
- 
-     def load(self, model_path, idx, category):
-         ''' Loads the data point.
-@@ -85,6 +99,10 @@ class ImagesField(Field):
-             idx (int): ID of data point
-             category (int): index of category
-         '''
-+        if self.is_bspnet:
-+            model_path = model_path.replace(self.cfg['data']['path'],
-+                                            self.cfg['data']['bspnet']['path'])
-+
-         folder = os.path.join(model_path, self.folder_name)
-         files = glob.glob(os.path.join(folder, '*.%s' % self.extension))
-         if self.random_view:
-@@ -93,9 +111,15 @@ class ImagesField(Field):
-             idx_img = 0
-         filename = files[idx_img]
- 
--        image = Image.open(filename).convert('RGB')
--        if self.transform is not None:
--            image = self.transform(image)
-+        if self.is_bspnet:
-+
-+            image = np.load(filename)['image']
-+            # 1 x 128 x 128
-+            image = torch.from_numpy(image)
-+        else:
-+            image = Image.open(filename).convert('RGB')
-+            if self.transform is not None:
-+                image = self.transform(image)
- 
-         data = {None: image}
- 
-@@ -238,10 +262,23 @@ class PointCloudField(Field):
-         with_transforms (bool): whether scaling and rotation dat should be
-             provided
-     '''
--    def __init__(self, file_name, transform=None, with_transforms=False):
-+    def __init__(self,
-+                 file_name,
-+                 transform=None,
-+                 cfg=None,
-+                 with_transforms=False):
-         self.file_name = file_name
-         self.transform = transform
-         self.with_transforms = with_transforms
-+        self.cfg = cfg
-+
-+        self.is_bspnet = False
-+        if cfg is not None and self.cfg['method'] == 'bspnet':
-+            self.is_bspnet = True
-+            assert 'bspnet' in self.cfg['data']
-+            bspnet_config = self.cfg['data']['bspnet']
-+            assert bspnet_config['path'].startswith('data')
-+            assert 'pointcloud_file' in bspnet_config
- 
-     def load(self, model_path, idx, category):
-         ''' Loads the data point.
-@@ -263,6 +300,15 @@ class PointCloudField(Field):
-             'normals': normals,
-         }
- 
-+        if self.is_bspnet:
-+            bsp_model_path = model_path.replace(
-+                self.cfg['data']['path'], self.cfg['data']['bspnet']['path'])
-+
-+            data['imnet_points'] = torch.from_numpy(
-+                trimesh.load(
-+                    os.path.join(bsp_model_path, self.cfg['data']['bspnet']
-+                                 ['pointcloud_file'])).vertices)
-+
-         if self.with_transforms and 'loc' in data and 'scale' in data:
-             data['loc'] = pointcloud_dict['loc'].astype(np.float32)
-             data['scale'] = pointcloud_dict['scale'].astype(np.float32)
-@@ -559,3 +605,57 @@ class PlanarPatchField(Field):
-                 'mesh_faces': self.faces.clone()
-             })
-         return data
-+
-+
-+class PartLabeledPointCloudField(Field):
-+    ''' Point cloud field.
-+
-+    It provides the field used for point cloud data. These are the points
-+    randomly sampled on the mesh.
-+
-+    Args:
-+        file_name (str): file name
-+        transform (list): list of transformations applied to data points
-+        with_transforms (bool): whether scaling and rotation dat should be
-+            provided
-+    '''
-+    def __init__(self, file_name, cfg, transform=None):
-+        self.file_name = file_name
-+        self.transform = transform
-+        self.shapenet_path = cfg['data']['path']
-+        self.semseg_shapenet_path = cfg['data']['semseg_path']
-+
-+    def load(self, model_path, idx, category):
-+        ''' Loads the data point.
-+
-+        Args:
-+            model_path (str): path to model
-+            idx (int): ID of data point
-+            category (int): index of category
-+        '''
-+        model_path = model_path.replace(self.shapenet_path,
-+                                        self.semseg_shapenet_path)
-+        file_path = os.path.join(model_path, self.file_name)
-+
-+        pointcloud_dict = np.load(file_path)
-+
-+        points = pointcloud_dict['points'].astype(np.float32)
-+        labels = pointcloud_dict['labels'].astype(np.float32)
-+
-+        data = {
-+            None: points,
-+            'labels': labels,
-+        }
-+
-+        if self.transform is not None:
-+            data = self.transform(data)
-+
-+        return data
-+
-+    def check_complete(self, files):
-+        ''' Check if field is complete.
-+        
-+        Args:
-+            files: files
-+        '''
-+        return True
-diff --git a/im2mesh/eval.py b/im2mesh/eval.py
-index 91052f8..820fafc 100644
---- a/im2mesh/eval.py
-+++ b/im2mesh/eval.py
-@@ -11,7 +11,8 @@ from pykeops.torch import LazyTensor
- import kaolin as kal
- import torch
- import warnings
--
-+import time
-+random.seed(0)
- # Maximum values for bounding box [-0.5, 0.5]^3
- EMPTY_PCL_DICT = {
-     'completeness': np.sqrt(3),
-@@ -38,8 +39,13 @@ class MeshEvaluator(object):
-     Args:
-         n_points (int): number of points to be used for evaluation
-     '''
--    def __init__(self, n_points=100000):
-+    def __init__(self,
-+                 n_points=100000,
-+                 is_sample_from_surface=False,
-+                 is_normalize_by_side_length=False):
-         self.n_points = n_points
-+        self.is_sample_from_surface = is_sample_from_surface
-+        self.is_normalize_by_side_length = is_normalize_by_side_length
- 
-     def eval_mesh(self,
-                   mesh,
-@@ -48,7 +54,8 @@ class MeshEvaluator(object):
-                   points_iou,
-                   occ_tgt,
-                   is_eval_explicit_mesh=False,
--                  vertex_visibility=None):
-+                  vertex_visibility=None,
-+                  skip_iou=False):
-         ''' Evaluates a mesh.
- 
-         Args:
-@@ -58,31 +65,44 @@ class MeshEvaluator(object):
-             points_iou (numpy_array): points tensor for IoU evaluation
-             occ_tgt (numpy_array): GT occupancy values for IoU points
-         '''
-+        t0 = time.time()
-         if is_eval_explicit_mesh:
--            assert vertex_visibility is not None
--            sampled_vertex_idx = np.zeros_like(vertex_visibility).astype(
--                np.bool)
--            pointcloud = mesh.vertices[vertex_visibility, :]
--            normals = mesh.vertex_normals[vertex_visibility, :]
-+            if vertex_visibility is not None:
-+                select_idx = vertex_visibility
-+                if self.is_sample_from_surface and select_idx.sum(
-+                ) > self.n_points:
-+                    select_idx = np.random.choice(np.nonzero(select_idx)[0],
-+                                                  size=self.n_points,
-+                                                  replace=False)
-+
-+                pointcloud = mesh.vertices[select_idx, :]
-+                normals = mesh.vertex_normals[select_idx, :]
-+            else:
-+                pointcloud = mesh.vertices
-+                normals = mesh.vertex_normals
-+            t0 = time.time()
-             pointcloud = pointcloud.astype(np.float32)
-             normals = normals.astype(np.float32)
-+            #print('copy pcd and normals to cpu', time.time() - t0)
- 
-+            t0 = time.time()
-             if pointcloud.shape[0] > self.n_points:
-                 select_idx = random.sample(range(pointcloud.shape[0]),
-                                            self.n_points)
-                 pointcloud = pointcloud[select_idx:]
--            if normals.shape[0] > self.n_points:
--                select_idx = random.sample(range(normals.shape[0]),
--                                           self.n_points)
-+                #if normals.shape[0] > self.n_points:
-+                #    select_idx = random.sample(range(normals.shape[0]),
-+                #                               self.n_points)
-                 normals = normals[select_idx:]
-             if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
--                select_idx = random.sample(range(pointcloud.shape[0]),
-+                select_idx = random.sample(range(pointcloud_tgt.shape[0]),
-                                            pointcloud.shape[0])
-                 pointcloud_tgt = pointcloud_tgt[select_idx, :]
--            if normals_tgt.shape[0] > normals.shape[0]:
--                select_idx = random.sample(range(normals.shape[0]),
--                                           pointcloud.shape[0])
-+                #if normals_tgt.shape[0] > normals.shape[0]:
-+                #    select_idx = random.sample(range(normals.shape[0]),
-+                #                               pointcloud.shape[0])
-                 normals_tgt = normals_tgt[select_idx, :]
-+            #print('random sample points', time.time() - t0)
-         else:
-             if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-                 pointcloud, idx = mesh.sample(self.n_points, return_index=True)
-@@ -92,15 +112,21 @@ class MeshEvaluator(object):
-                 pointcloud = np.empty((0, 3))
-                 normals = np.empty((0, 3))
- 
-+        t0 = time.time()
-         out_dict = self.eval_pointcloud(pointcloud, pointcloud_tgt, normals,
-                                         normals_tgt)
-+        #print('eval point cloud', time.time() - t0)
- 
--        if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-+        t0 = time.time()
-+        if len(mesh.vertices) != 0 and len(
-+                mesh.faces) != 0 and not skip_iou and False:
-             occ = check_mesh_contains(mesh, points_iou)
-             out_dict['iou'] = compute_iou(occ, occ_tgt)
-         else:
-             out_dict['iou'] = 0.
-+        #print('iou', time.time() - t0)
- 
-+        #print("eval_mesh", time.time() - t0)
-         return out_dict
- 
-     def eval_pointcloud(self,
-@@ -131,21 +157,31 @@ class MeshEvaluator(object):
-         # from thre predicted point cloud
-         completeness, completeness_normals = distance_p2p(
-             pointcloud_tgt, normals_tgt, pointcloud, normals)
--        completeness2 = completeness**2
- 
-+        if self.is_normalize_by_side_length:
-+            normalize_scale = 1. / (
-+                (pointcloud_tgt.max(axis=0) -
-+                 pointcloud_tgt.min(axis=0)).max() / 10).item()
-+            completeness = completeness * normalize_scale
-+        t0 = time.time()
-+        completeness2 = completeness**2
-         completeness = completeness.mean()
-         completeness2 = completeness2.mean()
-         completeness_normals = completeness_normals.mean()
-+        #print('calc compness', time.time() - t0)
- 
-         # Accuracy: how far are th points of the predicted pointcloud
-         # from the target pointcloud
-         accuracy, accuracy_normals = distance_p2p(pointcloud, normals,
-                                                   pointcloud_tgt, normals_tgt)
-+        if self.is_normalize_by_side_length:
-+            accuracy = accuracy * normalize_scale
-+        t0 = time.time()
-         accuracy2 = accuracy**2
--
-         accuracy = accuracy.mean()
-         accuracy2 = accuracy2.mean()
-         accuracy_normals = accuracy_normals.mean()
-+        #print('calc accuracy', time.time() - t0)
- 
-         # Chamfer distance
-         chamferL2 = 0.5 * (completeness2 + accuracy2)
-@@ -184,8 +220,17 @@ class MeshEvaluator(object):
-         '''
- 
-         if is_eval_explicit_mesh:
--            assert vertex_visibility is not None
--            pointcloud = mesh.vertices[vertex_visibility, :]
-+            if vertex_visibility is not None:
-+                select_idx = vertex_visibility
-+                if self.is_sample_from_surface and select_idx.sum(
-+                ) > self.n_points:
-+                    select_idx = np.random.choice(np.nonzero(select_idx)[0],
-+                                                  size=self.n_points,
-+                                                  replace=False)
-+                pointcloud = mesh.vertices[select_idx, :]
-+            else:
-+                pointcloud = mesh.vertices
-+
-             pointcloud = pointcloud.astype(np.float32)
- 
-             if pointcloud.shape[0] > self.n_points:
-@@ -194,7 +239,7 @@ class MeshEvaluator(object):
-                 pointcloud = pointcloud[select_idx:]
- 
-             if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
--                select_idx = random.sample(range(pointcloud.shape[0]),
-+                select_idx = random.sample(range(pointcloud_tgt.shape[0]),
-                                            pointcloud.shape[0])
-                 pointcloud_tgt = pointcloud_tgt[select_idx, :]
- 
-@@ -212,10 +257,18 @@ class MeshEvaluator(object):
-             pointcloud = np.empty((0, 3))
-         """
- 
-+        if self.is_normalize_by_side_length:
-+            normalize_scale = 1. / (
-+                (pointcloud_tgt.max(axis=0) -
-+                 pointcloud_tgt.min(axis=0)).max() / 10).item()
-+        else:
-+            normalize_scale = 1.
-+
-         out_dict = fscore(pointcloud[np.newaxis, ...],
-                           pointcloud_tgt[np.newaxis, ...],
-                           thresholds=thresholds,
--                          mode='pykeops')
-+                          mode='pykeops',
-+                          normalize_scale=normalize_scale)
-         if out_dict is None:
-             return out_dict
-         else:
-@@ -245,7 +298,11 @@ class MeshEvaluator(object):
-         return out_dict
- 
- 
--def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
-+def distance_p2p(points_src,
-+                 normals_src,
-+                 points_tgt,
-+                 normals_tgt,
-+                 mode='pykeops'):
-     ''' Computes minimal distances of each point in points_src to points_tgt.
- 
-     Args:
-@@ -254,22 +311,46 @@ def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
-         points_tgt (numpy array): target points
-         normals_tgt (numpy array): target normals
-     '''
--    kdtree = KDTree(points_tgt)
--    dist, idx = kdtree.query(points_src)
--
--    if normals_src is not None and normals_tgt is not None:
--        normals_src = \
--            normals_src / np.linalg.norm(normals_src, axis=-1, keepdims=True)
--        normals_tgt = \
--            normals_tgt / np.linalg.norm(normals_tgt, axis=-1, keepdims=True)
--
--        normals_dot_product = (normals_tgt[idx] * normals_src).sum(axis=-1)
--        # Handle normals that point into wrong direction gracefully
--        # (mostly due to mehtod not caring about this in generation)
--        normals_dot_product = np.abs(normals_dot_product)
-+    assert mode in ['pykeops', 'original']
-+    if mode == 'pykeops':
-+        # output is in torch tensor
-+        t0 = time.time()
-+        dist, idx = one_sided_chamfer_distance_with_index(
-+            points_src, points_tgt)
-+        #print('calc dist and dix', time.time() - t0)
-+        t0 = time.time()
-+        dist = dist.to('cpu').numpy()
-+        #print('copy dist to cpu', time.time() - t0)
-+        if normals_src is not None and normals_tgt is not None:
-+            t0 = time.time()
-+            normals_src = torch.nn.functional.normalize(
-+                torch.from_numpy(normals_src).to('cuda'), dim=-1)
-+            normals_tgt = torch.nn.functional.normalize(
-+                torch.from_numpy(normals_tgt).to('cuda'), dim=-1)
-+
-+            normals_dot_product = (normals_tgt[idx] * normals_src).sum(
-+                axis=-1).abs().to('cpu').numpy()
-+            #print('calc normal const', time.time() - t0)
-+        else:
-+            normals_dot_product = np.array([np.nan] * points_src.shape[0],
-+                                           dtype=np.float32)
-     else:
--        normals_dot_product = np.array([np.nan] * points_src.shape[0],
--                                       dtype=np.float32)
-+        kdtree = KDTree(points_tgt)
-+        dist, idx = kdtree.query(points_src)
-+
-+        if normals_src is not None and normals_tgt is not None:
-+            normals_src = \
-+                normals_src / np.linalg.norm(normals_src, axis=-1, keepdims=True)
-+            normals_tgt = \
-+                normals_tgt / np.linalg.norm(normals_tgt, axis=-1, keepdims=True)
-+
-+            normals_dot_product = (normals_tgt[idx] * normals_src).sum(axis=-1)
-+            # Handle normals that point into wrong direction gracefully
-+            # (mostly due to mehtod not caring about this in generation)
-+            normals_dot_product = np.abs(normals_dot_product)
-+        else:
-+            normals_dot_product = np.array([np.nan] * points_src.shape[0],
-+                                           dtype=np.float32)
-     return dist, normals_dot_product
- 
- 
-@@ -304,7 +385,41 @@ def chamfer_distance(pred, target, pykeops=True):
-     return pred2target, target2pred
- 
- 
--def fscore(pred_points, target_points, thresholds=[0.01], mode='pykeops'):
-+def one_sided_chamfer_distance_with_index(source_points, target_points):
-+    assert source_points.ndim in [2, 3]
-+    assert target_points.ndim in [2, 3]
-+    assert target_points.ndim == source_points.ndim
-+    original_ndim = target_points.ndim
-+    if isinstance(source_points, np.ndarray):
-+        source_points = torch.from_numpy(source_points).to('cuda')
-+    if isinstance(target_points, np.ndarray):
-+        target_points = torch.from_numpy(target_points).to('cuda')
-+
-+    if source_points.ndim == 2:
-+        source_points = source_points.unsqueeze(0)
-+    if target_points.ndim == 2:
-+        target_points = target_points.unsqueeze(0)
-+
-+    G_i1 = LazyTensor(source_points.unsqueeze(2))
-+    X_j1 = LazyTensor(target_points.unsqueeze(1))
-+
-+    dist = (G_i1 - X_j1).norm2()
-+
-+    # N
-+    idx = dist.argmin(dim=2).squeeze(-1)
-+    pred2target = dist.min(2).squeeze(-1)
-+    if original_ndim == 2:
-+        idx = idx[0]
-+        pred2target = pred2target[0]
-+
-+    return pred2target, idx
-+
-+
-+def fscore(pred_points,
-+           target_points,
-+           thresholds=[0.01],
-+           mode='pykeops',
-+           normalize_scale=1.):
-     assert mode in ['kaolin', 'pykeops']
-     assert isinstance(thresholds, list)
- 
-@@ -343,6 +458,9 @@ def fscore(pred_points, target_points, thresholds=[0.01], mode='pykeops'):
-         gt_distances, pred_distances = chamfer_distance(
-             pred_points, target_points)
- 
-+        gt_distances = gt_distances * normalize_scale
-+        pred_distances = pred_distances * normalize_scale
-+
-         for threshold in thresholds:
-             fn = (pred_distances > threshold).sum(-1).float()
-             fp = (gt_distances > threshold).sum(-1).float()
-diff --git a/im2mesh/pnet/config.py b/im2mesh/pnet/config.py
-index 73fc786..67ee7f5 100644
---- a/im2mesh/pnet/config.py
-+++ b/im2mesh/pnet/config.py
-@@ -153,14 +153,21 @@ def get_data_fields(mode, cfg):
-             sdf_points_transform,
-             with_transforms=with_transforms)
- 
--    pointcloud_transform = data.SubsamplePointcloud(
--        cfg['data']['pointcloud_target_n'])
--    if cfg.get('sdf_generation', False):
--        pointcloud_transform = None
-+    if cfg['test'].get('is_eval_semseg', False):
-+        fields['pointcloud'] = data.PartLabeledPointCloudField(
-+            cfg['data']['semseg_pointcloud_file'], cfg)
- 
--    fields['pointcloud'] = data.PointCloudField(cfg['data']['pointcloud_file'],
--                                                pointcloud_transform,
--                                                with_transforms=True)
-+    else:
-+        pointcloud_transform = data.SubsamplePointcloud(
-+            cfg['data']['pointcloud_target_n'])
-+        if cfg.get('sdf_generation', False):
-+            pointcloud_transform = None
-+
-+        fields['pointcloud'] = data.PointCloudField(
-+            cfg['data']['pointcloud_file'],
-+            pointcloud_transform,
-+            cfg,
-+            with_transforms=True)
-     fields['angles'] = data.SphericalCoordinateField(
-         cfg['data']['primitive_points_sample_n'],
-         mode,
-diff --git a/im2mesh/pnet/models/decoder.py b/im2mesh/pnet/models/decoder.py
-index 32fe043..32d791d 100644
---- a/im2mesh/pnet/models/decoder.py
-+++ b/im2mesh/pnet/models/decoder.py
-@@ -60,6 +60,7 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-         return_sdf=False,
-         is_radius_reg=False,
-         spherical_angles=False,
-+        extract_surface_point_by_max=False,
-         last_scale=.1):
-         super().__init__()
-         assert dim in [2, 3]
-@@ -113,6 +114,7 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-             is_feature_radius=is_feature_radius,
-             no_last_bias=no_last_bias,
-             spherical_angles=spherical_angles,
-+            extract_surface_point_by_max=extract_surface_point_by_max,
-             return_sdf=return_sdf)
-         # simple_sampler = super_shape_sampler.SuperShapeSampler(max_m,
-         #                                                        n_primitives,
-@@ -142,25 +144,26 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-                                     points=feature,
-                                     return_surface_mask=True)
-             pcoord, o1, o2, o3 = output
--            if self.is_radius_reg:
--                # B, N, P, dim
--                # pcoord
--
--                # B, N, 1, dim
--                transition = params['transition'].unsqueeze(2)
--                pcentered_coord = pcoord - transition
--                radius = (pcentered_coord**2).sum(-1).clamp(min=EPS).sqrt()
--                output = (pcoord, o1, o2, o3, radius)
--
--            else:
--                output = (pcoord, o1, o2, o3, None)
--
-         else:
-             output = self.simple_sampler(params,
-                                          thetas=angles,
-                                          coord=coord,
-                                          points=color_feature,
-                                          return_surface_mask=True)
-+            pcoord, o1, o2, o3 = output
-+        if self.is_radius_reg:
-+            # B, N, P, dim
-+            # pcoord
-+
-+            # B, N, 1, dim
-+            transition = params['transition'].unsqueeze(2)
-+            pcentered_coord = pcoord - transition
-+            radius = (pcentered_coord**2).sum(-1).clamp(min=EPS).sqrt()
-+            output = (pcoord, o1, o2, o3, radius)
-+
-+        else:
-+            output = (pcoord, o1, o2, o3, None)
-+
-         #shapes = [[10, 12, 900, 3], [10, 12, 900], [10, 12, 2048]]  #,
-         #[10, 12, 10800]]
-         """
-diff --git a/im2mesh/pnet/training.py b/im2mesh/pnet/training.py
-index 53db728..629b999 100644
---- a/im2mesh/pnet/training.py
-+++ b/im2mesh/pnet/training.py
-@@ -327,15 +327,27 @@ class Trainer(BaseTrainer):
-                     pointcloud.shape[:2], device=occ.device, dtype=occ.dtype)
-             ],
-                             axis=1)
--
-+        """
-         c = self.model.encode_inputs(inputs)
-         q_z = self.model.infer_z(points, occ, c, **kwargs)
-         z = q_z.rsample()
- 
-         # General points
- 
-+        output = self.model.decode(scaled_coord,
-+                                          z,
-+                                          c,
-+                                          angles=angles,
-+                                          **kwargs)
-+
-+        """
-         scaled_coord = points * self.pnet_point_scale
--        output = self.model.decode(scaled_coord, z, c, angles=angles, **kwargs)
-+        output = self.model(scaled_coord,
-+                            inputs,
-+                            sample=True,
-+                            angles=angles,
-+                            **kwargs)
-+
-         super_shape_point, surface_mask, sgn, sgn_BxNxNP, radius = output
- 
-         # losses
-@@ -396,11 +408,19 @@ class Trainer(BaseTrainer):
-             apply_surface_mask_before_chamfer=self.is_strict_chamfer)
- 
-         if self.is_normal_loss:
-+            """
-             output = self.model.decode(scaled_coord,
-                                        z,
-                                        c,
-                                        angles=normal_angles,
-                                        **kwargs)
-+            """
-+            output = self.model(scaled_coord,
-+                                inputs,
-+                                sample=True,
-+                                angles=normal_angles,
-+                                **kwargs)
-+
-             normal_vertices, normal_mask, _, _, _ = output
- 
-             B, N, P, D = normal_vertices.shape
-diff --git a/paper_resources/pix3d_comparison/scripts/generate_pix3d_table.py b/paper_resources/pix3d_comparison/scripts/generate_pix3d_table.py
-deleted file mode 100644
-index 54187a8..0000000
---- a/paper_resources/pix3d_comparison/scripts/generate_pix3d_table.py
-+++ /dev/null
-@@ -1,193 +0,0 @@
--# To add a new cell, type '# %%'
--# To add a new markdown cell, type '# %% [markdown]'
--# %%
--import pandas as pd
--import numpy as np
--import os
--from collections import OrderedDict
--import pickle
--import subprocess
--import glob
--
--
--def join(*args):
--    return os.path.join(*args)
--
--
--# %%
--resource_base_dir_path = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/pix3d_comparison/resources'
--fscore_values = [0.005, 0.0107337006427915, 0.05, 0.1, 0.2]
--fscore_display_values = ['0.5\%', '1\%', '5\%', '10\%', '20\%']
--sample_generation_classes = [
--    'chair', 'table', 'bed', 'bookcase', 'misc', 'sofa'
--]
--
--shapenetv1_path = '/data/unagi0/kawana/workspace/ShapeNetCore.v1'
--shapenetv2_path = '/data/unagi0/kawana/workspace/ShapeNetCore.v2'
--shapenetocc_path = '/home/mil/kawana/workspace/occupancy_networks/data/ShapeNet'
--pix3d_base_path = '/home/mil/kawana/workspace/occupancy_networks/data/Pix3D'
--rendering_script_path = '/home/mil/kawana/workspace/occupancy_networks/scripts/render_3dobj.sh'
--
--side_length_scale = 0.0107337006427915
--ours_name = 'PSNet30'
--theirs_name = 'OccNet'
--
--rendering_out_base_dir = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/pix3d_comparison'
--
--pix3d_df_path = join(pix3d_base_path, 'pix3d_*.pkl')
--rendering_out_dir = os.path.join(rendering_out_base_dir, 'resources')
--rendering_gt_mesh_cache_dir = os.path.join(rendering_out_base_dir, 'cache')
--
--id_to_dir_path_map = {
--    'OccNet':
--    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/onet_pretrained',
--    'PSNet30':
--    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_20200417_123951'
--}
--id_to_mesh_dir_name = {
--    'OccNet':
--    'pretrained_pix3d_class_agnostic_margin_224',
--    'PSNet30':
--    'generation_pix3d_class_agnostic_margin_224_explicit_20200417_163408'
--}
--id_to_fscore_map = {
--    'OccNet': 'eval_fscore_from_meshes.csv',
--    'PSNet30': 'eval_fscore_from_meshes_explicit.csv'
--}
--id_to_full_fscore_map = {
--    'OccNet': 'eval_fscore_from_meshes_full.pkl',
--    'PSNet30': 'eval_fscore_from_meshes_full_explicit.pkl'
--}
--id_to_cd1_map = {
--    'OccNet': 'eval_meshes_full.csv',
--    'PSNet30': 'eval_meshes_full_explicit.csv'
--}
--ids = list(id_to_cd1_map.keys())
--fscore_key_strs = ['fscore_th={} (mesh)'.format(val) for val in fscore_values]
--
--# %%
--data = []
--for idx in ids:
--    fdf = pd.read_csv(
--        os.path.join(id_to_dir_path_map[idx], id_to_mesh_dir_name[idx],
--                     id_to_fscore_map[idx]))
--    data.append(
--        OrderedDict({
--            disp_val: fdf[key].mean()
--            for disp_val, key in zip(fscore_display_values, fscore_key_strs)
--        }))
--df = pd.DataFrame(data, columns=fscore_display_values)
--
--# %%
--body = ""
--names = ' & '.join(['Threshold(%)', *df.columns]) + " \\ \hline"
--body += names
--for idx in range(len(df)):
--    method_id = ids[idx]
--
--    def cutdeci(s):
--        if isinstance(s, str):
--            return s
--        return "{:.3f}".format(s)
--
--    els = [method_id, *map(cutdeci, df.loc[idx].tolist())]
--    row = ' & '.join(els) + " \\ "
--    body += ('\n' + row)
--
--print(body)
--
--with open(os.path.join(resource_base_dir_path, 'table.txt'), 'w') as f:
--    print(body.replace('\\', '\\\\').replace('\\\h',
--                                             '\h').replace('\\\mu', '\mu'),
--          file=f)
--
--# %%
--
--synset_to_label = {
--    '04256520': 'sofa',
--    '04379243': 'table',
--    '02691156': 'bed',
--    '02828884': 'bookcase',
--    '02933112': 'desk',
--    '02958343': 'misc',
--    '03001627': 'chair',
--    '03211117': 'tool',
--    '03636649': 'wardrobe'
--}
--
--label_to_synset = {v: k for k, v in synset_to_label.items()}
--
--# %%
--oursdf = pickle.load(
--    open(
--        os.path.join(id_to_dir_path_map[ours_name],
--                     id_to_mesh_dir_name[ours_name],
--                     id_to_full_fscore_map[ours_name]), 'rb'))
--
--theirsdf = pickle.load(
--    open(
--        os.path.join(id_to_dir_path_map[theirs_name],
--                     id_to_mesh_dir_name[theirs_name],
--                     id_to_full_fscore_map[theirs_name]), 'rb'))
--
--# %%
--pix3d_df_paths = glob.glob(pix3d_df_path)
--dfs = []
--for path in pix3d_df_paths:
--    pix3d_df = pickle.load(open(path, 'rb'))
--    dfs.append(pix3d_df)
--pix3ddf = pd.concat(dfs)
--
--for class_name in sample_generation_classes:
--    class_id = label_to_synset[class_name]
--
--    oursdf_cls = oursdf[oursdf['class id'] == class_id]
--    theirsdf_cls = theirsdf[theirsdf['class id'] == class_id]
--
--    assert len(oursdf_cls) == len(theirsdf_cls), (len(oursdf_cls),
--                                                  len(theirsdf_cls))
--
--    fscore_key = 'fscore_th={} (mesh)'.format(0.005)
--    """
--    oursdf_cls['diff'] = (oursdf_cls[fscore_key] - theirsdf_cls[fscore_key])
--
--    filter = oursdf_cls['diff'] > 0
--    idx = oursdf_cls[filter]['diff'].argmax()
--    model_id = oursdf_cls[filter]['modelname'].iloc[idx]
--    """
--
--    model_idx = oursdf_cls[fscore_key].argmax()
--    print(model_idx, class_name)
--    print(oursdf_cls['modelname'].iloc[model_idx])
--    model_id = oursdf_cls['modelname'].iloc[model_idx]
--
--    #filter = oursdf_cls[fscore_key] > 0.5
--    #idx = oursdf_cls[filter]['diff'].argmax()
--    #model_id = oursdf_cls[filter]['modelname'].iloc[idx]
--
--    model_paths = {
--        idx: os.path.join(id_to_dir_path_map[idx], id_to_mesh_dir_name[idx],
--                          'meshes', label_to_synset[class_name],
--                          str(model_id) + '.off')
--        for idx in ids
--    }
--
--    model_paths['gt'] = os.path.join(pix3d_base_path, class_id, model_id,
--                                     'model.off')
--
--    if not os.path.exists(rendering_out_dir):
--        os.makedirs(rendering_out_dir)
--
--    camera_param_path = os.path.join(rendering_out_base_dir,
--                                     'camera_param.txt')
--    for idx in model_paths:
--        command = 'sh {script} {camera_param} {model} {out_dir} {idx}'.format(
--            script=rendering_script_path,
--            camera_param=camera_param_path,
--            model=model_paths[idx],
--            out_dir=rendering_out_dir,
--            idx=idx + '_' + class_name)
--        print(command)
--        subprocess.run(command, shell=True)
--
--# %%
-diff --git a/paper_resources/radius_distribution/scripts/generate_radius_distribution.py b/paper_resources/radius_distribution/scripts/generate_radius_distribution.py
-index ce00069..94a36b5 100644
---- a/paper_resources/radius_distribution/scripts/generate_radius_distribution.py
-+++ b/paper_resources/radius_distribution/scripts/generate_radius_distribution.py
-@@ -1,5 +1,6 @@
- # %%
- import argparse
-+import torch
- import os
- import subprocess
- import hashlib
-@@ -9,33 +10,30 @@ import numpy as np
- from datetime import datetime
- from tqdm import tqdm
- import sys
--sys.path.insert(0, '/home/mil/kawana/workspace/occupancy_networks/im2mesh')
--from im2mesh import config, data
--from im2mesh.checkpoints import CheckpointIO
- import shutil
- import yaml
-+import matplotlib.pyplot as plt
- from collections import OrderedDict
-+sys.path.insert(0, '/home/mil/kawana/workspace/occupancy_networks')
-+sys.path.insert(
-+    0, '/home/mil/kawana/workspace/occupancy_networks/external/atlasnetv2')
-+sys.path.insert(
-+    0,
-+    '/home/mil/kawana/workspace/occupancy_networks/external/periodic_shapes')
-+from im2mesh import config, data
-+from im2mesh.checkpoints import CheckpointIO
- 
- # %%
--config_path = ''
-+config_path = '/home/mil/kawana/workspace/occupancy_networks/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml'
-+"""
-+config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_20200414_051415/config.yaml'
-+"""
- resource_path = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/radius_distribution/resources'
--use_cache_flag = False
--
--# %%
--
--
--def represent_odict(dumper, instance):
--    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
--
--
--yaml.add_representer(OrderedDict, represent_odict)
--
--
--def construct_odict(loader, node):
--    return OrderedDict(loader.construct_pairs(node))
- 
-+project_base_path = '/home/mil/kawana/workspace/occupancy_networks'
-+use_cache_flag = False
- 
--yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
-+os.chdir(project_base_path)
- 
- # %%
- # Get configuration and basic arguments
-@@ -43,7 +41,9 @@ cfg = config.load_config(config_path, 'configs/default.yaml')
- cfg['data']['icosahedron_uv_margin'] = 0
- cfg['data']['icosahedron_uv_margin_phi'] = 0
- cfg['data']['points_subsample'] = 2
--cfg['data']['debug'] = {'sample_n': 50}
-+# Enable to get radius
-+cfg['trainer']['is_radius_reg'] = True
-+cfg['data']['debug'] = {'sample_n': 10}
- 
- is_cuda = torch.cuda.is_available()
- device = torch.device("cuda" if is_cuda else "cpu")
-@@ -67,7 +67,6 @@ trainer = config.get_trainer(model, None, cfg, device=device)
- 
- # Print model
- nparameters = sum(p.numel() for p in model.parameters())
--print(model)
- print('Total number of parameters: %d' % nparameters)
- 
- # Evaluate
-@@ -85,6 +84,9 @@ thetas = np.linspace(-np.pi, np.pi - 2 * np.pi / 100, 100)
- phis = np.linspace(-np.pi / 2, np.pi / 2 - np.pi / 100, 100)
- theta_radius_list = []
- phi_radius_list = []
-+theta_intra_primitive_radius_list_std = []
-+theta_intra_primitive_radius_list_mean = []
-+phi_intra_primitive_radius_list = []
- # Handle each dataset separately
- for it, data in enumerate(tqdm(test_loader)):
-     if data is None:
-@@ -112,40 +114,109 @@ for it, data in enumerate(tqdm(test_loader)):
-         'class name': category_name,
-         'modelname': modelname,
-     }
-+    points = data.get('points').to(device)
-     inputs = data.get('inputs', torch.empty(points.size(0), 0)).to(device)
--    theta = torch.meshgrid(
--        [torch.tensor(thetas, device=device),
--         torch.zeros([1], device=device)],
--        device=device).unsqueeze(0)
--    phi = torch.meshgrid(
--        [torch.zeros([1], device=device),
--         torch.tensor(phis, device=device)],
--        device=device).unsqueeze(0)
-+    theta = torch.cat(torch.meshgrid([
-+        torch.tensor(thetas, device=device).float(),
-+        torch.zeros([1], device=device).float()
-+    ], ),
-+                      axis=-1).unsqueeze(0)
-+    phi = torch.cat(torch.meshgrid([
-+        torch.tensor(phis, device=device).float(),
-+        torch.zeros([1], device=device).float()
-+    ], ),
-+                    axis=-1)[..., [1, 0]].unsqueeze(0)
-     points = data.get('points').to(device)
- 
-     feature = model.encode_inputs(inputs)
- 
-     kwargs = {}
--    scaled_coord = points * cfg['trainer']['pnet_point_scale']
-+    point_scale = cfg['trainer']['pnet_point_scale']
-+
-+    scaled_coord = points * point_scale
-     output_theta = model.decode(scaled_coord,
-                                 None,
-                                 feature,
-                                 angles=theta,
-                                 **kwargs)
-     _, _, _, _, theta_radius = output_theta
--    theta_radius_list.append(theta_radius[:, :, :,
--                                          0].mean(axis=(0, 1)).cpu().numpy())
-+    theta_radius /= point_scale
-+    theta_radius_list.append(
-+        theta_radius.mean(axis=(0, 1)).cpu().detach().numpy())
-+    theta_intra_primitive_radius_list_std.append(
-+        theta_radius.std(axis=2).mean(axis=(0)).cpu().detach().numpy())
-+    theta_intra_primitive_radius_list_mean.append(
-+        theta_radius.mean(axis=2).mean(axis=(0)).cpu().detach().numpy())
-     output_phi = model.decode(scaled_coord,
-                               None,
-                               feature,
-                               angles=phi,
-                               **kwargs)
-     _, _, _, _, phi_radius = output_phi
--    phi_radius_list.append(phi_radius[:, :, :,
--                                      1].mean(axis=(0, 1)).cpu().numpy())
--
-+    phi_radius /= point_scale
-+    phi_radius_list.append(phi_radius.mean(axis=(0, 1)).cpu().detach().numpy())
-+    phi_intra_primitive_radius_list.append(
-+        phi_radius.std(axis=2).mean(axis=(0)).cpu().detach().numpy())
-+# %%
-+thetas_in_deg = thetas / np.pi * 180
- # Create pandas dataframe and save
--theta_df = pd.DataFrame(theta_radius_list, columns=thetas).mean()
--theta_df.to_pickle('test')
-+thetas_mean_df = pd.DataFrame(theta_radius_list, columns=thetas_in_deg).mean()
-+thetas_std_df = pd.DataFrame(theta_radius_list, columns=thetas_in_deg).std()
-+
-+thetas_df = pd.DataFrame()
-+thetas_df['deg'] = thetas_in_deg
-+thetas_df['mean'] = thetas_mean_df.values
-+thetas_df['std'] = thetas_std_df.values
-+#theta_df.to_pickle('test')
- 
--print(theta_df)
-+phis_in_deg = phis / np.pi * 180
-+# Create pandas dataframe and save
-+phis_mean_df = pd.DataFrame(phi_radius_list, columns=phis_in_deg).mean()
-+phis_std_df = pd.DataFrame(phi_radius_list, columns=phis_in_deg).std()
-+
-+phis_df = pd.DataFrame()
-+phis_df['deg'] = phis_in_deg
-+phis_df['mean'] = phis_mean_df.values
-+phis_df['std'] = phis_std_df.values
-+#theta_df.to_pickle('test')
-+
-+thetas_intra_df_mean = pd.DataFrame(
-+    theta_intra_primitive_radius_list_mean).mean()
-+thetas_intra_df_std = pd.DataFrame(
-+    theta_intra_primitive_radius_list_std).mean()
-+
-+thetas_intra_df = pd.DataFrame()
-+thetas_intra_df['deg'] = np.arange(0, len(thetas_intra_df_mean))
-+thetas_intra_df['mean'] = thetas_intra_df_mean.values
-+thetas_intra_df['std'] = thetas_intra_df_std.values
-+# %%
-+fig = plt.figure()
-+degs = thetas_df['deg']
-+band = thetas_df['std'] * 2
-+mean = thetas_df['mean']
-+ax = fig.add_subplot(1, 2, 1)
-+ax.plot(degs, mean)
-+ax.fill_between(degs, mean - band, mean + band, color='gray', alpha=0.2)
-+ax.set_xticks([-180, -90, 0, 90, 180])
-+ax.set_xticklabels(['$-\pi$', '$-\pi/2$', '0', '$\pi/2$', '$\pi$'])
-+ax.legend(['mean', '$2\sigma$'])
-+
-+degs = phis_df['deg']
-+band = phis_df['std'] * 2
-+mean = phis_df['mean']
-+ax = fig.add_subplot(1, 2, 2)
-+ax.plot(degs, mean)
-+ax.fill_between(degs, mean - band, mean + band, color='gray', alpha=0.2)
-+ax.set_xticks([-90, 0, 90])
-+ax.set_xticklabels(['$-\pi/2$', '0', '$\pi/2$'])
-+ax.legend(['mean', '$2\sigma$'])
-+
-+fig.show()
-+
-+fig = plt.figure()
-+ax = fig.add_subplot(1, 2, 1)
-+degs = thetas_intra_df['deg']
-+band = thetas_intra_df['std'] * 2
-+mean = thetas_intra_df['mean']
-+ax = fig.add_subplot(1, 2, 1)
-+plt.errorbar(degs, mean, yerr=band)
-diff --git a/scripts/render_3dobj.sh b/scripts/render_3dobj.sh
-index fb2adbf..61ed991 100755
---- a/scripts/render_3dobj.sh
-+++ b/scripts/render_3dobj.sh
-@@ -4,6 +4,7 @@ param=$1
- model_path=$2
- out=$3
- name=$4
-+skip_reconvert_flag=${5:-"false"}
- RENDER_FOR_CNN_PATH=/home/mil/kawana/workspace/RenderForCNN
- 
- PYTHONPATH=$PYTHONPATH:RENDER_FOR_CNN_PATH \
-@@ -11,5 +12,6 @@ PYTHONPATH=$PYTHONPATH:RENDER_FOR_CNN_PATH \
- ${RENDER_FOR_CNN_PATH}/render_pipeline/blank.blend \
- --background \
- --python ${RENDER_FOR_CNN_PATH}/render_pipeline/render_model_views.py  \
-+${skip_reconvert_flag} \
- ${model_path} \
- ${name}  ${param} ${out}
-diff --git a/train.py b/train.py
-index d761de2..6a2c7d9 100755
---- a/train.py
-+++ b/train.py
-@@ -12,13 +12,18 @@ from im2mesh.checkpoints import CheckpointIO
- import wandb
- import dotenv
- 
--dotenv.load_dotenv(verbose=True)
-+dotenv.load_dotenv('/home/mil/kawana/workspace/occupancy_networks/.env',
-+                   verbose=True)
-+os.environ['WANDB_PROJECT'] = 'periodic_shape_occupancy_networks'
- 
- # Arguments
- parser = argparse.ArgumentParser(
-     description='Train a 3D reconstruction model.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
-+parser.add_argument('--data_parallel',
-+                    action='store_true',
-+                    help='Train with data parallel.')
- parser.add_argument(
-     '--exit-after',
-     type=int,
-@@ -35,7 +40,10 @@ device = torch.device("cuda" if is_cuda else "cpu")
- t0 = time.time()
- 
- # Shorthands
--out_dir = cfg['training']['out_dir']
-+#out_dir = cfg['training']['out_dir']
-+out_dir = os.path.join('out', cfg['data']['input_type'],
-+                       os.path.basename(args.config).split('.')[0])
-+cfg['training']['out_dir'] = out_dir
- batch_size = cfg['training']['batch_size']
- backup_every = cfg['training']['backup_every']
- exit_after = args.exit_after
-@@ -62,8 +70,13 @@ if 'debug' in cfg['data']:
- else:
-     train_shuffle = True
- 
-+if args.data_parallel:
-+    dist_coef = 1
-+else:
-+    dist_coef = 1
-+
- train_loader = torch.utils.data.DataLoader(train_dataset,
--                                           batch_size=batch_size,
-+                                           batch_size=batch_size * dist_coef,
-                                            num_workers=4,
-                                            shuffle=train_shuffle,
-                                            collate_fn=data.collate_remove_none,
-@@ -72,37 +85,44 @@ train_loader = torch.utils.data.DataLoader(train_dataset,
- 
- val_loader = torch.utils.data.DataLoader(
-     val_dataset,
--    batch_size=cfg['training']['val_batch_size'],
-+    batch_size=cfg['training']['val_batch_size'] * dist_coef,
-     num_workers=4,
-     shuffle=False,
-     collate_fn=data.collate_remove_none,
-     worker_init_fn=data.worker_init_fn)
- 
- # For visualizations
--vis_loader = torch.utils.data.DataLoader(val_dataset,
--                                         batch_size=cfg['training'].get(
--                                             'vis_batch_size', 1),
--                                         shuffle=False,
--                                         collate_fn=data.collate_remove_none,
--                                         worker_init_fn=data.worker_init_fn)
-+vis_loader = torch.utils.data.DataLoader(
-+    val_dataset,
-+    batch_size=cfg['training'].get('vis_batch_size', 1) * dist_coef,
-+    shuffle=False,
-+    collate_fn=data.collate_remove_none,
-+    worker_init_fn=data.worker_init_fn)
- data_vis = next(iter(vis_loader))
- 
- # Model
- model = config.get_model(cfg, device=device, dataset=train_dataset)
--
- # Intialize training
- npoints = 1000
--learning_rate = float(cfg['training'].get('learning_rate', 1e-4))
--optimizer = optim.Adam(model.parameters(), lr=learning_rate)
--# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)
--
--trainer = config.get_trainer(model, optimizer, cfg, device=device)
-+learning_rate = float(cfg['training'].get('learning_rate', 1e-4)) * dist_coef
-+if not args.data_parallel:
-+    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
-+    trainer = config.get_trainer(model, optimizer, cfg, device=device)
- 
- if cfg['training'].get('skip_load_pretrained_optimizer', False):
-     print('skip loading optimizer')
-     checkpoint_io = CheckpointIO(out_dir, model=model)
--else:
-+elif not args.data_parallel:
-     checkpoint_io = CheckpointIO(out_dir, model=model, optimizer=optimizer)
-+else:
-+    assert args.data_parallel and not cfg['training'].get(
-+        'skip_load_pretrained_optimizer', False)
-+    checkpoint_io = CheckpointIO(out_dir, model=model)
-+    model = torch.nn.DataParallel(model)
-+    torch.backends.cudnn.benchmark = True
-+    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
-+    CheckpointIO(out_dir, optimizer=optimizer)
-+    trainer = config.get_trainer(model, optimizer, cfg, device=device)
- try:
-     load_dict = checkpoint_io.load('model.pt')
- except FileExistsError:
diff --git a/configs/img/generation_pix3d_class_agnostic_margin_224_explicit_20200420_005849/gen_diff.patch b/configs/img/generation_pix3d_class_agnostic_margin_224_explicit_20200420_005849/gen_diff.patch
deleted file mode 100644
index 668a390..0000000
--- a/configs/img/generation_pix3d_class_agnostic_margin_224_explicit_20200420_005849/gen_diff.patch
+++ /dev/null
@@ -1,2755 +0,0 @@
-diff --git a/configs/default.yaml b/configs/default.yaml
-index 556a071..52c7f35 100644
---- a/configs/default.yaml
-+++ b/configs/default.yaml
-@@ -8,6 +8,7 @@ data:
-   val_split: val
-   test_split: test
-   dim: 3
-+  sdf_points_file: sdf_points.npz
-   points_file: points.npz
-   points_iou_file: points.npz
-   points_subsample: 1024
-@@ -42,6 +43,7 @@ training:
-   out_dir:  out/default
-   batch_size: 64
-   val_batch_size: 64
-+  vis_batch_size: 5
-   print_every: 10
-   visualize_every: 2000
-   checkpoint_every: 1000
-@@ -58,6 +60,8 @@ test:
-   threshold: 0.5
-   eval_mesh: true
-   eval_pointcloud: true
-+  eval_fscore: true
-+  fscore_thresholds: [0.005, 0.01, 0.0107337006427915, 0.02, 0.05, 0.1, 0.2]
-   model_file: model_best.pt
- generation:
-   batch_size: 100000
-diff --git a/configs/img/debug_pnet.yaml b/configs/img/debug_pnet.yaml
-index 6b8c891..54766e0 100644
---- a/configs/img/debug_pnet.yaml
-+++ b/configs/img/debug_pnet.yaml
-@@ -21,8 +21,8 @@ model:
- training:
-   out_dir:  out/img/test
-   batch_size: 10
--  visualize_every: 500
--  validate_every: 100
-+  visualize_every: 100
-+  validate_every: 1000
-   checkpoint_every: 10000
-   wandb_resume: null
-   skip_load_pretrained_optimizer: true
-diff --git a/configs/img/onet_legacy_pretrained.yaml b/configs/img/onet_legacy_pretrained.yaml
-index a74775e..fa9772b 100644
---- a/configs/img/onet_legacy_pretrained.yaml
-+++ b/configs/img/onet_legacy_pretrained.yaml
-@@ -1,11 +1,11 @@
--inherit_from: configs/img/onet.yaml
-+inherit_from: configs/img/onet_original.yaml
- data:
-   img_augment: true
- model:
-   decoder_kwargs:
-     legacy: true
- training:
--  out_dir:  out/img/onet_legacy
-+  out_dir:  out/submission/eval/img/onet_legacy
- test:
-   model_file: https://s3.eu-central-1.amazonaws.com/avg-projects/occupancy_networks/models/onet_img2mesh-0c7780d1.pt
- generation:
-diff --git a/configs/img/onet_pretrained.yaml b/configs/img/onet_pretrained.yaml
-index 2585eef..9135757 100644
---- a/configs/img/onet_pretrained.yaml
-+++ b/configs/img/onet_pretrained.yaml
-@@ -1,5 +1,7 @@
--inherit_from: configs/img/onet.yaml
-+inherit_from: configs/img/onet_original.yaml
- test:
-   model_file: https://s3.eu-central-1.amazonaws.com/avg-projects/occupancy_networks/models/onet_img2mesh_3-f786b04a.pt
-+training:
-+  out_dir:  out/submission/eval/img/onet_pretrained
- generation:
-   generation_dir: pretrained
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10.yaml b/configs/img/pnet_finetue_only_transition_cceff10.yaml
-index c9a8d9f..527f6a5 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10.yaml
-@@ -31,6 +31,7 @@ model:
-     is_feature_radius: false
-     is_feature_coord: true
-     is_feature_angles: false
-+    layer_depth: 0
-     disable_learn_pose_but_transition: true
- trainer:
-   overlap_reg_coef: 1
-@@ -53,6 +54,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: bve2kmpq
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
-index 10fd5ed..3455101 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
-@@ -5,7 +5,7 @@ data:
-   img_size: 224 
-   points_subsample: 2048 
-   pointcloud_target_n: 4096
--  primitive_points_sample_n: 20 # will be powered by dim - 1, if dim == 3, then 900
-+  primitive_points_sample_n: 12 # will be powered by dim - 1, if dim == 3, then 900
-   is_normal_icosahedron: false
-   icosahedron_subdiv: 2
-   icosahedron_uv_margin: 0.00001
-@@ -45,7 +45,7 @@ trainer:
- training:
-   out_dir:  out/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg
-   batch_size: 20
--  val_batch_size: 2
-+  val_batch_size: 1
-   model_selection_metric: iou
-   model_selection_mode: maximize
-   visualize_every: 1000
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml
-index ec3e6bd..dcd8812 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: 7o949ydv
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml
-index 1c635a8..d7e1eb6 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml
-@@ -57,6 +57,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: bvovwjdm
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml
-index d612575..faa0696 100644
---- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml
-+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml
-@@ -54,6 +54,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: false
-   learning_rate: 1e-4
-+  wandb_resume: g0d0yzjc
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml b/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml
-index b0d390f..6ac9b38 100644
---- a/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml
-+++ b/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml
-@@ -53,6 +53,7 @@ training:
-   checkpoint_every: 10000
-   skip_load_pretrained_optimizer: true
-   learning_rate: 1e-4
-+  wandb_resume: e5w6ge0h
- test:
-   threshold: 0.5
-   eval_mesh: true
-diff --git a/data/.gitkeep b/data/.gitkeep
-deleted file mode 100644
-index e69de29..0000000
-diff --git a/data/metadata.yaml b/data/metadata.yaml
-deleted file mode 100644
-index 1b06258..0000000
---- a/data/metadata.yaml
-+++ /dev/null
-@@ -1,54 +0,0 @@
--{
--    "04256520": {
--        "id": "04256520",
--        "name": "sofa,couch,lounge"
--    },
--    "02691156": {
--        "id": "02691156",
--        "name": "airplane,aeroplane,plane"
--    },
--    "03636649": {
--        "id": "03636649",
--        "name": "lamp"
--    },
--    "04401088": {
--        "id": "04401088",
--        "name": "telephone,phone,telephone set"
--    },
--    "04530566": {
--        "id": "04530566",
--        "name": "vessel,watercraft"
--    },
--    "03691459": {
--        "id": "03691459",
--        "name": "loudspeaker,speaker,speaker unit,loudspeaker system,speaker system"
--    },
--    "03001627": {
--        "id": "03001627",
--        "name": "chair"
--    },
--    "02933112": {
--        "id": "02933112",
--        "name": "cabinet"
--    },
--    "04379243": {
--        "id": "04379243",
--        "name": "table"
--    },
--    "03211117": {
--        "id": "03211117",
--        "name": "display,video display"
--    },
--    "02958343": {
--        "id": "02958343",
--        "name": "car,auto,automobile,machine,motorcar"
--    },
--    "02828884": {
--        "id": "02828884",
--        "name": "bench"
--    },
--    "04090263": {
--        "id": "04090263",
--        "name": "rifle"
--    }
--}
-diff --git a/dist_train.py b/dist_train.py
-index fc82025..d5a82df 100755
---- a/dist_train.py
-+++ b/dist_train.py
-@@ -75,7 +75,7 @@ def train(gpu_idx, args):
-     if rank == 0:
-         val_loader = torch.utils.data.DataLoader(
-             val_dataset,
--            batch_size=2,
-+            batch_size=cfg['training']['val_batch_size'],
-             num_workers=4,
-             shuffle=False,
-             collate_fn=data.collate_remove_none,
-@@ -84,7 +84,7 @@ def train(gpu_idx, args):
-         # For visualizations
-         vis_loader = torch.utils.data.DataLoader(
-             val_dataset,
--            batch_size=12,
-+            batch_size=cfg['training']['vis_batch_size'],
-             shuffle=False,
-             collate_fn=data.collate_remove_none,
-             worker_init_fn=data.worker_init_fn)
-diff --git a/eval.py b/eval.py
-index 07c4f36..a17d80f 100644
---- a/eval.py
-+++ b/eval.py
-@@ -1,27 +1,83 @@
- import argparse
- import os
-+import subprocess
-+import hashlib
- import pandas as pd
- import torch
- import numpy as np
-+from datetime import datetime
- from tqdm import tqdm
- from im2mesh import config, data
- from im2mesh.checkpoints import CheckpointIO
-+import shutil
-+import yaml
-+from collections import OrderedDict
- 
- 
--parser = argparse.ArgumentParser(
--    description='Evaluate mesh algorithms.'
--)
-+def represent_odict(dumper, instance):
-+    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
-+
-+
-+yaml.add_representer(OrderedDict, represent_odict)
-+
-+
-+def construct_odict(loader, node):
-+    return OrderedDict(loader.construct_pairs(node))
-+
-+
-+yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
-+
-+parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
- 
- # Get configuration and basic arguments
--args = parser.parse_args()
-+args, unknown_args = parser.parse_known_args()
- cfg = config.load_config(args.config, 'configs/default.yaml')
-+for idx, arg in enumerate(unknown_args):
-+    if arg.startswith('--'):
-+        arg = arg.replace('--', '')
-+        value = unknown_args[idx + 1]
-+        keys = arg.split('.')
-+        if keys[0] not in cfg:
-+            cfg[keys[0]] = {}
-+        child_cfg = cfg.get(keys[0], {})
-+        for key in keys[1:]:
-+            item = child_cfg.get(key, None)
-+            if isinstance(item, dict):
-+                child_cfg = item
-+            elif item is None:
-+                child_cfg[key] = value
-+            else:
-+                child_cfg[key] = type(item)(value)
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
- # Shorthands
--out_dir = cfg['training']['out_dir']
-+if '--dontcopy' in unknown_args:
-+    out_dir = cfg['training']['out_dir']
-+else:
-+    base_out_dir = cfg['training']['out_dir']
-+    out_dir = os.path.join(
-+        os.path.dirname(base_out_dir).replace('out', 'out/submission/eval'),
-+        os.path.basename(base_out_dir)) + '_' + datetime.now().strftime(
-+            ('%Y%m%d_%H%M%S'))
-+print('out dir for eval: ', out_dir)
-+if not '--dontcopy' in unknown_args:
-+    if not os.path.exists(out_dir):
-+        shutil.copytree(base_out_dir, out_dir)
-+    else:
-+        raise ValueError('out dir already exists')
-+
-+if not '--dontcopy' in unknown_args:
-+    patch_path = os.path.join(out_dir, 'diff.patch')
-+    subprocess.run('git diff > {}'.format(patch_path), shell=True)
-+    weight_path = os.path.join(out_dir, cfg['test']['model_file'])
-+    with open(weight_path, 'rb') as f:
-+        md5 = hashlib.md5(f.read()).hexdigest()
-+    cfg['test']['model_file_hash'] = md5
-+    yaml.dump(cfg, open(os.path.join(out_dir, 'config.yaml'), 'w'))
-+
- out_file = os.path.join(out_dir, 'eval_full.pkl')
- out_file_class = os.path.join(out_dir, 'eval.csv')
- 
-@@ -47,14 +103,13 @@ print('Total number of parameters: %d' % nparameters)
- # Evaluate
- model.eval()
- 
--eval_dicts = []   
-+eval_dicts = []
- print('Evaluating networks...')
--
--
--test_loader = torch.utils.data.DataLoader(
--    dataset, batch_size=1, shuffle=False,
--    collate_fn=data.collate_remove_none,
--    worker_init_fn=data.worker_init_fn)
-+test_loader = torch.utils.data.DataLoader(dataset,
-+                                          batch_size=1,
-+                                          shuffle=False,
-+                                          collate_fn=data.collate_remove_none,
-+                                          worker_init_fn=data.worker_init_fn)
- 
- # Handle each dataset separately
- for it, data in enumerate(tqdm(test_loader)):
-@@ -68,7 +123,7 @@ for it, data in enumerate(tqdm(test_loader)):
-         model_dict = dataset.get_model_dict(idx)
-     except AttributeError:
-         model_dict = {'model': str(idx), 'category': 'n/a'}
--    
-+
-     modelname = model_dict['model']
-     category_id = model_dict['category']
- 
-@@ -81,13 +136,12 @@ for it, data in enumerate(tqdm(test_loader)):
-         'idx': idx,
-         'class id': category_id,
-         'class name': category_name,
--        'modelname':modelname,
-+        'modelname': modelname,
-     }
-     eval_dicts.append(eval_dict)
-     eval_data = trainer.eval_step(data)
-     eval_dict.update(eval_data)
- 
--
- # Create pandas dataframe and save
- eval_df = pd.DataFrame(eval_dicts)
- eval_df.set_index(['idx'], inplace=True)
-@@ -99,4 +153,4 @@ eval_df_class.to_csv(out_file_class)
- 
- # Print results
- eval_df_class.loc['mean'] = eval_df_class.mean()
--print(eval_df_class)
-\ No newline at end of file
-+print(eval_df_class)
-diff --git a/eval_meshes.py b/eval_meshes.py
-index cba5ce0..44e5d88 100644
---- a/eval_meshes.py
-+++ b/eval_meshes.py
-@@ -5,17 +5,16 @@ from tqdm import tqdm
- import pandas as pd
- import trimesh
- import torch
-+from torch.utils import data as torch_data
- from im2mesh import config, data
- from im2mesh.eval import MeshEvaluator
- from im2mesh.utils.io import load_pointcloud
--
--
--parser = argparse.ArgumentParser(
--    description='Evaluate mesh algorithms.'
--)
-+import numpy as np
-+parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
--parser.add_argument('--eval_input', action='store_true',
-+parser.add_argument('--eval_input',
-+                    action='store_true',
-                     help='Evaluate inputs instead.')
- 
- args = parser.parse_args()
-@@ -23,24 +22,33 @@ cfg = config.load_config(args.config, 'configs/default.yaml')
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
-+is_eval_explicit_mesh = cfg['test'].get('is_eval_explicit_mesh', False)
-+
- # Shorthands
--out_dir = cfg['training']['out_dir']
-+out_dir = os.path.dirname(args.config)
-+#out_dir = cfg['training']['out_dir']
- generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
- if not args.eval_input:
--    out_file = os.path.join(generation_dir, 'eval_meshes_full.pkl')
--    out_file_class = os.path.join(generation_dir, 'eval_meshes.csv')
-+    out_file = os.path.join(
-+        generation_dir, 'eval_meshes_full{}.pkl'.format(
-+            '_explicit' if is_eval_explicit_mesh else ''))
-+    out_file_class = os.path.join(
-+        generation_dir, 'eval_meshes{}.csv'.format(
-+            '_explicit' if is_eval_explicit_mesh else ''))
- else:
--    out_file = os.path.join(generation_dir, 'eval_input_full.pkl')
--    out_file_class = os.path.join(generation_dir, 'eval_input.csv')
-+    out_file = os.path.join(
-+        generation_dir, 'eval_input_full{}.pkl'.format(
-+            '_explicit' if is_eval_explicit_mesh else ''))
-+    out_file_class = os.path.join(
-+        generation_dir, 'eval_input{}.csv'.format(
-+            '_explicit' if is_eval_explicit_mesh else ''))
- 
- # Dataset
- points_field = data.PointsField(
--    cfg['data']['points_iou_file'], 
-+    cfg['data']['points_iou_file'],
-     unpackbits=cfg['data']['points_unpackbits'],
- )
--pointcloud_field = data.PointCloudField(
--    cfg['data']['pointcloud_chamfer_file']
--)
-+pointcloud_field = data.PointCloudField(cfg['data']['pointcloud_chamfer_file'])
- fields = {
-     'points_iou': points_field,
-     'pointcloud_chamfer': pointcloud_field,
-@@ -50,17 +58,23 @@ fields = {
- print('Test split: ', cfg['data']['test_split'])
- 
- dataset_folder = cfg['data']['path']
--dataset = data.Shapes3dDataset(
--    dataset_folder, fields,
--    cfg['data']['test_split'],
--    categories=cfg['data']['classes'])
-+dataset = data.Shapes3dDataset(dataset_folder,
-+                               fields,
-+                               cfg['data']['test_split'],
-+                               categories=cfg['data']['classes'])
-+
-+if 'debug' in cfg['data']:
-+    dataset = torch_data.Subset(dataset,
-+                                range(cfg['data']['debug']['sample_n']))
- 
- # Evaluator
- evaluator = MeshEvaluator(n_points=100000)
- 
- # Loader
--test_loader = torch.utils.data.DataLoader(
--    dataset, batch_size=1, num_workers=0, shuffle=False)
-+test_loader = torch.utils.data.DataLoader(dataset,
-+                                          batch_size=1,
-+                                          num_workers=0,
-+                                          shuffle=False)
- 
- # Evaluate all classes
- eval_dicts = []
-@@ -85,7 +99,7 @@ for it, data in enumerate(tqdm(test_loader)):
-         model_dict = dataset.get_model_dict(idx)
-     except AttributeError:
-         model_dict = {'model': str(idx), 'category': 'n/a'}
--    
-+
-     modelname = model_dict['model']
-     category_id = model_dict['category']
- 
-@@ -118,10 +132,24 @@ for it, data in enumerate(tqdm(test_loader)):
-     if cfg['test']['eval_mesh']:
-         mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
- 
-+        if is_eval_explicit_mesh:
-+            visbility_file = os.path.join(
-+                mesh_dir, '%s_vertex_visbility.npz' % modelname)
-         if os.path.exists(mesh_file):
-             mesh = trimesh.load(mesh_file, process=False)
-+            if is_eval_explicit_mesh:
-+                vertex_visibility = np.load(
-+                    visbility_file)['vertex_visibility']
-+            else:
-+                vertex_visibility = None
-             eval_dict_mesh = evaluator.eval_mesh(
--                mesh, pointcloud_tgt, normals_tgt, points_tgt, occ_tgt)
-+                mesh,
-+                pointcloud_tgt,
-+                normals_tgt,
-+                points_tgt,
-+                occ_tgt,
-+                is_eval_explicit_mesh=is_eval_explicit_mesh,
-+                vertex_visibility=vertex_visibility)
-             for k, v in eval_dict_mesh.items():
-                 eval_dict[k + ' (mesh)'] = v
-         else:
-@@ -129,18 +157,16 @@ for it, data in enumerate(tqdm(test_loader)):
- 
-     # Evaluate point cloud
-     if cfg['test']['eval_pointcloud']:
--        pointcloud_file = os.path.join(
--            pointcloud_dir, '%s.ply' % modelname)
-+        pointcloud_file = os.path.join(pointcloud_dir, '%s.ply' % modelname)
- 
-         if os.path.exists(pointcloud_file):
-             pointcloud = load_pointcloud(pointcloud_file)
--            eval_dict_pcl = evaluator.eval_pointcloud(
--                pointcloud, pointcloud_tgt)
-+            eval_dict_pcl = evaluator.eval_pointcloud(pointcloud,
-+                                                      pointcloud_tgt)
-             for k, v in eval_dict_pcl.items():
-                 eval_dict[k + ' (pcl)'] = v
-         else:
--            print('Warning: pointcloud does not exist: %s'
--                    % pointcloud_file)
-+            print('Warning: pointcloud does not exist: %s' % pointcloud_file)
- 
- # Create pandas dataframe and save
- eval_df = pd.DataFrame(eval_dicts)
-diff --git a/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py b/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py
-index 16a8f4a..935495f 100644
---- a/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py
-+++ b/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py
-@@ -9,6 +9,7 @@ def polar2cartesian(radius, angles):
-         r: radius (B, N, P, 1 or 2) last dim is one in 2D mode, two in 3D.
-         angles: angle (B, 1, P, 1 or 2) 
-     """
-+    print('radius angles in s2c mean', radius[..., -1].mean(), angles.mean())
-     dim = radius.shape[-1]
-     dim2 = angles.shape[-1]
-     P = radius.shape[-2]
-@@ -41,6 +42,7 @@ def sphere2cartesian(radius, angles):
-         r: radius (B, N, P, 1 or 2) last dim is one in 2D mode, two in 3D.
-         angles: angle (B, 1, P, 1 or 2) 
-     """
-+    print('radius angles in s2c mean', radius[..., 0].mean(), angles.mean())
-     dim = radius.shape[-1]
-     dim2 = angles.shape[-1]
-     P = radius.shape[-2]
-@@ -74,16 +76,18 @@ def cartesian2sphere(coord):
-     x = coord[..., 0]
-     y = coord[..., 1]
-     z = torch.zeros([1], device=coord.device) if dim == 2 else coord[..., 2]
--    x_non_zero = torch.where(x == 0, x.sign() * EPS, x)
-+    x_non_zero = torch.where(x == 0, x + EPS, x)
-     theta = torch.atan2(y, x_non_zero)
- 
-     assert not torch.isnan(theta).any(), (theta)
--    r = (coord**2).sum(-1).sqrt()
-+    r = (coord**2).sum(-1).clamp(min=EPS).sqrt()
-+    assert not torch.isnan(r).any(), (r)
- 
--    xysq = (x_non_zero**2 + y**2).sqrt()
-+    xysq_non_zero = (x_non_zero**2 + y**2).clamp(min=EPS).sqrt()
-     #xysq_non_zero = torch.where(xysq == 0, EPS + xysq, xysq)
--    xysq_non_zero = xysq.clamp(min=EPS)
--    phi = torch.atan(z / xysq_non_zero)
-+    #xysq_non_zero = xysq.clamp(min=EPS)
-+    #phi = torch.atan(z / xysq_non_zero)
-+    phi = torch.atan((xysq_non_zero / z.clamp(min=EPS)).clamp(min=EPS))
- 
-     assert not torch.isnan(phi).any(), (phi)
- 
-diff --git a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
-index 5530413..d41d3c3 100644
---- a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
-+++ b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
-@@ -112,7 +112,6 @@ class BaseShapeSampler(nn.Module):
-                 scaled_cartesian_coord, rotation)
-         else:
-             rotated_cartesian_coord = scaled_cartesian_coord
--        assert not torch.isnan(rotated_cartesian_coord).any()
-         posed_cartesian_coord = rotated_cartesian_coord + transition.view(
-             B, self.n_primitives, 1, self.dim)
-         assert not torch.isnan(posed_cartesian_coord).any()
-@@ -222,6 +221,42 @@ class BaseShapeSampler(nn.Module):
-         # (B, N, P)
-         return r1, r2, theta, phi
- 
-+    def cartesian2sphere(self, coord, params, *args, **kwargs):
-+        """Convert polar coordinate to cartesian coordinate.
-+        Args:
-+            coord: (B, N, P, D)
-+        """
-+        dim = coord.shape[-1]
-+        B, _, P, dim = coord.shape
-+        x = coord[..., 0]
-+        y = coord[..., 1]
-+        z = torch.zeros([1], device=coord.device) if dim == 2 else coord[...,
-+                                                                         2]
-+        x_non_zero = torch.where(x == 0, x + EPS, x)
-+        theta = torch.atan2(y, x_non_zero)
-+
-+        assert not torch.isnan(theta).any(), (theta)
-+        #r = (coord**2).sum(-1).clamp(min=EPS).sqrt()
-+        r = self.get_r_check_shape(theta.view(B, self.n_primitives, P, 1),
-+                                   params, *args, **kwargs)[..., 0]
-+        print('r in c2p', r.mean())
-+        assert not torch.isnan(r).any(), (r)
-+
-+        xysq_non_zero = (x_non_zero**2 + y**2).clamp(min=EPS).sqrt()
-+        #xysq_non_zero = torch.where(xysq == 0, EPS + xysq, xysq)
-+        #xysq_non_zero = xysq.clamp(min=EPS)
-+        #phi = torch.atan(z / xysq_non_zero)
-+        phi = torch.atan2(z, xysq_non_zero)
-+        #phi = torch.atan(xysq_non_zero / z)
-+        #phi = torch.acos((z / r_phi))
-+
-+        assert not torch.isnan(phi).any(), (phi)
-+
-+        # (B, N, P)
-+        return r.unsqueeze(-1).expand([*coord.shape[:-1],
-+                                       dim - 1]), torch.stack([theta, phi],
-+                                                              axis=-1)
-+
-     def extract_super_shapes_surface_point(self, super_shape_point,
-                                            primitive_params, *args, **kwargs):
-         """Extract surface point for visualziation purpose"""
-diff --git a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
-index 9b599de..6a23afd 100644
---- a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
-+++ b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
-@@ -19,15 +19,16 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-                  act='leaky',
-                  decoder_class='PrimitiveWiseGroupConvDecoder',
-                  is_shape_sampler_sphere=False,
-+                 spherical_angles=False,
-                  no_encoder=False,
-                  is_feature_angles=True,
-                  is_feature_coord=True,
-                  is_feature_radius=True,
-                  no_last_bias=False,
-+                 return_sdf=False,
-                  **kwargs):
-         super().__init__(*args, **kwargs)
-         self.clamp = True
--        self.spherical_angles = False
-         self.factor = factor
-         self.num_points = num_points
-         self.num_labels = 1  # Only infer r2 for 3D
-@@ -37,6 +38,8 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-         self.act = act
-         self.no_encoder = no_encoder
-         self.is_shape_sampler_sphere = is_shape_sampler_sphere
-+        self.spherical_angles = spherical_angles
-+        self.return_sdf = return_sdf
- 
-         c64 = 64 // self.factor
-         self.encoder_dim = c64 * 2
-@@ -80,7 +83,7 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-             coord = super_shape_functions.sphere2cartesian(r, thetas_reshaped)
-         else:
-             coord = super_shape_functions.polar2cartesian(r, thetas_reshaped)
--
-+        assert not torch.isnan(coord).any()
-         # posed_coord = B, n_primitives, P, dim
-         if self.learn_pose:
-             posed_coord = self.project_primitive_to_world(
-@@ -88,11 +91,20 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-         else:
-             posed_coord = coord
- 
-+        print('')
-+        print('get pr from points')
-         periodic_net_r = self.get_periodic_net_r(thetas.unsqueeze(1), points,
-                                                  r[..., -1], posed_coord)
- 
-         final_r = r.clone()
--        final_r[..., -1] = r[..., -1] + periodic_net_r.squeeze(-1)
-+        if self.is_shape_sampler_sphere and self.spherical_angles:
-+            print('mean r1 in points', r[..., 0].mean())
-+            final_r[..., 0] = r[..., 0] + periodic_net_r.squeeze(-1)
-+            print('mean final r in points', final_r[..., 0].mean())
-+        else:
-+            print('mean r1 in points', r[..., -1].mean())
-+            final_r[..., -1] = r[..., -1] + periodic_net_r.squeeze(-1)
-+            print('mean final r in points', final_r[..., -1].mean())
- 
-         if self.clamp:
-             final_r = final_r.clamp(min=EPS)
-@@ -125,6 +137,7 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
- 
-     def get_periodic_net_r(self, thetas, points, radius, coord):
-         # B, 1 or N, P, dim - 1
-+        print('mean coord in pr', coord.mean())
-         assert len(thetas.shape) == 4, thetas.shape
-         assert thetas.shape[-1] == self.dim - 1
-         assert points.shape[0] == thetas.shape[0]
-@@ -148,6 +161,7 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-         radius = self.decoder(encoded, thetas, radius, coord)
-         radius = radius * self.last_scale
- 
-+        print('mean from pr ', radius.mean())
-         return radius
- 
-     def get_indicator(self,
-@@ -185,16 +199,26 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-         if self.learn_pose:
-             posed_coord = self.project_primitive_to_world(posed_coord, params)
- 
-+        print('get pr from sgn')
-         rp = self.get_periodic_net_r(angles, points, radius, posed_coord)
- 
-+        print('mean r1 in sgn', r1.mean())
-         numerator = (coord**2).sum(-1)
-         if self.is_shape_sampler_sphere:
-             r1 = r1 + rp.squeeze(-1)
-+            print('mean final r in sgn', r1.mean())
-             if self.clamp:
-                 r1 = r1.clamp(min=EPS)
-+                nep = numerator.clamp(min=EPS)
-             else:
-                 r1 = nn.functional.relu(r1) + EPS
--            indicator = 1 - numerator.clamp(min=EPS).sqrt() / r1
-+                nep = (numerator + EPS)
-+            if self.return_sdf:
-+                dist = nep.sqrt() - r1
-+                indicator = dist.sign() * dist**2
-+            else:
-+                indicator = 1 - numerator.clamp(min=EPS).sqrt() / r1
-+
-         else:
-             if is3d:
-                 r2 = r2 + rp.squeeze(-1)
-@@ -211,18 +235,28 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
-             if self.clamp:
-                 denominator = ((r1**2) * (r2**2) * (phi.cos()**2) + (r2**2) *
-                                (phi.sin()**2)).clamp(min=EPS)
--                indicator = 1. - (numerator /
--                                  denominator).clamp(min=EPS).sqrt()
-             else:
-                 denominator = ((r1**2) * (r2**2) * (phi.cos()**2) + (r2**2) *
-                                (phi.sin()**2)) + EPS
--                indicator = 1. - (numerator / denominator + EPS).sqrt()
-+            if self.return_sdf:
-+                if self.clamp:
-+                    nep = numerator.clamp(min=EPS)
-+                else:
-+                    nep = (numerator + EPS)
-+                dist = nep.sqrt() - denominator.sqrt()
-+                indicator = (dist).sign() * dist**2
-+            else:
-+                if self.clamp:
-+                    indicator = 1. - (numerator /
-+                                      denominator).clamp(min=EPS).sqrt()
-+                else:
-+                    indicator = 1. - (numerator / denominator + EPS).sqrt()
- 
-         return indicator
- 
-     def get_sgn(self, coord, params, *args, **kwargs):
-         if self.is_shape_sampler_sphere and self.spherical_angles:
--            r, angles = super_shape_functions.cartesian2sphere(coord)
-+            r, angles = self.cartesian2sphere(coord, params, *args, **kwargs)
-             r1 = r[..., 0]
-             r2 = r[..., 1]
-             theta = angles[..., 0]
-diff --git a/external/periodic_shapes/test_polar2cart.py b/external/periodic_shapes/test_polar2cart.py
-index 0fb3f5e..8efd25d 100644
---- a/external/periodic_shapes/test_polar2cart.py
-+++ b/external/periodic_shapes/test_polar2cart.py
-@@ -10,8 +10,8 @@ print(radius2.shape, angles2.shape)
- coord2 = super_shape_functions.sphere2cartesian(radius2, angles2)
- print(coord)
- print(coord2)
--assert torch.all(torch.eq(radius.mean(), radius2.mean()))
- assert torch.allclose(coord, coord2), (coord - coord2)
-+assert torch.all(torch.eq(radius.mean(), radius2.mean()))
- assert torch.allclose(
-     angles[..., 0].sin(),
-     angles2[...,
-diff --git a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
-index 1896ca9..4a46eae 100644
---- a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
-+++ b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
-@@ -780,10 +780,11 @@ def test_decoder_consistency_OtherDecoders():
-         decoder_class='PrimitiveWiseGroupConvDecoder',
-         #decoder_class='BatchNormDecoderSharedWeight',
-         #decoder_class='PrimitiveWiseGroupConvDecoderLegacy',
--        last_scale=.1,
-+        last_scale=10,
-         no_encoder=True,
-         is_shape_sampler_sphere=True,
--        is_feature_coord=False,
-+        spherical_angles=True,
-+        is_feature_coord=True,
-         is_feature_angles=False,
-         is_feature_radius=False,
-         dim=dim)
-@@ -832,4 +833,91 @@ def test_decoder_consistency_OtherDecoders():
-                               atol=1e-5), (sgn.max(), sgn.min(), sgn.median(),
-                                            sgn.mean())
- 
--    assert False
-+
-+def test_decoder_sdf_consistency_OtherDecoders():
-+    batch = 3
-+    m = 3
-+    n = 1
-+    n1 = 1
-+    n2 = 10
-+    n3 = 3
-+    a = 1
-+    b = 1
-+    theta = math.pi / 2.
-+    sample_num = 200
-+    points_num = 5
-+    P = 10
-+
-+    dim = 3
-+
-+    rotations = [[0., 0., 0.]] * n
-+    #rotations = [[0., math.pi / 2, 0.]] * n
-+    #rotations = [[0., math.pi / 2., math.pi / 2.],
-+    #             [0., math.pi / 2., math.pi / 4.], [0., math.pi, math.pi / 2.]]
-+    transitions = [[0., 0., 0.]] * n
-+    #transitions = [[0., 1., 0.], [0., 1., 10.], [0., 1., 1.]]
-+    linear_scales = [[1., 1., 1.]] * n
-+    #linear_scales = [[1., 1., 1.], [0.2, 0.8, 1.2], [1., 1.2, 2.2]]
-+
-+    sampler = periodic_shape_sampler.PeriodicShapeSampler(
-+        points_num,
-+        m,
-+        n,
-+        #decoder_class='MLPDecoder',
-+        #decoder_class='BatchNormDecoder',
-+        decoder_class='PrimitiveWiseGroupConvDecoder',
-+        #decoder_class='BatchNormDecoderSharedWeight',
-+        #decoder_class='PrimitiveWiseGroupConvDecoderLegacy',
-+        last_scale=10,
-+        no_encoder=True,
-+        is_shape_sampler_sphere=True,
-+        spherical_angles=False,
-+        is_feature_coord=True,
-+        is_feature_angles=False,
-+        is_feature_radius=False,
-+        return_sdf=False,
-+        dim=dim)
-+    preset_params = utils.generate_multiple_primitive_params(
-+        m,
-+        n1,
-+        n2,
-+        n3,
-+        a,
-+        b,
-+        rotations_angle=rotations,
-+        transitions=transitions,
-+        linear_scales=linear_scales,
-+        nn=n,
-+        batch=batch)
-+
-+    batched_theta_test_tensor = utils.sample_spherical_angles(
-+        sample_num=P, batch=batch, sgn_convertible=True, dim=dim)
-+
-+    batched_points = torch.rand([batch, points_num]).float()
-+
-+    # B, N, P
-+    radius = sampler.transform_circumference_angle_to_super_shape_radius(
-+        batched_theta_test_tensor, preset_params, points=batched_points)
-+    # B, P, dim
-+    coords = sampler.transform_circumference_angle_to_super_shape_world_cartesian_coord(
-+        batched_theta_test_tensor,
-+        radius,
-+        preset_params,
-+        points=batched_points)
-+
-+    print('coord mean main', coords.mean(), (coords**2).sum(-1).sqrt().mean())
-+
-+    print('check coord')
-+    sgn = sampler.transform_world_cartesian_coord_to_tsd(coords.view(
-+        batch, -1, dim),
-+                                                         preset_params,
-+                                                         points=batched_points)
-+    print('check coord done')
-+    for idx in range(n):
-+        coord = coords[:, idx, :, :]
-+        sgn = sampler.transform_world_cartesian_coord_to_tsd(
-+            coord, preset_params, points=batched_points)[:, idx, :]
-+        print(sgn.shape)
-+        assert torch.allclose(sgn, torch.zeros_like(sgn),
-+                              atol=1e-5), (sgn.max(), sgn.min(), sgn.median(),
-+                                           sgn.mean())
-diff --git a/generate.py b/generate.py
-index c21a7b0..bbf4e8d 100644
---- a/generate.py
-+++ b/generate.py
-@@ -5,31 +5,109 @@ import shutil
- import argparse
- from tqdm import tqdm
- import time
--from collections import defaultdict
-+from collections import defaultdict, OrderedDict
- import pandas as pd
- from im2mesh import config
- from im2mesh.checkpoints import CheckpointIO
- from im2mesh.utils.io import export_pointcloud
- from im2mesh.utils.visualize import visualize_data
- from im2mesh.utils.voxels import VoxelGrid
-+import numpy as np
-+import hashlib
-+import subprocess
-+import yaml
-+from datetime import datetime
-+import subprocess
- 
- 
-+def represent_odict(dumper, instance):
-+    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
-+
-+
-+def construct_odict(loader, node):
-+    return OrderedDict(loader.construct_pairs(node))
-+
-+
-+yaml.add_representer(OrderedDict, represent_odict)
-+yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
-+
- parser = argparse.ArgumentParser(
--    description='Extract meshes from occupancy process.'
--)
-+    description='Extract meshes from occupancy process.')
- parser.add_argument('config', type=str, help='Path to config file.')
- parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
-+parser.add_argument(
-+    '--explicit',
-+    action='store_true',
-+    help=
-+    'to generate mesh with explicit rep, run: python3 generate.py --explicit --data.is_normal_icosahedron true --data.icosahedron_subdiv 4'
-+)
-+parser.add_argument('--unique_name',
-+                    default='',
-+                    type=str,
-+                    help='String name for generation.')
-+
-+args, unknown_args = parser.parse_known_args()
- 
--args = parser.parse_args()
- cfg = config.load_config(args.config, 'configs/default.yaml')
-+
-+for idx, arg in enumerate(unknown_args):
-+    if arg.startswith('--'):
-+        arg = arg.replace('--', '')
-+        value = unknown_args[idx + 1]
-+        keys = arg.split('.')
-+        if keys[0] not in cfg:
-+            cfg[keys[0]] = {}
-+        child_cfg = cfg.get(keys[0], {})
-+        for key in keys[1:]:
-+            item = child_cfg.get(key, None)
-+            if isinstance(item, dict):
-+                child_cfg = item
-+            elif item is None:
-+                if value == 'true':
-+                    value = True
-+                if value == 'false':
-+                    value = False
-+                if value == 'null':
-+                    value = None
-+                if isinstance(value, str) and value.isdigit():
-+                    value = float(value)
-+                child_cfg[key] = value
-+            else:
-+                child_cfg[key] = type(item)(value)
-+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
-+if args.explicit:
-+    assert cfg['data'].get('is_normal_icosahedron', False) or cfg['data'].get(
-+        'is_normal_uv_sphere', False)
-+    cfg['generation']['is_explicit_mesh'] = True
-+    cfg['test']['is_eval_explicit_mesh'] = True
-+if args.explicit:
-+    cfg['generation']['generation_dir'] += '_explicit'
-+cfg['generation']['generation_dir'] += ('_' + date_str)
-+
- is_cuda = (torch.cuda.is_available() and not args.no_cuda)
- device = torch.device("cuda" if is_cuda else "cpu")
- 
--out_dir = cfg['training']['out_dir']
-+out_dir = os.path.dirname(args.config)
-+
- generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
-+if not os.path.exists(generation_dir):
-+    os.makedirs(generation_dir)
- out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')
- out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')
- 
-+patch_path = os.path.join(generation_dir, 'gen_diff.patch')
-+subprocess.run('git diff > {}'.format(patch_path), shell=True)
-+weight_path = os.path.join(out_dir, cfg['test']['model_file'])
-+with open(weight_path, 'rb') as f:
-+    md5 = hashlib.md5(f.read()).hexdigest()
-+cfg['test']['model_file_hash'] = md5
-+yaml.dump(
-+    cfg,
-+    open(
-+        os.path.join(
-+            out_dir, 'gen_config_{}_{}.yaml'.format(args.unique_name,
-+                                                    date_str)), 'w'))
-+
- batch_size = cfg['generation']['batch_size']
- input_type = cfg['data']['input_type']
- vis_n_outputs = cfg['generation']['vis_n_outputs']
-@@ -60,10 +138,11 @@ if generate_pointcloud and not hasattr(generator, 'generate_pointcloud'):
-     generate_pointcloud = False
-     print('Warning: generator does not support pointcloud generation.')
- 
--
- # Loader
--test_loader = torch.utils.data.DataLoader(
--    dataset, batch_size=1, num_workers=0, shuffle=False)
-+test_loader = torch.utils.data.DataLoader(dataset,
-+                                          batch_size=1,
-+                                          num_workers=0,
-+                                          shuffle=False)
- 
- # Statistics
- time_dicts = []
-@@ -79,7 +158,10 @@ for it, data in enumerate(tqdm(test_loader)):
-     mesh_dir = os.path.join(generation_dir, 'meshes')
-     pointcloud_dir = os.path.join(generation_dir, 'pointcloud')
-     in_dir = os.path.join(generation_dir, 'input')
--    generation_vis_dir = os.path.join(generation_dir, 'vis', )
-+    generation_vis_dir = os.path.join(
-+        generation_dir,
-+        'vis',
-+    )
- 
-     # Get index etc.
-     idx = data['idx'].item()
-@@ -88,7 +170,7 @@ for it, data in enumerate(tqdm(test_loader)):
-         model_dict = dataset.get_model_dict(idx)
-     except AttributeError:
-         model_dict = {'model': str(idx), 'category': 'n/a'}
--    
-+
-     modelname = model_dict['model']
-     category_id = model_dict.get('category', 'n/a')
- 
-@@ -120,7 +202,7 @@ for it, data in enumerate(tqdm(test_loader)):
- 
-     if not os.path.exists(in_dir):
-         os.makedirs(in_dir)
--    
-+
-     # Timing dict
-     time_dict = {
-         'idx': idx,
-@@ -135,9 +217,8 @@ for it, data in enumerate(tqdm(test_loader)):
- 
-     # Also copy ground truth
-     if cfg['generation']['copy_groundtruth']:
--        modelpath = os.path.join(
--            dataset.dataset_folder, category_id, modelname, 
--            cfg['data']['watertight_file'])
-+        modelpath = os.path.join(dataset.dataset_folder, category_id,
-+                                 modelname, cfg['data']['watertight_file'])
-         out_file_dict['gt'] = modelpath
- 
-     if generate_mesh:
-@@ -156,13 +237,19 @@ for it, data in enumerate(tqdm(test_loader)):
-         mesh_out_file = os.path.join(mesh_dir, '%s.off' % modelname)
-         mesh.export(mesh_out_file)
-         out_file_dict['mesh'] = mesh_out_file
-+        if cfg['generation'].get('is_explicit_mesh', False):
-+            visibility = mesh.vertex_attributes['vertex_visibility']
-+            visibility_out_file = os.path.join(
-+                mesh_dir, '%s_vertex_visbility.npz' % modelname)
-+            np.savez(visibility_out_file, vertex_visibility=visibility)
-+            out_file_dict['vertex_visibility'] = visibility_out_file
- 
-     if generate_pointcloud:
-         t0 = time.time()
-         pointcloud = generator.generate_pointcloud(data)
-         time_dict['pcl'] = time.time() - t0
--        pointcloud_out_file = os.path.join(
--            pointcloud_dir, '%s.ply' % modelname)
-+        pointcloud_out_file = os.path.join(pointcloud_dir,
-+                                           '%s.ply' % modelname)
-         export_pointcloud(pointcloud, pointcloud_out_file)
-         out_file_dict['pointcloud'] = pointcloud_out_file
- 
-@@ -192,8 +279,8 @@ for it, data in enumerate(tqdm(test_loader)):
-         img_name = '%02d.off' % c_it
-         for k, filepath in out_file_dict.items():
-             ext = os.path.splitext(filepath)[1]
--            out_file = os.path.join(generation_vis_dir, '%02d_%s%s'
--                                    % (c_it, k, ext))
-+            out_file = os.path.join(generation_vis_dir,
-+                                    '%02d_%s%s' % (c_it, k, ext))
-             shutil.copyfile(filepath, out_file)
- 
-     model_counter[category_id] += 1
-diff --git a/im2mesh/checkpoints.py b/im2mesh/checkpoints.py
-index 332600c..75ff6ab 100644
---- a/im2mesh/checkpoints.py
-+++ b/im2mesh/checkpoints.py
-@@ -112,7 +112,7 @@ class CheckpointIO(object):
-                         new_pretrained_dict.keys())
-                     print('ignored parameters')
-                     for key in diff:
--                        print(key)
-+                        print(key, pretrained_dict[key].shape)
-                     pretrained_dict_new_param = {
-                         key: val
-                         for key, val in model_dict.items()
-@@ -120,7 +120,7 @@ class CheckpointIO(object):
-                     }
-                     print('new parameters')
-                     for key in pretrained_dict_new_param:
--                        print(key)
-+                        print(key, pretrained_dict_new_param[key].shape)
-                     new_pretrained_dict.update(pretrained_dict_new_param)
-                     v.load_state_dict(new_pretrained_dict)
-                 else:
-diff --git a/im2mesh/config.py b/im2mesh/config.py
-index 007aca6..00e3fc4 100644
---- a/im2mesh/config.py
-+++ b/im2mesh/config.py
-@@ -1,7 +1,7 @@
- import yaml
- from torchvision import transforms
- from im2mesh import data
--from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet
-+from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet, atlasnetv2
- from im2mesh import preprocess
- from torch.utils import data as torch_data
- 
-@@ -12,6 +12,7 @@ method_dict = {
-     'pix2mesh': pix2mesh,
-     'dmc': dmc,
-     'pnet': pnet,
-+    'atlasnetv2': atlasnetv2
- }
- 
- 
-@@ -229,6 +230,8 @@ def get_inputs_field(mode, cfg):
-         inputs_field = data.VoxelsField(cfg['data']['voxels_file'])
-     elif input_type == 'idx':
-         inputs_field = data.IndexField()
-+    elif input_type == 'raw_id':
-+        inputs_field = data.RawIDField()
-     else:
-         raise ValueError('Invalid input type (%s)' % input_type)
-     return inputs_field
-diff --git a/im2mesh/data/__init__.py b/im2mesh/data/__init__.py
-index 4f0aab1..870a44f 100644
---- a/im2mesh/data/__init__.py
-+++ b/im2mesh/data/__init__.py
-@@ -4,11 +4,11 @@ from im2mesh.data.core import (
- )
- from im2mesh.data.fields import (
-     IndexField, CategoryField, ImagesField, PointsField,
--    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField
-+    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField, RawIDField, SDFPointsField, PlanarPatchField
- )
- from im2mesh.data.transforms import (
-     PointcloudNoise, SubsamplePointcloud,
--    SubsamplePoints
-+    SubsamplePoints, SubsampleSDFPoints
- )
- from im2mesh.data.real import (
-     KittiDataset, OnlineProductDataset,
-@@ -24,12 +24,15 @@ __all__ = [
-     # Fields
-     IndexField,
-     CategoryField,
-+    RawIDField,
-+    SDFPointsField,
-     ImagesField,
-     PointsField,
-     VoxelsField,
-     PointCloudField,
-     MeshField,
-     SphericalCoordinateField,
-+    PlanarPatchField,
-     # Transforms
-     PointcloudNoise,
-     SubsamplePointcloud,
-diff --git a/im2mesh/data/core.py b/im2mesh/data/core.py
-index 96d5c86..a224c72 100644
---- a/im2mesh/data/core.py
-+++ b/im2mesh/data/core.py
-@@ -3,8 +3,7 @@ import logging
- from torch.utils import data
- import numpy as np
- import yaml
--
--
-+import traceback
- logger = logging.getLogger(__name__)
- 
- 
-@@ -12,7 +11,6 @@ logger = logging.getLogger(__name__)
- class Field(object):
-     ''' Data fields class.
-     '''
--
-     def load(self, data_path, idx, category):
-         ''' Loads a data point.
- 
-@@ -35,9 +33,13 @@ class Field(object):
- class Shapes3dDataset(data.Dataset):
-     ''' 3D Shapes dataset class.
-     '''
--
--    def __init__(self, dataset_folder, fields, split=None,
--                 categories=None, no_except=True, transform=None):
-+    def __init__(self,
-+                 dataset_folder,
-+                 fields,
-+                 split=None,
-+                 categories=None,
-+                 no_except=True,
-+                 transform=None):
-         ''' Initialization of the the 3D shape dataset.
- 
-         Args:
-@@ -57,8 +59,10 @@ class Shapes3dDataset(data.Dataset):
-         # If categories is None, use all subfolders
-         if categories is None:
-             categories = os.listdir(dataset_folder)
--            categories = [c for c in categories
--                          if os.path.isdir(os.path.join(dataset_folder, c))]
-+            categories = [
-+                c for c in categories
-+                if os.path.isdir(os.path.join(dataset_folder, c))
-+            ]
- 
-         # Read metadata file
-         metadata_file = os.path.join(dataset_folder, 'metadata.yaml')
-@@ -67,10 +71,8 @@ class Shapes3dDataset(data.Dataset):
-             with open(metadata_file, 'r') as f:
-                 self.metadata = yaml.load(f)
-         else:
--            self.metadata = {
--                c: {'id': c, 'name': 'n/a'} for c in categories
--            } 
--        
-+            self.metadata = {c: {'id': c, 'name': 'n/a'} for c in categories}
-+
-         # Set index
-         for c_idx, c in enumerate(categories):
-             self.metadata[c]['idx'] = c_idx
-@@ -85,11 +87,8 @@ class Shapes3dDataset(data.Dataset):
-             split_file = os.path.join(subpath, split + '.lst')
-             with open(split_file, 'r') as f:
-                 models_c = f.read().split('\n')
--            
--            self.models += [
--                {'category': c, 'model': m}
--                for m in models_c
--            ]
-+
-+            self.models += [{'category': c, 'model': m} for m in models_c]
- 
-     def __len__(self):
-         ''' Returns the length of the dataset.
-@@ -113,11 +112,11 @@ class Shapes3dDataset(data.Dataset):
-             try:
-                 field_data = field.load(model_path, idx, c_idx)
-             except Exception:
-+                traceback.print_exc()
-                 if self.no_except:
-                     logger.warn(
--                        'Error occured when loading field %s of model %s'
--                        % (field_name, model)
--                    )
-+                        'Error occured when loading field %s of model %s' %
-+                        (field_name, model))
-                     return None
-                 else:
-                     raise
-@@ -149,8 +148,8 @@ class Shapes3dDataset(data.Dataset):
-         files = os.listdir(model_path)
-         for field_name, field in self.fields.items():
-             if not field.check_complete(files):
--                logger.warn('Field "%s" is incomplete: %s'
--                            % (field_name, model_path))
-+                logger.warn('Field "%s" is incomplete: %s' %
-+                            (field_name, model_path))
-                 return False
- 
-         return True
-diff --git a/im2mesh/data/fields.py b/im2mesh/data/fields.py
-index 71e5e7b..64f1ae8 100644
---- a/im2mesh/data/fields.py
-+++ b/im2mesh/data/fields.py
-@@ -7,6 +7,7 @@ import trimesh
- from im2mesh.data.core import Field
- from im2mesh.utils import binvox_rw
- from periodic_shapes import utils
-+from im2mesh.atlasnetv2 import utils as atv2_utils
- import torch
- 
- 
-@@ -173,7 +174,7 @@ class PointsField(Field):
-             'occ': occupancies,
-         }
- 
--        if self.with_transforms:
-+        if self.with_transforms and 'loc' in data and 'scale' in data:
-             data['loc'] = points_dict['loc'].astype(np.float32)
-             data['scale'] = points_dict['scale'].astype(np.float32)
- 
-@@ -262,7 +263,7 @@ class PointCloudField(Field):
-             'normals': normals,
-         }
- 
--        if self.with_transforms:
-+        if self.with_transforms and 'loc' in data and 'scale' in data:
-             data['loc'] = pointcloud_dict['loc'].astype(np.float32)
-             data['scale'] = pointcloud_dict['scale'].astype(np.float32)
- 
-@@ -342,8 +343,10 @@ class SphericalCoordinateField(Field):
-                  primitive_points_sample_n,
-                  mode,
-                  is_normal_icosahedron=False,
-+                 is_normal_uv_sphere=False,
-                  icosahedron_subdiv=2,
-                  icosahedron_uv_margin=1e-5,
-+                 icosahedron_uv_margin_phi=1e-5,
-                  uv_sphere_length=20,
-                  normal_mesh_no_invert=False,
-                  *args,
-@@ -351,6 +354,9 @@ class SphericalCoordinateField(Field):
-         self.primitive_points_sample_n = primitive_points_sample_n
-         self.mode = mode
-         self.is_normal_icosahedron = is_normal_icosahedron
-+        self.is_normal_uv_sphere = is_normal_uv_sphere
-+        self.icosahedron_uv_margin = icosahedron_uv_margin
-+        self.icosahedron_uv_margin_phi = icosahedron_uv_margin_phi
- 
-         if self.is_normal_icosahedron:
-             icosamesh = trimesh.creation.icosphere(
-@@ -371,6 +377,27 @@ class SphericalCoordinateField(Field):
-             self.angles_for_nomal = torch.stack([uv_th, uv_ph], axis=-1)
-             self.face_for_normal = torch.from_numpy(icosamesh.faces)
- 
-+        elif self.is_normal_uv_sphere:
-+            thetas = utils.sample_spherical_angles(
-+                batch=1,
-+                sample_num=uv_sphere_length,
-+                sampling='grid',
-+                device='cpu',
-+                dim=3,
-+                sgn_convertible=True,
-+                phi_margin=icosahedron_uv_margin_phi,
-+                theta_margin=icosahedron_uv_margin)
-+            mesh = trimesh.creation.uv_sphere(
-+                theta=np.linspace(0, np.pi, uv_sphere_length),
-+                phi=np.linspace(-np.pi, np.pi, uv_sphere_length))
-+            #thetas = torch.where(thetas.abs() < icosahedron_uv_margin_phi,
-+            #                     torch.tensor([icosahedron_uv_margin_phi]),
-+            #                     thetas)
-+            if not normal_mesh_no_invert:
-+                mesh.invert()
-+            self.angles_for_nomal = thetas[0]
-+            self.face_for_normal = torch.from_numpy(mesh.faces)
-+
-     def load(self, model_path, idx, category):
-         ''' Sample spherical coordinate.
- 
-@@ -386,11 +413,11 @@ class SphericalCoordinateField(Field):
-             device='cpu',
-             dim=3,  #sgn_convertible=True, phi_margin=1e-5, theta_margin=1e-5)
-             sgn_convertible=True,
--            phi_margin=1e-5,
--            theta_margin=1e-5).squeeze(0)
-+            phi_margin=self.icosahedron_uv_margin_phi,
-+            theta_margin=self.icosahedron_uv_margin).squeeze(0)
- 
-         data = {None: angles}
--        if self.is_normal_icosahedron:
-+        if self.is_normal_icosahedron or self.is_normal_uv_sphere:
-             data.update({
-                 'normal_angles': self.angles_for_nomal.clone(),
-                 'normal_face': self.face_for_normal.clone()
-@@ -403,3 +430,132 @@ class SphericalCoordinateField(Field):
-         Returns: True
-         '''
-         return True
-+
-+
-+class RawIDField(Field):
-+    ''' Basic index field.'''
-+    def load(self, model_path, idx, category):
-+        ''' Loads the index field.
-+
-+        Args:
-+            model_path (str): path to model
-+            idx (int): ID of data point
-+            category (int): index of category
-+        '''
-+        category_id, object_id = model_path.split('/')[-2:]
-+        data = {'category': category_id, 'object': object_id}
-+        return data
-+
-+    def check_complete(self, files):
-+        ''' Check if field is complete.
-+        
-+        Args:
-+            files: files
-+        '''
-+        return True
-+
-+
-+# 3D Fields
-+class SDFPointsField(Field):
-+    ''' Point Field.
-+
-+    It provides the field to load point data. This is used for the points
-+    randomly sampled in the bounding volume of the 3D shape.
-+
-+    Args:
-+        file_name (str): file name
-+        transform (list): list of transformations which will be applied to the
-+            points tensor
-+        with_transforms (bool): whether scaling and rotation data should be
-+            provided
-+
-+    '''
-+    def __init__(self, file_name, transform=None, with_transforms=False):
-+        self.file_name = file_name
-+        self.transform = transform
-+        self.with_transforms = with_transforms
-+
-+    def load(self, model_path, idx, category):
-+        ''' Loads the data point.
-+
-+        Args:
-+            model_path (str): path to model
-+            idx (int): ID of data point
-+            category (int): index of category
-+        '''
-+        file_path = os.path.join(model_path, self.file_name)
-+
-+        points_dict = np.load(file_path)
-+        points = points_dict['points']
-+        # Break symmetry if given in float16:
-+        if points.dtype == np.float16:
-+            points = points.astype(np.float32)
-+            points += 1e-4 * np.random.randn(*points.shape)
-+        else:
-+            points = points.astype(np.float32)
-+
-+        occupancies = points_dict['distances']
-+        occupancies = occupancies.astype(np.float32)
-+
-+        data = {
-+            None: points,
-+            'distances': occupancies,
-+        }
-+
-+        if self.with_transforms:
-+            raise ValueError('data for transform not stored')
-+
-+        if self.transform is not None:
-+            data = self.transform(data)
-+
-+        return data
-+
-+
-+class PlanarPatchField(Field):
-+    ''' Angle field class.
-+
-+    It provides the class used for spherical coordinate data.
-+
-+    Args:
-+        file_name (str): file name
-+        transform (list): list of transformations applied to data points
-+    '''
-+    def __init__(self,
-+                 mode,
-+                 patch_side_length=20,
-+                 is_generate_mesh=False,
-+                 *args,
-+                 **kwargs):
-+        self.mode = mode
-+        self.patch_side_length = patch_side_length
-+        self.is_generate_mesh = is_generate_mesh
-+
-+        if self.is_generate_mesh:
-+            vertices, faces = atv2_utils.create_planar_mesh(patch_side_length)
-+
-+            self.vertices = torch.from_numpy(vertices)
-+            self.faces = torch.from_numpy(faces)
-+
-+    def load(self, model_path, idx, category):
-+        ''' Sample spherical coordinate.
-+
-+        Args:
-+            model_path (str): path to model
-+            idx (int): ID of data point
-+            category (int): index of category
-+        '''
-+        plane_points = utils.generate_grid_samples(
-+            [0, 1],
-+            batch=1,
-+            sample_num=self.patch_side_length,
-+            sampling='uniform',
-+            device='cpu',
-+            dim=2)
-+
-+        data = {None: plane_points}
-+        if self.is_generate_mesh:
-+            data.update({
-+                'mesh_vertices': self.vertices.clone(),
-+                'mesh_faces': self.faces.clone()
-+            })
-+        return data
-diff --git a/im2mesh/data/transforms.py b/im2mesh/data/transforms.py
-index 26fec98..b471e37 100644
---- a/im2mesh/data/transforms.py
-+++ b/im2mesh/data/transforms.py
-@@ -10,7 +10,6 @@ class PointcloudNoise(object):
-     Args:
-         stddev (int): standard deviation
-     '''
--
-     def __init__(self, stddev):
-         self.stddev = stddev
- 
-@@ -81,7 +80,7 @@ class SubsamplePoints(object):
-             idx = np.random.randint(points.shape[0], size=self.N)
-             data_out.update({
-                 None: points[idx, :],
--                'occ':  occ[idx],
-+                'occ': occ[idx],
-             })
-         else:
-             Nt_out, Nt_in = self.N
-@@ -109,3 +108,58 @@ class SubsamplePoints(object):
-                 'volume': volume,
-             })
-         return data_out
-+
-+
-+class SubsampleSDFPoints(object):
-+    ''' Points subsampling transformation class.
-+
-+    It subsamples the points data.
-+
-+    Args:
-+        N (int): number of points to be subsampled
-+    '''
-+    def __init__(self, N):
-+        self.N = N
-+
-+    def __call__(self, data):
-+        ''' Calls the transformation.
-+
-+        Args:
-+            data (dictionary): data dictionary
-+        '''
-+        points = data[None]
-+        occ = data['distances']
-+
-+        data_out = data.copy()
-+        if isinstance(self.N, int):
-+            idx = np.random.randint(points.shape[0], size=self.N)
-+            data_out.update({
-+                None: points[idx, :],
-+                'distances': occ[idx],
-+            })
-+        else:
-+            Nt_out, Nt_in = self.N
-+            occ_binary = (occ >= 0.5)
-+            points0 = points[~occ_binary]
-+            points1 = points[occ_binary]
-+
-+            idx0 = np.random.randint(points0.shape[0], size=Nt_out)
-+            idx1 = np.random.randint(points1.shape[0], size=Nt_in)
-+
-+            points0 = points0[idx0, :]
-+            points1 = points1[idx1, :]
-+            points = np.concatenate([points0, points1], axis=0)
-+
-+            occ0 = np.zeros(Nt_out, dtype=np.float32)
-+            occ1 = np.ones(Nt_in, dtype=np.float32)
-+            occ = np.concatenate([occ0, occ1], axis=0)
-+
-+            volume = occ_binary.sum() / len(occ_binary)
-+            volume = volume.astype(np.float32)
-+
-+            data_out.update({
-+                None: points,
-+                'distances': occ,
-+                'volume': volume,
-+            })
-+        return data_out
-diff --git a/im2mesh/eval.py b/im2mesh/eval.py
-index 02f6885..91052f8 100644
---- a/im2mesh/eval.py
-+++ b/im2mesh/eval.py
-@@ -1,12 +1,16 @@
- # from im2mesh import icp
- import logging
-+import random
- import numpy as np
- import trimesh
- # from scipy.spatial import cKDTree
- from im2mesh.utils.libkdtree import KDTree
- from im2mesh.utils.libmesh import check_mesh_contains
- from im2mesh.common import compute_iou
--
-+from pykeops.torch import LazyTensor
-+import kaolin as kal
-+import torch
-+import warnings
- 
- # Maximum values for bounding box [-0.5, 0.5]^3
- EMPTY_PCL_DICT = {
-@@ -34,12 +38,17 @@ class MeshEvaluator(object):
-     Args:
-         n_points (int): number of points to be used for evaluation
-     '''
--
-     def __init__(self, n_points=100000):
-         self.n_points = n_points
- 
--    def eval_mesh(self, mesh, pointcloud_tgt, normals_tgt,
--                  points_iou, occ_tgt):
-+    def eval_mesh(self,
-+                  mesh,
-+                  pointcloud_tgt,
-+                  normals_tgt,
-+                  points_iou,
-+                  occ_tgt,
-+                  is_eval_explicit_mesh=False,
-+                  vertex_visibility=None):
-         ''' Evaluates a mesh.
- 
-         Args:
-@@ -49,16 +58,42 @@ class MeshEvaluator(object):
-             points_iou (numpy_array): points tensor for IoU evaluation
-             occ_tgt (numpy_array): GT occupancy values for IoU points
-         '''
--        if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
--            pointcloud, idx = mesh.sample(self.n_points, return_index=True)
-+        if is_eval_explicit_mesh:
-+            assert vertex_visibility is not None
-+            sampled_vertex_idx = np.zeros_like(vertex_visibility).astype(
-+                np.bool)
-+            pointcloud = mesh.vertices[vertex_visibility, :]
-+            normals = mesh.vertex_normals[vertex_visibility, :]
-             pointcloud = pointcloud.astype(np.float32)
--            normals = mesh.face_normals[idx]
-+            normals = normals.astype(np.float32)
-+
-+            if pointcloud.shape[0] > self.n_points:
-+                select_idx = random.sample(range(pointcloud.shape[0]),
-+                                           self.n_points)
-+                pointcloud = pointcloud[select_idx:]
-+            if normals.shape[0] > self.n_points:
-+                select_idx = random.sample(range(normals.shape[0]),
-+                                           self.n_points)
-+                normals = normals[select_idx:]
-+            if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
-+                select_idx = random.sample(range(pointcloud.shape[0]),
-+                                           pointcloud.shape[0])
-+                pointcloud_tgt = pointcloud_tgt[select_idx, :]
-+            if normals_tgt.shape[0] > normals.shape[0]:
-+                select_idx = random.sample(range(normals.shape[0]),
-+                                           pointcloud.shape[0])
-+                normals_tgt = normals_tgt[select_idx, :]
-         else:
--            pointcloud = np.empty((0, 3))
--            normals = np.empty((0, 3))
-+            if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-+                pointcloud, idx = mesh.sample(self.n_points, return_index=True)
-+                pointcloud = pointcloud.astype(np.float32)
-+                normals = mesh.face_normals[idx]
-+            else:
-+                pointcloud = np.empty((0, 3))
-+                normals = np.empty((0, 3))
- 
--        out_dict = self.eval_pointcloud(
--            pointcloud, pointcloud_tgt, normals, normals_tgt)
-+        out_dict = self.eval_pointcloud(pointcloud, pointcloud_tgt, normals,
-+                                        normals_tgt)
- 
-         if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-             occ = check_mesh_contains(mesh, points_iou)
-@@ -68,8 +103,11 @@ class MeshEvaluator(object):
- 
-         return out_dict
- 
--    def eval_pointcloud(self, pointcloud, pointcloud_tgt,
--                        normals=None, normals_tgt=None):
-+    def eval_pointcloud(self,
-+                        pointcloud,
-+                        pointcloud_tgt,
-+                        normals=None,
-+                        normals_tgt=None):
-         ''' Evaluates a point cloud.
- 
-         Args:
-@@ -92,8 +130,7 @@ class MeshEvaluator(object):
-         # Completeness: how far are the points of the target point cloud
-         # from thre predicted point cloud
-         completeness, completeness_normals = distance_p2p(
--            pointcloud_tgt, normals_tgt, pointcloud, normals
--        )
-+            pointcloud_tgt, normals_tgt, pointcloud, normals)
-         completeness2 = completeness**2
- 
-         completeness = completeness.mean()
-@@ -102,9 +139,8 @@ class MeshEvaluator(object):
- 
-         # Accuracy: how far are th points of the predicted pointcloud
-         # from the target pointcloud
--        accuracy, accuracy_normals = distance_p2p(
--            pointcloud, normals, pointcloud_tgt, normals_tgt
--        )
-+        accuracy, accuracy_normals = distance_p2p(pointcloud, normals,
-+                                                  pointcloud_tgt, normals_tgt)
-         accuracy2 = accuracy**2
- 
-         accuracy = accuracy.mean()
-@@ -113,9 +149,8 @@ class MeshEvaluator(object):
- 
-         # Chamfer distance
-         chamferL2 = 0.5 * (completeness2 + accuracy2)
--        normals_correctness = (
--            0.5 * completeness_normals + 0.5 * accuracy_normals
--        )
-+        normals_correctness = (0.5 * completeness_normals +
-+                               0.5 * accuracy_normals)
-         chamferL1 = 0.5 * (completeness + accuracy)
- 
-         out_dict = {
-@@ -132,6 +167,83 @@ class MeshEvaluator(object):
- 
-         return out_dict
- 
-+    def eval_fscore_from_mesh(self,
-+                              mesh,
-+                              pointcloud_tgt,
-+                              thresholds,
-+                              is_eval_explicit_mesh=False,
-+                              vertex_visibility=None):
-+        ''' Evaluates a mesh.
-+
-+        Args:
-+            mesh (trimesh): mesh which should be evaluated
-+            pointcloud_tgt (numpy array): target point cloud
-+            normals_tgt (numpy array): target normals
-+            points_iou (numpy_array): points tensor for IoU evaluation
-+            occ_tgt (numpy_array): GT occupancy values for IoU points
-+        '''
-+
-+        if is_eval_explicit_mesh:
-+            assert vertex_visibility is not None
-+            pointcloud = mesh.vertices[vertex_visibility, :]
-+            pointcloud = pointcloud.astype(np.float32)
-+
-+            if pointcloud.shape[0] > self.n_points:
-+                select_idx = random.sample(range(pointcloud.shape[0]),
-+                                           self.n_points)
-+                pointcloud = pointcloud[select_idx:]
-+
-+            if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
-+                select_idx = random.sample(range(pointcloud.shape[0]),
-+                                           pointcloud.shape[0])
-+                pointcloud_tgt = pointcloud_tgt[select_idx, :]
-+
-+        else:
-+            if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-+                pointcloud, idx = mesh.sample(self.n_points, return_index=True)
-+                pointcloud = pointcloud.astype(np.float32)
-+            else:
-+                pointcloud = np.empty((0, 3))
-+        """
-+        if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-+            pointcloud = mesh.sample(self.n_points, return_index=False)
-+            pointcloud = pointcloud.astype(np.float32)
-+        else:
-+            pointcloud = np.empty((0, 3))
-+        """
-+
-+        out_dict = fscore(pointcloud[np.newaxis, ...],
-+                          pointcloud_tgt[np.newaxis, ...],
-+                          thresholds=thresholds,
-+                          mode='pykeops')
-+        if out_dict is None:
-+            return out_dict
-+        else:
-+            out_dict = {
-+                k: v[0].item()
-+                for k, v in out_dict.items() if v is not None
-+            }
-+
-+        return out_dict
-+
-+    def eval_fscore_from_mesh_batch(self, pointcloud_pred, pointcloud_tgt,
-+                                    thresholds):
-+        ''' Evaluates a mesh.
-+
-+        Args:
-+            mesh (trimesh): mesh which should be evaluated
-+            pointcloud_tgt (numpy array): target point cloud
-+            normals_tgt (numpy array): target normals
-+            points_iou (numpy_array): points tensor for IoU evaluation
-+            occ_tgt (numpy_array): GT occupancy values for IoU points
-+        '''
-+        out_dict = fscore(pointcloud_pred,
-+                          pointcloud_tgt,
-+                          thresholds=thresholds,
-+                          mode='pykeops')
-+
-+        return out_dict
-+
- 
- def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
-     ''' Computes minimal distances of each point in points_src to points_tgt.
-@@ -156,8 +268,8 @@ def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
-         # (mostly due to mehtod not caring about this in generation)
-         normals_dot_product = np.abs(normals_dot_product)
-     else:
--        normals_dot_product = np.array(
--            [np.nan] * points_src.shape[0], dtype=np.float32)
-+        normals_dot_product = np.array([np.nan] * points_src.shape[0],
-+                                       dtype=np.float32)
-     return dist, normals_dot_product
- 
- 
-@@ -171,3 +283,77 @@ def distance_p2m(points, mesh):
-     '''
-     _, dist, _ = trimesh.proximity.closest_point(mesh, points)
-     return dist
-+
-+
-+def chamfer_distance(pred, target, pykeops=True):
-+    assert pykeops
-+    # B, P, 1, dim
-+    pred_lazy = LazyTensor(pred.unsqueeze(2))
-+    # B, 1, P2, dim
-+    target_lazy = LazyTensor(target.unsqueeze(1))
-+
-+    # B, P, P2, dim
-+    dist = (pred_lazy - target_lazy).norm2()
-+
-+    # B, P, dim
-+    pred2target = dist.min(2).squeeze(-1)
-+
-+    # B, P2, dim
-+    target2pred = dist.min(1).squeeze(-1)
-+
-+    return pred2target, target2pred
-+
-+
-+def fscore(pred_points, target_points, thresholds=[0.01], mode='pykeops'):
-+    assert mode in ['kaolin', 'pykeops']
-+    assert isinstance(thresholds, list)
-+
-+    if isinstance(pred_points, np.ndarray):
-+        pred_points = torch.from_numpy(pred_points).to('cuda')
-+    if isinstance(target_points, np.ndarray):
-+        target_points = torch.from_numpy(target_points).to('cuda')
-+    assert pred_points.ndim == 3 and target_points.ndim == 3
-+    assert len(pred_points.shape) == 3 and len(
-+        target_points.shape) == 3, (pred_points.shape, target_points.shape,
-+                                    len(pred_points.shape),
-+                                    len(target_points.shape))
-+
-+    try:
-+        assert pred_points.shape[1] == target_points.shape[
-+            1], pred_points.shape[1] == target_points.shape[1]
-+    except:
-+        warnings.warn('point shapes are not same!')
-+        return {}
-+
-+    f_scores = {}
-+    if mode == 'kaolin':
-+        assert False
-+        for threshold in thresholds:
-+            b_f = []
-+            for idx in range(target_points.shape[0]):
-+                t = target_points[idx]
-+                p = pred_points[idx]
-+                f_score = kal.metrics.f_score(t, p, radius=threshold)
-+                b_f.append(f_score)
-+            f_scores['fscore_th={}'.format(threshold)] = torch.stack(
-+                b_f, 0).detach().to('cpu')
-+
-+    elif mode == 'pykeops':
-+
-+        gt_distances, pred_distances = chamfer_distance(
-+            pred_points, target_points)
-+
-+        for threshold in thresholds:
-+            fn = (pred_distances > threshold).sum(-1).float()
-+            fp = (gt_distances > threshold).sum(-1).float()
-+            tp = (gt_distances <= threshold).sum(-1).float()
-+
-+            precision = tp / (tp + fp)
-+            recall = tp / (tp + fn)
-+
-+            f_score = 2 * (precision * recall) / (precision + recall + 1e-8)
-+            f_scores['fscore_th={}'.format(threshold)] = f_score.detach().to(
-+                'cpu')
-+    else:
-+        raise NotImplementedError
-+    return f_scores
-diff --git a/im2mesh/pnet/config.py b/im2mesh/pnet/config.py
-index 169bcc6..73fc786 100644
---- a/im2mesh/pnet/config.py
-+++ b/im2mesh/pnet/config.py
-@@ -26,6 +26,10 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
-     encoder_kwargs = cfg['model']['encoder_kwargs']
-     encoder_latent_kwargs = cfg['model']['encoder_latent_kwargs']
- 
-+    decoder_kwargs['return_sdf'] = cfg['trainer'].get('is_sdf', False)
-+    decoder_kwargs['is_radius_reg'] = cfg['trainer'].get(
-+        'is_radius_reg', False)
-+
-     decoder = models.decoder_dict[decoder](dim=dim,
-                                            z_dim=z_dim,
-                                            c_dim=c_dim,
-@@ -100,6 +104,9 @@ def get_generator(model, cfg, device, **kwargs):
-         refinement_step=cfg['generation']['refinement_step'],
-         simplify_nfaces=cfg['generation']['simplify_nfaces'],
-         preprocessor=preprocessor,
-+        pnet_point_scale=cfg['trainer']['pnet_point_scale'],
-+        is_explicit_mesh=cfg['generation'].get('is_explicit_mesh', False),
-+        **cfg['generation'].get('mesh_kwargs', {}),
-     )
-     return generator
- 
-@@ -126,6 +133,8 @@ def get_data_fields(mode, cfg):
-         cfg (dict): imported yaml config
-     '''
-     points_transform = data.SubsamplePoints(cfg['data']['points_subsample'])
-+    if cfg.get('sdf_generation', False):
-+        points_transform = None
-     with_transforms = cfg['model']['use_camera']
- 
-     fields = {}
-@@ -136,8 +145,18 @@ def get_data_fields(mode, cfg):
-         unpackbits=cfg['data']['points_unpackbits'],
-     )
- 
-+    if not cfg.get('sdf_generation', False):
-+        sdf_points_transform = data.SubsampleSDFPoints(
-+            cfg['data']['points_subsample'])
-+        fields['sdf_points'] = data.SDFPointsField(
-+            cfg['data']['sdf_points_file'],
-+            sdf_points_transform,
-+            with_transforms=with_transforms)
-+
-     pointcloud_transform = data.SubsamplePointcloud(
-         cfg['data']['pointcloud_target_n'])
-+    if cfg.get('sdf_generation', False):
-+        pointcloud_transform = None
- 
-     fields['pointcloud'] = data.PointCloudField(cfg['data']['pointcloud_file'],
-                                                 pointcloud_transform,
-@@ -146,8 +165,11 @@ def get_data_fields(mode, cfg):
-         cfg['data']['primitive_points_sample_n'],
-         mode,
-         is_normal_icosahedron=cfg['data'].get('is_normal_icosahedron', False),
-+        is_normal_uv_sphere=cfg['data'].get('is_normal_uv_sphere', False),
-         icosahedron_subdiv=cfg['data'].get('icosahedron_subdiv', 2),
-         icosahedron_uv_margin=cfg['data'].get('icosahedron_uv_margin', 1e-5),
-+        icosahedron_uv_margin_phi=cfg['data'].get('icosahedron_uv_margin_phi',
-+                                                  1e-5),
-         uv_sphere_length=cfg['data'].get('uv_sphere_length', 20),
-         normal_mesh_no_invert=cfg['data'].get('normal_mesh_no_invert', False))
-     if mode in ('val', 'test'):
-diff --git a/im2mesh/pnet/generation.py b/im2mesh/pnet/generation.py
-index 49786de..e2b5bd7 100644
---- a/im2mesh/pnet/generation.py
-+++ b/im2mesh/pnet/generation.py
-@@ -8,6 +8,7 @@ from im2mesh.utils import libmcubes
- from im2mesh.common import make_3d_grid
- from im2mesh.utils.libsimplify import simplify_mesh
- from im2mesh.utils.libmise import MISE
-+from periodic_shapes.models import model_utils
- import time
- 
- 
-@@ -30,13 +31,22 @@ class Generator3D(object):
-         simplify_nfaces (int): number of faces the mesh should be simplified to
-         preprocessor (nn.Module): preprocessor for inputs
-     '''
--
--    def __init__(self, model, points_batch_size=100000,
--                 threshold=0.5, refinement_step=0, device=None,
--                 resolution0=16, upsampling_steps=3,
--                 with_normals=False, padding=0.1, sample=False,
-+    def __init__(self,
-+                 model,
-+                 points_batch_size=100000,
-+                 threshold=0.5,
-+                 refinement_step=0,
-+                 device=None,
-+                 resolution0=16,
-+                 upsampling_steps=3,
-+                 with_normals=False,
-+                 padding=0.1,
-+                 sample=False,
-                  simplify_nfaces=None,
--                 preprocessor=None):
-+                 preprocessor=None,
-+                 pnet_point_scale=6,
-+                 is_explicit_mesh=False,
-+                 **kwargs):
-         self.model = model.to(device)
-         self.points_batch_size = points_batch_size
-         self.refinement_step = refinement_step
-@@ -49,6 +59,8 @@ class Generator3D(object):
-         self.sample = sample
-         self.simplify_nfaces = simplify_nfaces
-         self.preprocessor = preprocessor
-+        self.pnet_point_scale = pnet_point_scale
-+        self.is_explicit_mesh = is_explicit_mesh
- 
-     def generate_mesh(self, data, return_stats=True):
-         ''' Generates the output mesh.
-@@ -77,15 +89,24 @@ class Generator3D(object):
-             c = self.model.encode_inputs(inputs)
-         stats_dict['time (encode inputs)'] = time.time() - t0
- 
--        z = self.model.get_z_from_prior((1,), sample=self.sample).to(device)
--        mesh = self.generate_from_latent(z, c, stats_dict=stats_dict, **kwargs)
-+        z = self.model.get_z_from_prior((1, ), sample=self.sample).to(device)
-+        mesh = self.generate_from_latent(z,
-+                                         c,
-+                                         stats_dict=stats_dict,
-+                                         data=data,
-+                                         **kwargs)
- 
-         if return_stats:
-             return mesh, stats_dict
-         else:
-             return mesh
- 
--    def generate_from_latent(self, z, c=None, stats_dict={}, **kwargs):
-+    def generate_from_latent(self,
-+                             z,
-+                             c=None,
-+                             stats_dict={},
-+                             data=None,
-+                             **kwargs):
-         ''' Generates mesh from latent.
- 
-         Args:
-@@ -93,48 +114,88 @@ class Generator3D(object):
-             c (tensor): latent conditioned code c
-             stats_dict (dict): stats dictionary
-         '''
--        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-+        assert data is not None
- 
--        t0 = time.time()
--        # Compute bounding box size
--        box_size = 1 + self.padding
-+        if self.is_explicit_mesh:
-+            normal_faces = data.get('angles.normal_face').to(self.device)
-+            normal_angles = data.get('angles.normal_angles').to(self.device)
-+            t0 = time.time()
-+            output = self.model.decode(None,
-+                                       z,
-+                                       c,
-+                                       angles=normal_angles,
-+                                       **kwargs)
-+            normal_vertices, normal_mask, _, _, _ = output
-+            stats_dict['time (eval points)'] = time.time() - t0
- 
--        # Shortcut
--        if self.upsampling_steps == 0:
--            nx = self.resolution0
--            pointsf = box_size * make_3d_grid(
--                (-0.5,)*3, (0.5,)*3, (nx,)*3
--            )
--            values = self.eval_points(pointsf, z, c, **kwargs).cpu().numpy()
--            value_grid = values.reshape(nx, nx, nx)
--        else:
--            mesh_extractor = MISE(
--                self.resolution0, self.upsampling_steps, threshold)
--
--            points = mesh_extractor.query()
--
--            while points.shape[0] != 0:
--                # Query points
--                pointsf = torch.FloatTensor(points).to(self.device)
--                # Normalize to bounding box
--                pointsf = pointsf / mesh_extractor.resolution
--                pointsf = box_size * (pointsf - 0.5)
--                # Evaluate model and update
--                values = self.eval_points(
--                    pointsf, z, c, **kwargs).cpu().numpy()
--                values = values.astype(np.float64)
--                mesh_extractor.update(points, values)
--                points = mesh_extractor.query()
-+            t0 = time.time()
-+            B, N, P, D = normal_vertices.shape
-+            normal_faces_all = torch.cat([(normal_faces + idx * P)
-+                                          for idx in range(N)],
-+                                         axis=1)
-+            assert B == 1
-+            mem_t = time.time()
-+            verts = normal_vertices.view(
-+                N * P, D).to('cpu').detach().numpy() / self.pnet_point_scale
-+            faces = normal_faces_all.view(-1, 3).to('cpu').detach().numpy()
-+            visbility = (normal_mask > 0.5).view(N *
-+                                                 P).to('cpu').detach().numpy()
-+            skip_t = time.time() - mem_t
-+
-+            mesh = trimesh.Trimesh(
-+                verts,
-+                faces,
-+                process=False,
-+                vertex_attributes={'vertex_visibility': visbility})
-+            stats_dict['time (copy to trimesh)'] = time.time() - t0 - skip_t
- 
--            value_grid = mesh_extractor.to_dense()
-+        else:
-+            t0 = time.time()
-+            threshold = 0.
-+            #threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-+
-+            # Compute bounding box size
-+            box_size = 1 + self.padding
-+
-+            # Shortcut
-+            if self.upsampling_steps == 0:
-+                nx = self.resolution0
-+                pointsf = box_size * make_3d_grid((-0.5, ) * 3, (0.5, ) * 3,
-+                                                  (nx, ) * 3)
-+                values = self.eval_points(pointsf, z, c, data=data,
-+                                          **kwargs).cpu().numpy()
-+                value_grid = values.reshape(nx, nx, nx)
-+            else:
-+                mesh_extractor = MISE(self.resolution0, self.upsampling_steps,
-+                                      threshold)
- 
--        # Extract mesh
--        stats_dict['time (eval points)'] = time.time() - t0
-+                points = mesh_extractor.query()
- 
--        mesh = self.extract_mesh(value_grid, z, c, stats_dict=stats_dict)
-+                while points.shape[0] != 0:
-+                    # Query points
-+                    pointsf = torch.FloatTensor(points).to(self.device)
-+                    # Normalize to bounding box
-+                    pointsf = pointsf / mesh_extractor.resolution
-+                    pointsf = box_size * (pointsf - 0.5)
-+                    # Evaluate model and update
-+                    values = self.eval_points(pointsf,
-+                                              z,
-+                                              c,
-+                                              data=data,
-+                                              **kwargs).cpu().numpy()
-+                    values = values.astype(np.float64)
-+                    mesh_extractor.update(points, values)
-+                    points = mesh_extractor.query()
-+
-+                value_grid = mesh_extractor.to_dense()
-+
-+            # Extract mesh
-+            stats_dict['time (eval points)'] = time.time() - t0
-+
-+            mesh = self.extract_mesh(value_grid, z, c, stats_dict=stats_dict)
-         return mesh
- 
--    def eval_points(self, p, z, c=None, **kwargs):
-+    def eval_points(self, p, z, c=None, data=None, **kwargs):
-         ''' Evaluates the occupancy values for the points.
- 
-         Args:
-@@ -142,13 +203,26 @@ class Generator3D(object):
-             z (tensor): latent code z
-             c (tensor): latent conditioned code c
-         '''
-+        assert data is not None
-         p_split = torch.split(p, self.points_batch_size)
-+
-+        angles = data.get('angles').to(self.device)
-         occ_hats = []
- 
-         for pi in p_split:
-             pi = pi.unsqueeze(0).to(self.device)
-+            an = angles.to(self.device)
-             with torch.no_grad():
--                occ_hat = self.model.decode(pi, z, c, **kwargs).logits
-+                #_, _, sgn, _ = self.model.decode(pi, z, c, **kwargs).logits
-+                _, _, sgn, _, _ = self.model.decode(pi * self.pnet_point_scale,
-+                                                    z,
-+                                                    c,
-+                                                    angles=an,
-+                                                    **kwargs)
-+
-+                occ_hat = (
-+                    model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1) >
-+                    self.threshold).float()
- 
-             occ_hats.append(occ_hat.squeeze(0).detach().cpu())
- 
-@@ -168,20 +242,20 @@ class Generator3D(object):
-         # Some short hands
-         n_x, n_y, n_z = occ_hat.shape
-         box_size = 1 + self.padding
--        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-+        threshold = 0.
-+        #threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-         # Make sure that mesh is watertight
-         t0 = time.time()
--        occ_hat_padded = np.pad(
--            occ_hat, 1, 'constant', constant_values=-1e6)
--        vertices, triangles = libmcubes.marching_cubes(
--            occ_hat_padded, threshold)
-+        occ_hat_padded = np.pad(occ_hat, 1, 'constant', constant_values=-1e6)
-+        vertices, triangles = libmcubes.marching_cubes(occ_hat_padded,
-+                                                       threshold)
-         stats_dict['time (marching cubes)'] = time.time() - t0
-         # Strange behaviour in libmcubes: vertices are shifted by 0.5
-         vertices -= 0.5
-         # Undo padding
-         vertices -= 1
-         # Normalize to bounding box
--        vertices /= np.array([n_x-1, n_y-1, n_z-1])
-+        vertices /= np.array([n_x - 1, n_y - 1, n_z - 1])
-         vertices = box_size * (vertices - 0.5)
- 
-         # mesh_pymesh = pymesh.form_mesh(vertices, triangles)
-@@ -197,7 +271,8 @@ class Generator3D(object):
-             normals = None
- 
-         # Create mesh
--        mesh = trimesh.Trimesh(vertices, triangles,
-+        mesh = trimesh.Trimesh(vertices,
-+                               triangles,
-                                vertex_normals=normals,
-                                process=False)
- 
-@@ -261,7 +336,7 @@ class Generator3D(object):
- 
-         # Some shorthands
-         n_x, n_y, n_z = occ_hat.shape
--        assert(n_x == n_y == n_z)
-+        assert (n_x == n_y == n_z)
-         # threshold = np.log(self.threshold) - np.log(1. - self.threshold)
-         threshold = self.threshold
- 
-@@ -290,10 +365,9 @@ class Generator3D(object):
-             face_normal = face_normal / \
-                 (face_normal.norm(dim=1, keepdim=True) + 1e-10)
-             face_value = torch.sigmoid(
--                self.model.decode(face_point.unsqueeze(0), z, c).logits
--            )
--            normal_target = -autograd.grad(
--                [face_value.sum()], [face_point], create_graph=True)[0]
-+                self.model.decode(face_point.unsqueeze(0), z, c).logits)
-+            normal_target = -autograd.grad([face_value.sum()], [face_point],
-+                                           create_graph=True)[0]
- 
-             normal_target = \
-                 normal_target / \
-diff --git a/im2mesh/pnet/models/__init__.py b/im2mesh/pnet/models/__init__.py
-index fd4e062..85fd048 100644
---- a/im2mesh/pnet/models/__init__.py
-+++ b/im2mesh/pnet/models/__init__.py
-@@ -109,9 +109,6 @@ class PeriodicShapeNetwork(nn.Module):
- 
-         assert angles is not None
-         return self.decoder(p, z, c, angles=angles, **kwargs)
--        logits = self.decoder(p, z, c, **kwargs)
--        p_r = dist.Bernoulli(logits=logits)
--        return p_r
- 
-     def infer_z(self, p, occ, c, **kwargs):
-         ''' Infers z.
-diff --git a/im2mesh/pnet/models/decoder.py b/im2mesh/pnet/models/decoder.py
-index ab92e38..32fe043 100644
---- a/im2mesh/pnet/models/decoder.py
-+++ b/im2mesh/pnet/models/decoder.py
-@@ -8,6 +8,8 @@ sys.path.insert(0, './external.periodic_shapes')
- from periodic_shapes.models import super_shape, super_shape_sampler, periodic_shape_sampler, sphere_sampler
- import time
- 
-+EPS = 1e-7
-+
- 
- class PeriodicShapeDecoderSimplest(nn.Module):
-     ''' Decoder with CBN class 2.
-@@ -45,7 +47,7 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-         paramnet_hidden_size=128,
-         paramnet_dense=True,
-         is_single_paramnet=False,
--        layer_depth=4,
-+        layer_depth=0,
-         skip_position=3,  # count start from input fc
-         is_skip=True,
-         shape_sampler_decoder_class='PrimitiveWiseGroupConvDecoder',
-@@ -54,12 +56,17 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-         no_last_bias=False,
-         supershape_freeze_rotation_scale=False,
-         get_features_from=[],
--        concat_input_feature_with_pose_feature=False):
-+        concat_input_feature_with_pose_feature=False,
-+        return_sdf=False,
-+        is_radius_reg=False,
-+        spherical_angles=False,
-+        last_scale=.1):
-         super().__init__()
-         assert dim in [2, 3]
-         self.is_train_periodic_shape_sampler = is_train_periodic_shape_sampler
-         self.get_features_from = get_features_from
-         self.concat_input_feature_with_pose_feature = concat_input_feature_with_pose_feature
-+        self.is_radius_reg = is_radius_reg
- 
-         self.primitive = super_shape.SuperShapes(
-             max_m,
-@@ -97,13 +104,16 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-             dim=dim,
-             factor=shape_sampler_decoder_factor,
-             no_encoder=True,
-+            last_scale=last_scale,
-             disable_learn_pose_but_transition=disable_learn_pose_but_transition,
-             is_shape_sampler_sphere=is_shape_sampler_sphere,
-             decoder_class=shape_sampler_decoder_class,
-             is_feature_angles=is_feature_angles,
-             is_feature_coord=is_feature_coord,
-             is_feature_radius=is_feature_radius,
--            no_last_bias=no_last_bias)
-+            no_last_bias=no_last_bias,
-+            spherical_angles=spherical_angles,
-+            return_sdf=return_sdf)
-         # simple_sampler = super_shape_sampler.SuperShapeSampler(max_m,
-         #                                                        n_primitives,
-         #                                                        dim=dim)
-@@ -126,12 +136,25 @@ class PeriodicShapeDecoderSimplest(nn.Module):
-             else:
-                 feature = color_feature
- 
--            assert feature.shape[-1] == 256 + 128
-             output = self.p_sampler(params,
-                                     thetas=angles,
-                                     coord=coord,
-                                     points=feature,
-                                     return_surface_mask=True)
-+            pcoord, o1, o2, o3 = output
-+            if self.is_radius_reg:
-+                # B, N, P, dim
-+                # pcoord
-+
-+                # B, N, 1, dim
-+                transition = params['transition'].unsqueeze(2)
-+                pcentered_coord = pcoord - transition
-+                radius = (pcentered_coord**2).sum(-1).clamp(min=EPS).sqrt()
-+                output = (pcoord, o1, o2, o3, radius)
-+
-+            else:
-+                output = (pcoord, o1, o2, o3, None)
-+
-         else:
-             output = self.simple_sampler(params,
-                                          thetas=angles,
-diff --git a/im2mesh/pnet/training.py b/im2mesh/pnet/training.py
-index c0ed9b9..53db728 100644
---- a/im2mesh/pnet/training.py
-+++ b/im2mesh/pnet/training.py
-@@ -49,7 +49,18 @@ class Trainer(BaseTrainer):
-                  is_onet_style_occ_loss=False,
-                  is_logits_by_softmax=False,
-                  is_l2_occ_loss=False,
--                 sgn_scale=100):
-+                 sgn_scale=100,
-+                 is_sdf=False,
-+                 is_logits_by_logsumexp=False,
-+                 is_logits_by_min=False,
-+                 is_cvx_net_merged_loss=False,
-+                 cvx_net_merged_loss_topk_samples=10,
-+                 cvx_net_merged_loss_coef=1,
-+                 is_eval_logits_by_max=False,
-+                 is_radius_reg=False,
-+                 radius_reg_coef=1.,
-+                 use_surface_mask=True,
-+                 sgn_offset=0):
-         self.model = model
-         self.optimizer = optimizer
-         self.device = device
-@@ -75,10 +86,27 @@ class Trainer(BaseTrainer):
-         self.is_logits_by_softmax = is_logits_by_softmax
-         self.is_l2_occ_loss = is_l2_occ_loss
-         self.sgn_scale = sgn_scale
-+        self.is_sdf = is_sdf
-+        self.is_logits_by_logsumexp = is_logits_by_logsumexp
-+        self.is_logits_by_min = is_logits_by_min
-+        self.is_eval_logits_by_max = is_eval_logits_by_max
-+        self.is_cvx_net_merged_loss = is_cvx_net_merged_loss
-+        self.cvx_net_merged_loss_topk_samples = cvx_net_merged_loss_topk_samples
-+        self.cvx_net_merged_loss_coef = cvx_net_merged_loss_coef
-+        self.sgn_offset = sgn_offset
-+        self.is_radius_reg = is_radius_reg
-+        self.radius_reg_coef = radius_reg_coef
-+        self.use_surface_mask = use_surface_mask
- 
-         if vis_dir is not None and not os.path.exists(vis_dir):
-             os.makedirs(vis_dir)
- 
-+        if self.add_pointcloud_occ:
-+            assert not self.is_sdf
-+
-+        if self.is_eval_logits_by_max:
-+            assert not self.is_sdf
-+
-     def train_step(self, data):
-         ''' Performs a training step.
- 
-@@ -130,21 +158,29 @@ class Trainer(BaseTrainer):
-         batch_size = points.size(0)
- 
-         with torch.no_grad():
--            _, _, sgn, _ = self.model(points_iou * self.pnet_point_scale,
--                                      inputs,
--                                      sample=self.eval_sample,
--                                      angles=angles,
--                                      **kwargs)
--
--        if self.is_logits_by_max:
--            logits = model_utils.convert_tsd_range_to_zero_to_one(
--                sgn.max(1)[0])
--        elif self.is_logits_by_sign_filter:
--            positive = torch.relu(sgn).sum(1)
--            negative = torch.relu(-sgn).sum(1)
--            logits = torch.where(positive >= negative, positive, -negative)
-+            _, _, sgn, _, _ = self.model(points_iou * self.pnet_point_scale,
-+                                         inputs,
-+                                         sample=self.eval_sample,
-+                                         angles=angles,
-+                                         **kwargs)
-+        if self.is_sdf:
-+            if self.is_logits_by_min:
-+                logits = (sgn.min(1)[0] <= 0).float()
-+            else:
-+                raise NotImplementedError
-         else:
--            logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1)
-+            if self.is_eval_logits_by_max:
-+                logits = (sgn >= 0.).float().max(1)[0]
-+            elif self.is_logits_by_max:
-+                logits = model_utils.convert_tsd_range_to_zero_to_one(
-+                    sgn.max(1)[0])
-+            elif self.is_logits_by_sign_filter:
-+                positive = torch.relu(sgn).sum(1)
-+                negative = torch.relu(-sgn).sum(1)
-+                logits = torch.where(positive >= negative, positive, -negative)
-+            else:
-+                logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(
-+                    1)
- 
-         occ_iou_np = (occ_iou >= self.threshold).cpu().numpy()
-         occ_iou_hat_np = (logits >= threshold).cpu().numpy()
-@@ -160,23 +196,32 @@ class Trainer(BaseTrainer):
-                                                  *points_voxels.size())
-             points_voxels = points_voxels.to(device)
-             with torch.no_grad():
--                _, _, sgn, _ = self.model(points_voxels *
--                                          self.pnet_point_scale,
--                                          inputs,
--                                          sample=self.eval_sample,
--                                          angles=angles,
--                                          **kwargs)
--
--            if self.is_logits_by_max:
--                logits = model_utils.convert_tsd_range_to_zero_to_one(
--                    sgn.max(1)[0])
--            elif self.is_logits_by_sign_filter:
--                positive = torch.relu(sgn).sum(1)
--                negative = torch.relu(-sgn).sum(1)
--                logits = torch.where(positive >= negative, positive, -negative)
-+                _, _, sgn, _, _ = self.model(points_voxels *
-+                                             self.pnet_point_scale,
-+                                             inputs,
-+                                             sample=self.eval_sample,
-+                                             angles=angles,
-+                                             **kwargs)
-+
-+            if self.is_sdf:
-+                if self.is_logits_by_min:
-+                    logits = (sgn.min(1)[0] <= 0).float()
-+                else:
-+                    raise NotImplementedError
-             else:
--                logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(
--                    1)
-+                if self.is_eval_logits_by_max:
-+                    logits = (sgn >= 0.).float().max(1)[0]
-+                elif self.is_logits_by_max:
-+                    logits = model_utils.convert_tsd_range_to_zero_to_one(
-+                        sgn.max(1)[0])
-+                elif self.is_logits_by_sign_filter:
-+                    positive = torch.relu(sgn).sum(1)
-+                    negative = torch.relu(-sgn).sum(1)
-+                    logits = torch.where(positive >= negative, positive,
-+                                         -negative)
-+                else:
-+                    logits = model_utils.convert_tsd_range_to_zero_to_one(
-+                        sgn).sum(1)
- 
-             voxels_occ_np = (voxels_occ >= 0.5).cpu().numpy()
-             occ_hat_np = (logits >= threshold).cpu().numpy()
-@@ -204,21 +249,28 @@ class Trainer(BaseTrainer):
- 
-         kwargs = {}
-         with torch.no_grad():
--            _, _, sgn, _ = self.model(p * self.pnet_point_scale,
--                                      inputs,
--                                      sample=self.eval_sample,
--                                      angles=angles,
--                                      **kwargs)
--
--        if self.is_logits_by_max:
--            logits = model_utils.convert_tsd_range_to_zero_to_one(
--                sgn.max(1)[0])
--        elif self.is_logits_by_sign_filter:
--            positive = torch.relu(sgn).sum(1)
--            negative = torch.relu(-sgn).sum(1)
--            logits = torch.where(positive >= negative, positive, -negative)
-+            _, _, sgn, _, _ = self.model(p * self.pnet_point_scale,
-+                                         inputs,
-+                                         sample=self.eval_sample,
-+                                         angles=angles,
-+                                         **kwargs)
-+
-+        if self.is_sdf:
-+            if self.is_logits_by_min:
-+                logits = (sgn.min(1)[0] <= 0).float()
-+            else:
-+                raise NotImplementedError
-         else:
--            logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1)
-+            if self.is_logits_by_max:
-+                logits = model_utils.convert_tsd_range_to_zero_to_one(
-+                    sgn.max(1)[0])
-+            elif self.is_logits_by_sign_filter:
-+                positive = torch.relu(sgn).sum(1)
-+                negative = torch.relu(-sgn).sum(1)
-+                logits = torch.where(positive >= negative, positive, -negative)
-+            else:
-+                logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(
-+                    1)
-         occ_hat = logits.view(batch_size, *shape)
-         voxels_out = (occ_hat >= self.threshold).cpu().numpy()
- 
-@@ -250,9 +302,13 @@ class Trainer(BaseTrainer):
-             data (dict): data dictionary
-         '''
-         device = self.device
--        points = data.get('points').to(device)
--        occ = data.get('points.occ').to(device)
--        print(occ.max(), occ.min(), (occ == 1).sum(), (occ == 0).sum())
-+        if self.is_sdf:
-+            points = data.get('sdf_points').to(device)
-+            sdf = data.get('sdf_points.distances').to(device)
-+            occ = (sdf <= 0).float()
-+        else:
-+            points = data.get('points').to(device)
-+            occ = data.get('points.occ').to(device)
-         inputs = data.get('inputs', torch.empty(points.size(0), 0)).to(device)
-         angles = data.get('angles').to(device)
-         pointcloud = data['pointcloud'].to(device)
-@@ -263,7 +319,7 @@ class Trainer(BaseTrainer):
-             normal_angles = data.get('angles.normal_angles').to(device)
- 
-         kwargs = {}
--        if self.add_pointcloud_occ:
-+        if self.add_pointcloud_occ and not self.is_sdf:
-             points = torch.cat([points, pointcloud], axis=1)
-             occ = torch.cat([
-                 occ,
-@@ -280,37 +336,61 @@ class Trainer(BaseTrainer):
- 
-         scaled_coord = points * self.pnet_point_scale
-         output = self.model.decode(scaled_coord, z, c, angles=angles, **kwargs)
--        super_shape_point, surface_mask, sgn, sgn_BxNxNP = output
-+        super_shape_point, surface_mask, sgn, sgn_BxNxNP, radius = output
- 
-         # losses
--        if self.is_logits_by_max:
--            logits = sgn.max(1)[0]
--        elif self.is_logits_by_sign_filter:
--            positive = torch.relu(sgn).sum(1)
--            negative = torch.relu(-sgn).sum(1)
--            logits = torch.where(positive >= negative, positive, -negative)
--        elif self.is_logits_by_softmax:
--            logits = (torch.softmax(sgn * 10, 1) * sgn).sum(1)
--        else:
--            logits = model_utils.convert_tsd_range_to_zero_to_one(
--                sgn, scale=self.sgn_scale).sum(1)
--
--        if self.is_onet_style_occ_loss:
--            loss_i = F.binary_cross_entropy_with_logits(logits,
--                                                        occ,
--                                                        reduction='none')
--            occupancy_loss = loss_i.sum(-1).mean()
--        elif self.is_l2_occ_loss:
--            occupancy_loss = ((logits - occ)**2).sum()
-+        if self.is_sdf:
-+            if self.is_logits_by_logsumexp:
-+                raise NotImplementedError
-+            if self.is_logits_by_sign_filter:
-+                raise NotImplementedError
-+            if self.is_logits_by_softmax:
-+                raise NotImplementedError
-+            if self.is_logits_by_max:
-+                raise NotImplementedError
-+            if self.is_logits_by_min:
-+                logits = sgn.min(1)[0]
-+            else:
-+                raise NotImplementedError
-+            """
-+            delta = 0.01 * self.pnet_point_scale**2
-+            occupancy_loss = (logits.clamp(min=-delta, max=delta) -
-+                              (sdf * self.pnet_point_scale**2).clamp(
-+                                  min=-delta, max=delta)).abs().mean()
-+            """
-+            occupancy_loss = (logits -
-+                              (sdf * self.pnet_point_scale**2)).abs().mean()
-+
-         else:
--            occupancy_loss = F.binary_cross_entropy_with_logits(logits, occ)
-+            if self.is_logits_by_max:
-+                logits = sgn.max(1)[0]
-+            elif self.is_logits_by_sign_filter:
-+                positive = torch.relu(sgn).sum(1)
-+                negative = torch.relu(-sgn).sum(1)
-+                logits = torch.where(positive >= negative, positive, -negative)
-+            elif self.is_logits_by_softmax:
-+                logits = (torch.softmax(sgn * 10, 1) * sgn).sum(1)
-+            else:
-+                logits = model_utils.convert_tsd_range_to_zero_to_one(
-+                    sgn, scale=self.sgn_scale).sum(1)
-+
-+            if self.is_onet_style_occ_loss:
-+                loss_i = F.binary_cross_entropy_with_logits(logits,
-+                                                            occ,
-+                                                            reduction='none')
-+                occupancy_loss = loss_i.sum(-1).mean()
-+            elif self.is_l2_occ_loss:
-+                occupancy_loss = ((logits - occ)**2).sum()
-+            else:
-+                occupancy_loss = F.binary_cross_entropy_with_logits(
-+                    logits - self.sgn_offset, occ)
- 
-         scaled_target_point = pointcloud * self.pnet_point_scale
- 
-         chamfer_loss, _ = custom_chamfer_loss.custom_chamfer_loss(
-             super_shape_point,
-             scaled_target_point,
--            surface_mask=surface_mask,
-+            surface_mask=(surface_mask if self.use_surface_mask else None),
-             prob=None,
-             pykeops=True,
-             apply_surface_mask_before_chamfer=self.is_strict_chamfer)
-@@ -321,7 +401,7 @@ class Trainer(BaseTrainer):
-                                        c,
-                                        angles=normal_angles,
-                                        **kwargs)
--            normal_vertices, normal_mask, _, _ = output
-+            normal_vertices, normal_mask, _, _, _ = output
- 
-             B, N, P, D = normal_vertices.shape
-             normal_faces_all = torch.cat([(normal_faces + idx * P)
-@@ -354,8 +434,24 @@ class Trainer(BaseTrainer):
-                       overlap_reg * self.overlap_reg_coef +
-                       self_overlap_reg * self.self_overlap_reg_coef)
- 
-+        if self.is_cvx_net_merged_loss:
-+            merged_loss = torch.topk(sgn,
-+                                     self.cvx_net_merged_loss_topk_samples,
-+                                     2)[0]
-+            merged_loss = (torch.relu(-merged_loss)**
-+                           2).mean(-1).mean() * self.cvx_net_merged_loss_coef
-+            print('cvx merged loss:', merged_loss.item())
-+            total_loss = total_loss + merged_loss
-+
-         if self.is_normal_loss:
-             total_loss = total_loss + normal_loss * self.normal_loss_coef
-+
-+        if self.is_radius_reg:
-+            assert radius is not None
-+            radius_reg_loss = torch.relu(1. -
-+                                         radius.std()) * self.radius_reg_coef
-+            print('radius reg loss', radius_reg_loss.item())
-+            total_loss = total_loss + radius_reg_loss
-         losses = {
-             'total_loss': total_loss,
-             'occupancy_loss': occupancy_loss * self.occupancy_loss_coef,
-diff --git a/train.py b/train.py
-index 7f90be1..d761de2 100755
---- a/train.py
-+++ b/train.py
-@@ -57,10 +57,15 @@ if not os.path.exists(out_dir):
- train_dataset = config.get_dataset('train', cfg)
- val_dataset = config.get_dataset('val', cfg)
- 
-+if 'debug' in cfg['data']:
-+    train_shuffle = cfg['data']['debug'].get('train_shuffle', False)
-+else:
-+    train_shuffle = True
-+
- train_loader = torch.utils.data.DataLoader(train_dataset,
-                                            batch_size=batch_size,
-                                            num_workers=4,
--                                           shuffle=True,
-+                                           shuffle=train_shuffle,
-                                            collate_fn=data.collate_remove_none,
-                                            worker_init_fn=data.worker_init_fn,
-                                            drop_last=True)
-@@ -75,7 +80,8 @@ val_loader = torch.utils.data.DataLoader(
- 
- # For visualizations
- vis_loader = torch.utils.data.DataLoader(val_dataset,
--                                         batch_size=5,
-+                                         batch_size=cfg['training'].get(
-+                                             'vis_batch_size', 1),
-                                          shuffle=False,
-                                          collate_fn=data.collate_remove_none,
-                                          worker_init_fn=data.worker_init_fn)
diff --git a/eval_fscore.py b/eval_fscore.py
index f885115..46145fd 100644
--- a/eval_fscore.py
+++ b/eval_fscore.py
@@ -104,6 +104,10 @@ test_loader = torch.utils.data.DataLoader(dataset,
                                           num_workers=0,
                                           shuffle=False)
 
+vertex_attribute_filename = 'vertex_attributes'
+if cfg['test'].get('eval_from_vertex_attributes', False):
+    vertex_attribute_filename = cfg['test']['vertex_attribute_filename']
+    cfg['method'] = 'bspnet'
 # Evaluate all classes
 eval_dicts = []
 print('Evaluating meshes...')
@@ -178,8 +182,8 @@ for it, data in enumerate(tqdm(test_loader)):
                           visbility_file)
                     continue
         elif cfg['method'] == 'bspnet':
-            vertex_file = os.path.join(mesh_dir,
-                                       '%s_vertex_attributes.npz' % modelname)
+            vertex_file = os.path.join(
+                mesh_dir, '%s_%s.npz' % (modelname, vertex_attribute_filename))
             is_eval_explicit_mesh = True
             if os.path.exists(vertex_file):
                 try:
@@ -188,13 +192,17 @@ for it, data in enumerate(tqdm(test_loader)):
                 except:
                     print('Error in bspnet loading vertex')
                     continue
-                vertex_visibility = vertex_attributes['vertex_visibility']
+                try:
+                    vertex_visibility = vertex_attributes['vertex_visibility']
+                except:
+                    vertex_visibility = None
             else:
                 print('Warning: vertex file does not exist: %s' % vertex_file)
                 continue
         elif cfg['method'] == 'atlasnetv2':
             mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
 
+            is_eval_explicit_mesh = False
             if os.path.exists(mesh_file):
                 mesh = trimesh.load(mesh_file, process=False)
             else:
diff --git a/eval_fscore_cd1.py b/eval_fscore_cd1.py
index d9bcb0a..4a87d28 100644
--- a/eval_fscore_cd1.py
+++ b/eval_fscore_cd1.py
@@ -4,25 +4,32 @@ import time
 
 config_path = sys.argv[1]
 gpu_id = sys.argv[2]
-
+skip_cd1 = True
 command_cd1 = 'CUDA_VISIBLE_DEVICES={gpu_id} python3 eval_meshes.py {config_path} {args}'
 command_fsc = 'CUDA_VISIBLE_DEVICES={gpu_id} python3 eval_fscore.py {config_path} {args}'
 procs = []
 command3 = command_cd1.format(
     gpu_id=gpu_id,
     config_path=config_path,
-    args='--unique_name normalize --test.is_normalize_by_side_length true')
+    args=
+    '--unique_name mesh_iou_normalize --test.is_normalize_by_side_length true'
+    #'--unique_name mesh_iou_normalize --test.is_normalize_by_side_length true --test.is_eval_iou_by_split true'
+)
 print(command3)
 command4 = command_fsc.format(
     gpu_id=gpu_id,
     config_path=config_path,
-    args='--unique_name normalize --test.is_normalize_by_side_length true')
+    args=
+    '--unique_name mesh_iou_normalize --test.is_normalize_by_side_length true'
+    #'--unique_name mesh_iou_normalize_sample100k --test.is_normalize_by_side_length true --test.eval_from_vertex_attributes true --test.vertex_attribute_filename vertex_attributes_sample100k'
+)
 print(command4)
-proc_cd1 = subprocess.Popen(command3, shell=True)
-
-time.sleep(60)
+if not skip_cd1:
+    proc_cd1 = subprocess.Popen(command3, shell=True)
+    procs.append(proc_cd1)
+    time.sleep(60)
 proc_fsc = subprocess.Popen(command4, shell=True)
-procs.extend([proc_cd1, proc_fsc])
+procs.append(proc_fsc)
 
 for proc in procs:
     proc.communicate()
diff --git a/eval_meshes.py b/eval_meshes.py
index f13f25a..1c9dbab 100644
--- a/eval_meshes.py
+++ b/eval_meshes.py
@@ -93,14 +93,18 @@ yaml.dump(
 evaluator = MeshEvaluator(
     n_points=cfg['test']['n_points'],
     is_sample_from_surface=cfg['test']['is_sample_from_surface'],
-    is_normalize_by_side_length=cfg['test']['is_normalize_by_side_length'])
+    is_normalize_by_side_length=cfg['test']['is_normalize_by_side_length'],
+    is_eval_iou_by_split=cfg['test'].get('is_eval_iou_by_split', False))
 
 # Loader
 test_loader = torch.utils.data.DataLoader(dataset,
                                           batch_size=1,
                                           num_workers=0,
                                           shuffle=False)
-
+vertex_attribute_filename = 'vertex_attributes'
+if cfg['test'].get('eval_from_vertex_attributes', False):
+    vertex_attribute_filename = cfg['test']['vertex_attribute_filename']
+    cfg['method'] = 'bspnet'
 # Evaluate all classes
 eval_dicts = []
 print('Evaluating meshes...')
@@ -155,7 +159,7 @@ for it, data in enumerate(tqdm(test_loader)):
 
     # Evaluate mesh
     if cfg['test']['eval_mesh']:
-
+        mesh_for_iou = None
         vertex_visibility = None
         if cfg['method'] == 'pnet':
             mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
@@ -176,8 +180,8 @@ for it, data in enumerate(tqdm(test_loader)):
                           visbility_file)
                     continue
         elif cfg['method'] == 'bspnet':
-            vertex_file = os.path.join(mesh_dir,
-                                       '%s_vertex_attributes.npz' % modelname)
+            vertex_file = os.path.join(
+                mesh_dir, '%s_%s.npz' % (modelname, vertex_attribute_filename))
             is_eval_explicit_mesh = True
             if os.path.exists(vertex_file):
                 try:
@@ -185,10 +189,16 @@ for it, data in enumerate(tqdm(test_loader)):
                     mesh = trimesh.Trimesh(
                         vertex_attributes['vertices'],
                         vertex_normals=vertex_attributes['normals'])
+                    mesh_for_iou = trimesh.load(
+                        os.path.join(mesh_dir, '{}.off'.format(modelname)))
+                    assert isinstance(mesh_for_iou, trimesh.Trimesh)
                 except:
                     print('Error in bspnet loading vertex')
                     continue
-                vertex_visibility = vertex_attributes['vertex_visibility']
+                try:
+                    vertex_visibility = vertex_attributes['vertex_visibility']
+                except:
+                    vertex_visibility = None
             else:
                 print('Warning: vertex file does not exist: %s' % vertex_file)
                 continue
@@ -216,6 +226,7 @@ for it, data in enumerate(tqdm(test_loader)):
             normals_tgt,
             points_tgt,
             occ_tgt,
+            mesh_for_iou=mesh_for_iou,
             is_eval_explicit_mesh=is_eval_explicit_mesh,
             vertex_visibility=vertex_visibility,
             skip_iou=(cfg['method'] == 'atlasnetv2'))
diff --git a/eval_part_label.py b/eval_part_label.py
index 276ebf4..af7f00e 100644
--- a/eval_part_label.py
+++ b/eval_part_label.py
@@ -79,6 +79,7 @@ config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn30_20200501_191145/part_assignment_20200505_014749.yaml'
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn30_20200503_221032/debug_part_assignment_20200505_030654.yaml'
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn30_20200503_221032/part_assignment_20200505_031557.yaml'
+config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_20200502_104902/part_assignment_mesh_400_points_20200505_223546.yaml'
 if len(sys.argv) > 1:
     config_path = sys.argv[1]
 base_eval_dir = os.path.dirname(config_path)
diff --git a/eval_utils.py b/eval_utils.py
index 1a6761b..0c29ce3 100644
--- a/eval_utils.py
+++ b/eval_utils.py
@@ -114,14 +114,16 @@ def render_by_blender(rendering_script_path,
                       model_path,
                       rendering_out_dir,
                       name,
-                      skip_reconvert=False):
-    command = 'sh {script} {camera_param} {model} {out_dir} {idx} {skip_reconvert}'.format(
+                      skip_reconvert=False,
+                      use_cycles=False):
+    command = 'sh {script} {camera_param} {model} {out_dir} {idx} {skip_reconvert} {use_cycles}'.format(
         script=rendering_script_path,
         camera_param=camera_param_path,
         model=model_path,
         out_dir=rendering_out_dir,
         idx=name,
-        skip_reconvert=('true' if skip_reconvert else 'false'))
+        skip_reconvert=('true' if skip_reconvert else 'false'),
+        use_cycles=('true' if use_cycles else 'false'))
     print(command)
     subprocess.run(command, shell=True)
 
@@ -172,3 +174,92 @@ d {tr}
 
     with open('{}.obj'.format(name_wo_ext), 'w') as f:
         print(obj_body_text, file=f)
+
+
+def export_textured_mesh(meshes, textures, name):
+    mtl_format = """
+newmtl material{idx}
+Ka 0.4 0.4 0.4
+Ks 0.4 0.4 0.4
+Ns 1.0
+Tf 1.0 1.0 1.0
+map_Kd {texture}
+"""
+
+    scene = meshes[0].scene()
+    for mesh in meshes[1:]:
+        scene.add_geometry(mesh)
+    obj_body_text = trimesh.exchange.obj.export_obj(scene)
+
+    basename = os.path.basename(name)
+    basename, _ = os.path.splitext(basename)
+    name_wo_ext = os.path.splitext(name)[0]
+    obj_body_text = 'mtllib {}.mtl\n'.format(basename) + obj_body_text
+    mtl_bodies = []
+
+    assert len(textures) == len(meshes)
+
+    n_verts_sofar = 0
+    for idx, texture in enumerate(textures):
+
+        obj_body_text = obj_body_text.replace(
+            'o geometry_{id}\n'.format(id=idx),
+            'o geometry_{id}\nusemtl material{id}\n'.format(id=idx))
+        mtl = mtl_format.format(idx=idx, texture=texture)
+        mtl_bodies.append(mtl)
+
+    with open('{}.mtl'.format(name_wo_ext), 'w') as f:
+        for line in mtl_bodies:
+            print(line, file=f)
+
+    with open('{}.obj'.format(name_wo_ext), 'w') as f:
+        print(obj_body_text, file=f)
+
+
+def as_mesh(scene_or_mesh):
+    """
+    Convert a possible scene to a mesh.
+
+    If conversion occurs, the returned mesh has only vertex and face data.
+    """
+    if isinstance(scene_or_mesh, trimesh.Scene):
+        if len(scene_or_mesh.geometry) == 0:
+            mesh = None  # empty scene
+        else:
+            # we lose texture information here
+            mesh = trimesh.util.concatenate(
+                tuple(
+                    trimesh.Trimesh(vertices=g.vertices, faces=g.faces)
+                    for g in scene_or_mesh.geometry.values()))
+    else:
+        mesh = scene_or_mesh
+        assert (isinstance(mesh, trimesh.Trimesh))
+    return mesh
+
+
+def export_gt_mesh(shapenet_model_path, export_path):
+    mesh = as_mesh(trimesh.load(shapenet_model_path))
+    min3 = [0] * 3
+    max3 = [0] * 3
+
+    for i in range(3):
+        min3[i] = np.min(mesh.vertices[:, i])
+        max3[i] = np.max(mesh.vertices[:, i])
+
+    bb_min, bb_max = tuple(min3), tuple(max3)
+
+    # Get extents of model.
+    bb_min, bb_max = np.array(bb_min), np.array(bb_max)
+    total_size = (bb_max - bb_min).max()
+
+    # Set the center (although this should usually be the origin already).
+    centers = np.array([[(bb_min[0] + bb_max[0]) / 2,
+                         (bb_min[1] + bb_max[1]) / 2,
+                         (bb_min[2] + bb_max[2]) / 2]])
+    # Scales all dimensions equally.
+    scale = total_size
+
+    mesh.vertices -= centers
+    mesh.vertices *= 1. / scale
+    mesh.vertices *= (1 + side_length_scale)
+    mesh.export(export_path)
diff --git a/external/atlasnetv2/atlasnetv2 b/external/atlasnetv2/atlasnetv2
--- a/external/atlasnetv2/atlasnetv2
+++ b/external/atlasnetv2/atlasnetv2
@@ -1 +1 @@
-Subproject commit 09739b6328f969a6de084376696a5b57890494e8
+Subproject commit 09739b6328f969a6de084376696a5b57890494e8-dirty
diff --git a/external/bspnet b/external/bspnet
--- a/external/bspnet
+++ b/external/bspnet
@@ -1 +1 @@
-Subproject commit 61d5ec8951f5f1a3d33ae251c46faba62cd0b5dc
+Subproject commit 61d5ec8951f5f1a3d33ae251c46faba62cd0b5dc-dirty
diff --git a/generate.py b/generate.py
index 9866c3c..11decf9 100644
--- a/generate.py
+++ b/generate.py
@@ -87,6 +87,14 @@ else:
     out_dir = os.path.dirname(args.resume_generation_dir)
     generation_dir = args.resume_generation_dir
 
+if not args.explicit:
+    threshold_txt_path = os.path.join(out_dir, 'threshold')
+    if os.path.exists(threshold_txt_path):
+        with open(threshold_txt_path) as f:
+            threshold = float(f.readlines()[0].strip())
+            print('Use threshold in dir', threshold)
+            cfg['test']['threshold'] = threshold
+
 if not os.path.exists(generation_dir) and args.resume_generation_dir is None:
     os.makedirs(generation_dir)
 
@@ -285,7 +293,8 @@ for it, data in enumerate(tqdm(test_loader)):
             np.savez(visibility_out_file, vertex_visibility=visibility)
             out_file_dict['vertex_visibility'] = visibility_out_file
 
-        elif cfg['method'] == 'bspnet':
+        elif cfg['method'] == 'bspnet' and not cfg['generation'].get(
+                'is_gen_skip_vertex_attributes', False):
             np.savez(visibility_out_file,
                      vertex_visibility=visibility,
                      vertices=vertices,
diff --git a/generate_part_label.py b/generate_part_label.py
index 20825e2..51b0705 100644
--- a/generate_part_label.py
+++ b/generate_part_label.py
@@ -7,11 +7,12 @@ import pandas
 import pickle
 import subprocess
 import torch
+import argparse
 import matplotlib.pyplot as plt
 from matplotlib.ticker import FormatStrFormatter
 import sys
 import tempfile
-from collections import defaultdict
+from collections import defaultdict, OrderedDict
 sys.path.insert(0, '/home/mil/kawana/workspace/occupancy_networks')
 sys.path.insert(
     0,
@@ -34,10 +35,37 @@ import yaml
 import dotenv
 from bspnet import modelSVR
 from bspnet import utils as bsp_utils
+import eval_utils
 
 dotenv.load_dotenv('/home/mil/kawana/workspace/occupancy_networks/.env',
                    verbose=True)
 
+
+def represent_odict(dumper, instance):
+    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
+
+
+def construct_odict(loader, node):
+    return OrderedDict(loader.construct_pairs(node))
+
+
+yaml.add_representer(OrderedDict, represent_odict)
+yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
+
+parser = argparse.ArgumentParser(
+    description='Extract meshes from occupancy process.')
+parser.add_argument('config', type=str, help='Path to config file.')
+parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
+parser.add_argument(
+    '--explicit',
+    action='store_true',
+    help=
+    'to generate mesh with explicit rep, run: python3 generate.py --explicit --data.is_normal_icosahedron true --data.icosahedron_subdiv 4'
+)
+parser.add_argument('--unique_name',
+                    default='',
+                    type=str,
+                    help='String name for generation.')
 date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
 
 
@@ -62,10 +90,9 @@ class_names = ['airplane', 'chair', 'lamp', 'table']
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_20200502_001739/eval_config_20200502_105908.yaml'
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn256_author_provided_20200501_184850/config.yaml'
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal_20200502_202018/eval_config_20200502_172135.yaml'
-config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_20200502_104902/eval_config_20200502_173040.yaml'
-config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_20200502_041512/eval_config_20200502_200733.yaml'
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn30_20200501_191145/gen_part_label_20200504_210430.yaml'
 config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn30_20200503_221032/eval_config_20200503_221032.yaml'
+config_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_20200502_104902/eval_config_20200502_173040.yaml'
 if len(sys.argv) > 1:
     config_path = sys.argv[1]
 base_eval_dir = os.path.dirname(config_path)
@@ -122,6 +149,9 @@ class_names_to_part_ids = {
 
 # %%
 cfg = config.load_config(config_path, 'configs/default.yaml')
+
+args, unknown_args = parser.parse_known_args()
+eval_utils.update_dict_with_options(cfg, unknown_args)
 cfg['data']['classes'] = [label_to_synset[label] for label in class_names]
 
 #cfg['model']['decoder_kwargs']['extract_surface_point_by_max'] = True
@@ -132,6 +162,9 @@ cfg['data']['val_split'] = 'trainval'
 cfg['data']['train_split'] = 'trainval'
 cfg['data']['test_split'] = 'test'
 
+cfg['data']['is_normal_icosahedron'] = False
+cfg['data']['is_normal_uv_sphere'] = True
+
 if debug:
     #cfg['data']['debug'] = {'sample_n': 50}
     cfg['data']['classes'] = ['02691156']
diff --git a/im2mesh/atlasnetv2/config.py b/im2mesh/atlasnetv2/config.py
index 7b5e767..ae6d9a3 100644
--- a/im2mesh/atlasnetv2/config.py
+++ b/im2mesh/atlasnetv2/config.py
@@ -19,6 +19,7 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
     '''
     decoder = cfg['model']['decoder']
     encoder = cfg['model']['encoder']
+    encoder_pointcloud = cfg['model'].get('encoder_pointcloud', None)
     encoder_latent = cfg['model']['encoder_latent']
     dim = cfg['data']['dim']
     z_dim = cfg['model']['z_dim']
@@ -42,6 +43,11 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
         encoder = nn.Embedding(len(dataset), c_dim)
     elif encoder is not None:
         encoder = encoder_dict[encoder](c_dim=c_dim, **encoder_kwargs)
+        if encoder_pointcloud is not None:
+            encoder_pointcloud = encoder_dict[encoder_pointcloud](
+                npoint=cfg['data']['pointcloud_target_n'],
+                nlatent=c_dim,
+                **encoder_kwargs)
     else:
         encoder = None
 
@@ -49,9 +55,13 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
     model = models.AtlasNetV2(decoder,
                               encoder,
                               encoder_latent,
+                              encoder_pointcloud,
                               p0_z,
                               device=device)
-    if cfg['data']['input_type'] == 'pointcloud':
+    if cfg['data'][
+            'input_type'] == 'pointcloud' or encoder_pointcloud is not None and cfg[
+                'training'].get('atlasnetv2', {}).get('training_step',
+                                                      None) == 'autoencoder':
         model.apply(weights_init)
     return model
 
@@ -78,6 +88,9 @@ def get_trainer(model, optimizer, cfg, device, **kwargs):
                                threshold=threshold,
                                eval_sample=cfg['training']['eval_sample'],
                                debugged=cfg['training'].get('debugged', False),
+                               training_step=cfg['training'].get(
+                                   'atlasnetv2',
+                                   {}).get('training_step', None),
                                **cfg['trainer'])
 
     return trainer
@@ -104,6 +117,8 @@ def get_generator(model, cfg, device, **kwargs):
         simplify_nfaces=cfg['generation']['simplify_nfaces'],
         preprocessor=preprocessor,
         debugged=cfg['training'].get('debugged', False),
+        is_fit_to_gt_loc_scale=cfg['generation'].get('is_fit_to_gt_loc_scale',
+                                                     False),
         point_scale=cfg['trainer']['point_scale'])
     return generator
 
@@ -155,6 +170,7 @@ def get_data_fields(mode, cfg):
 
     fields['pointcloud'] = data.PointCloudField(cfg['data']['pointcloud_file'],
                                                 pointcloud_transform,
+                                                cfg=cfg,
                                                 with_transforms=True)
 
     if mode in ('val', 'test'):
diff --git a/im2mesh/atlasnetv2/generation.py b/im2mesh/atlasnetv2/generation.py
index 422aae4..ad320f0 100644
--- a/im2mesh/atlasnetv2/generation.py
+++ b/im2mesh/atlasnetv2/generation.py
@@ -9,6 +9,7 @@ from im2mesh.common import make_3d_grid
 from im2mesh.utils.libsimplify import simplify_mesh
 from im2mesh.utils.libmise import MISE
 from periodic_shapes.models import model_utils
+from bspnet import utils as bsp_utils
 import time
 
 
@@ -46,6 +47,7 @@ class Generator3D(object):
                  preprocessor=None,
                  point_scale=1,
                  debugged=False,
+                 is_fit_to_gt_loc_scale=False,
                  **kwargs):
         self.model = model.to(device)
         self.points_batch_size = points_batch_size
@@ -61,6 +63,7 @@ class Generator3D(object):
         self.preprocessor = preprocessor
         self.point_scale = point_scale
         self.debugged = debugged
+        self.is_fit_to_gt_loc_scale = is_fit_to_gt_loc_scale
 
     def generate_mesh(self, data, return_stats=True):
         ''' Generates the output mesh.
@@ -130,7 +133,16 @@ class Generator3D(object):
         faces_all = torch.cat([(faces + idx * P) for idx in range(N)], axis=1)
         assert B == 1
         mem_t = time.time()
-        npverts = predicted_vertices.view(N * P, D).to('cpu').detach().numpy()
+        npverts = predicted_vertices.view(1, N * P, D)
+
+        if self.is_fit_to_gt_loc_scale:
+            pointcloud = data.get('pointcloud').to(self.device).float()
+            npverts = bsp_utils.realign(npverts,
+                                        npverts.clone().detach(),
+                                        pointcloud,
+                                        adjust_bbox=True)
+
+        npverts = npverts.view(N * P, D).to('cpu').detach().numpy()
         npfaces = faces_all.view(-1, 3).to('cpu').detach().numpy()
         skip_t = time.time() - mem_t
 
diff --git a/im2mesh/atlasnetv2/models/__init__.py b/im2mesh/atlasnetv2/models/__init__.py
index 0dd0fef..a4cba92 100644
--- a/im2mesh/atlasnetv2/models/__init__.py
+++ b/im2mesh/atlasnetv2/models/__init__.py
@@ -26,6 +26,7 @@ class AtlasNetV2(nn.Module):
                  decoder,
                  encoder=None,
                  encoder_latent=None,
+                 encoder_pointcloud=None,
                  p0_z=None,
                  device=None):
         super().__init__()
@@ -43,6 +44,10 @@ class AtlasNetV2(nn.Module):
             self.encoder = encoder.to(device)
         else:
             self.encoder = None
+        if encoder_pointcloud is not None:
+            self.encoder_pointcloud = encoder_pointcloud.to(device)
+        else:
+            self.encoder_pointcloud = None
 
         self._device = device
         self.p0_z = p0_z
@@ -95,6 +100,21 @@ class AtlasNetV2(nn.Module):
 
         return c
 
+    def encode_pointcloud_inputs(self, inputs):
+        ''' Encodes the input.
+
+        Args:
+            input (tensor): the input
+        '''
+
+        if self.encoder is not None:
+            c = self.encoder_pointcloud(inputs)
+        else:
+            # Return inputs?
+            c = torch.empty(inputs.size(0), 0)
+
+        return c
+
     def decode(self, p, z, c, **kwargs):
         ''' Returns occupancy probabilities for the sampled points.
 
diff --git a/im2mesh/atlasnetv2/models/decoder.py b/im2mesh/atlasnetv2/models/decoder.py
index 0aeefc6..cfe7424 100644
--- a/im2mesh/atlasnetv2/models/decoder.py
+++ b/im2mesh/atlasnetv2/models/decoder.py
@@ -27,6 +27,7 @@ class AtlasNetV2Decoder(nn.Module):
                  c_dim=128,
                  hidden_size=128,
                  leaky=False,
+                 author_original_implementation=False,
                  **decoder_kwargs):
         super().__init__()
         self.z_dim = z_dim
@@ -34,7 +35,10 @@ class AtlasNetV2Decoder(nn.Module):
         decoder_kwargs['nlatent'] = hidden_size
         self.options = type('', (), decoder_kwargs)
         #self.decoder = PatchDeformGroupWiseMLPAdjInOcc(self.options)
-        self.decoder = PatchDeformMLPAdjInOcc(self.options)
+        self.decoder = PatchDeformMLPAdjInOcc(
+            self.options,
+            author_original_implementation=author_original_implementation)
+        self.author_original_implementation = author_original_implementation
 
     def forward(self, p, z, color_feature, grid=None, **kwargs):
         # grid (B, P, dim) -> (B, dim, P)
@@ -48,7 +52,7 @@ class AtlasNetV2Decoder(nn.Module):
 
 class PatchDeformMLPAdjInOcc(nn.Module):
     """Atlas net auto encoder"""
-    def __init__(self, options):
+    def __init__(self, options, author_original_implementation=False):
 
         super(PatchDeformMLPAdjInOcc, self).__init__()
 
@@ -57,6 +61,7 @@ class PatchDeformMLPAdjInOcc(nn.Module):
         self.patchDim = options.patchDim
         assert self.patchDim == 2
         self.patchDeformDim = options.patchDeformDim
+        self.author_original_implementation = author_original_implementation
 
         #encoder decoder and patch deformation module
         #==============================================================================
@@ -76,7 +81,8 @@ class PatchDeformMLPAdjInOcc(nn.Module):
         #==============================================================================
         #x = self.encoder(x.transpose(2, 1).contiguous())
         #==============================================================================
-
+        if self.training and self.author_original_implementation:
+            grid.uniform_(0, 1)
         outs = []
         patches = []
         for i in range(0, self.npatch):
diff --git a/im2mesh/atlasnetv2/scripts/generate_mesh.sh b/im2mesh/atlasnetv2/scripts/generate_mesh.sh
index 720a4d6..00c78da 100644
--- a/im2mesh/atlasnetv2/scripts/generate_mesh.sh
+++ b/im2mesh/atlasnetv2/scripts/generate_mesh.sh
@@ -3,5 +3,7 @@ config=$1
 GPU=${2:-0}
 CUDA_VISIBLE_DEVICES=$GPU python3 generate.py $1 \
 --explicit \
---data.is_generate_mesh true
+--unique_name f60k \
+--data.is_generate_mesh true \
+--data.patch_side_length 25
 #--data.classes [\"02691156\"] \
diff --git a/im2mesh/atlasnetv2/training.py b/im2mesh/atlasnetv2/training.py
index 356e7ca..9c2f51a 100644
--- a/im2mesh/atlasnetv2/training.py
+++ b/im2mesh/atlasnetv2/training.py
@@ -24,17 +24,19 @@ class Trainer(BaseTrainer):
         eval_sample (bool): whether to evaluate samples
 
     '''
-    def __init__(self,
-                 model,
-                 optimizer,
-                 device=None,
-                 input_type='img',
-                 vis_dir=None,
-                 threshold=0.5,
-                 point_scale=1,
-                 is_pykeops_loss=True,
-                 debugged=False,
-                 eval_sample=False):
+    def __init__(
+        self,
+        model,
+        optimizer,
+        device=None,
+        input_type='img',
+        vis_dir=None,
+        threshold=0.5,
+        point_scale=1,
+        is_pykeops_loss=True,
+        debugged=False,
+        training_step=None,  # if training_step == None, then directry training model wiht svr. Otherwise 'autoencoder' or 'svr'
+        eval_sample=False):
         self.model = model
         self.optimizer = optimizer
         self.device = device
@@ -45,6 +47,7 @@ class Trainer(BaseTrainer):
         self.point_scale = point_scale
         self.is_pykeops_loss = is_pykeops_loss
         self.debugged = debugged
+        self.training_step = training_step
 
         self.distChamferL2 = dist_chamfer.chamferDist()
 
@@ -79,7 +82,14 @@ class Trainer(BaseTrainer):
             pointcloud.size(0),
             0)).to(device) * (1 if self.debugged else self.point_scale)
         assert self.debugged
-        feature = self.model.encode_inputs(inputs)
+
+        if inputs.ndim == 3:  #pointcloud
+            inputs = inputs * self.point_scale
+
+        if self.training_step == 'autoencoder':
+            feature = self.model.encode_pointcloud_inputs(pointcloud)
+        else:
+            feature = self.model.encode_inputs(inputs)
 
         eval_dict = {}
         kwargs = {}
@@ -112,7 +122,10 @@ class Trainer(BaseTrainer):
             pointcloud.size(0),
             0)).to(device) * (1 if self.debugged else self.point_scale)
 
-        feature = self.model.encode_inputs(inputs)
+        if self.training_step == 'autoencoder':
+            feature = self.model.encode_pointcloud_inputs(pointcloud)
+        else:
+            feature = self.model.encode_inputs(inputs)
 
         eval_dict = {}
         kwargs = {}
@@ -153,6 +166,7 @@ class Trainer(BaseTrainer):
         Args:
             data (dict): data dictionary
         '''
+        assert self.debugged
         device = self.device
         pointcloud = data.get('pointcloud').to(device) * self.point_scale
         patch = data.get('patch').to(device)
@@ -163,22 +177,29 @@ class Trainer(BaseTrainer):
         kwargs = {}
 
         feature = self.model.encode_inputs(inputs)
+        if self.training_step in ['autoencoder', 'svr']:
+            pointcloud_feature = self.model.encode_pointcloud_inputs(
+                pointcloud)
 
         # General points
-        coords = self.model.decode(pointcloud,
-                                   None,
-                                   feature,
-                                   grid=patch,
-                                   **kwargs)
-        B, N, P, dims = coords.shape
-
-        if self.is_pykeops_loss:
-            loss = atv2_utils.chamfer_loss(coords.view(B, N * P, dims),
-                                           pointcloud)
+        if self.train_step == 'svr':
+            loss = torch.mean((feature - pointcloud_feature.detach())**2)
         else:
-            dist1, dist2 = self.distChamferL2(coords.view(B, N * P, dims),
-                                              pointcloud)
-            loss = torch.mean(dist1) + torch.mean(dist2)
+            coords = self.model.decode(
+                pointcloud,
+                None,
+                feature if self.training_step is None else pointcloud_feature,
+                grid=patch,
+                **kwargs)
+            B, N, P, dims = coords.shape
+
+            if self.is_pykeops_loss:
+                loss = atv2_utils.chamfer_loss(coords.view(B, N * P, dims),
+                                               pointcloud)
+            else:
+                dist1, dist2 = self.distChamferL2(coords.view(B, N * P, dims),
+                                                  pointcloud)
+                loss = torch.mean(dist1) + torch.mean(dist2)
 
         losses = {'total_loss': loss}
         return losses
diff --git a/im2mesh/bspnet/config.py b/im2mesh/bspnet/config.py
index e7975bb..7a3107e 100644
--- a/im2mesh/bspnet/config.py
+++ b/im2mesh/bspnet/config.py
@@ -81,6 +81,19 @@ def get_generator(model, cfg, device, **kwargs):
         sample=cfg['generation']['use_sampling'],
         refinement_step=cfg['generation']['refinement_step'],
         simplify_nfaces=cfg['generation']['simplify_nfaces'],
+        is_gen_primitive_wise_watertight_mesh=cfg['generation'].get(
+            'is_gen_primitive_wise_watertight_mesh', False),
+        is_gen_skip_vertex_attributes=cfg['generation'].get(
+            'is_gen_skip_vertex_attributes', False),
+        is_gen_watertight_mesh=cfg['generation'].get('is_gen_integrated_mesh',
+                                                     False),
+        is_just_measuring_time=cfg['generation'].get('is_just_measuring_time',
+                                                     False),
+        is_skip_realign=cfg['generation'].get('bspnet', {
+            'is_skip_realign': False
+        }).get('is_skip_realign', False),
+        is_fit_to_gt_loc_scale=cfg['generation'].get('is_fit_to_gt_loc_scale',
+                                                     False),
         preprocessor=preprocessor)
     return generator
 
@@ -121,10 +134,13 @@ def get_data_fields(mode, cfg):
     if cfg.get('sdf_generation', False):
         pointcloud_transform = None
 
-    fields['pointcloud'] = data.PointCloudField(cfg['data']['pointcloud_file'],
-                                                pointcloud_transform,
-                                                cfg,
-                                                with_transforms=True)
+    fields['pointcloud'] = data.PointCloudField(
+        cfg['data']['pointcloud_file'],
+        pointcloud_transform,
+        cfg,
+        with_transforms=True,
+        force_disable_bspnet_mode=cfg['data']['bspnet'].get(
+            'force_disable_bspnet_mode', False))
     if cfg['test'].get('is_eval_semseg', False):
         fields['labeled_pointcloud'] = data.PartLabeledPointCloudField(
             cfg['data']['semseg_pointcloud_file'], cfg)
diff --git a/im2mesh/bspnet/generation.py b/im2mesh/bspnet/generation.py
index e97b23d..ca88de1 100644
--- a/im2mesh/bspnet/generation.py
+++ b/im2mesh/bspnet/generation.py
@@ -45,6 +45,12 @@ class Generator3D(object):
                  sample=False,
                  simplify_nfaces=None,
                  preprocessor=None,
+                 is_gen_primitive_wise_watertight_mesh=False,
+                 is_gen_integrated_mesh=False,
+                 is_gen_skip_vertex_attributes=False,
+                 is_just_measuring_time=False,
+                 is_skip_realign=False,
+                 is_fit_to_gt_loc_scale=False,
                  **kwargs):
         self.model = model.to(device)
         self.points_batch_size = points_batch_size
@@ -58,6 +64,12 @@ class Generator3D(object):
         self.sample = sample
         self.simplify_nfaces = simplify_nfaces
         self.preprocessor = preprocessor
+        self.is_gen_primitive_wise_watertight_mesh = is_gen_primitive_wise_watertight_mesh
+        self.is_gen_skip_vertex_attributes = is_gen_skip_vertex_attributes
+        self.is_gen_integrated_mesh = is_gen_integrated_mesh
+        self.is_just_measuring_time = is_just_measuring_time
+        self.is_skip_realign = is_skip_realign
+        self.is_fit_to_gt_loc_scale = is_fit_to_gt_loc_scale
 
         self.gen_helper = modelSVR.BSPNetMeshGenerator(self.model,
                                                        device=self.device)
@@ -78,9 +90,11 @@ class Generator3D(object):
 
         occ_pointcloud = data.get('pointcloud').to(device)
 
-        imnet_pointcloud = data.get('pointcloud.imnet_points').to(
-            device).float()
-        imnet_pointcloud = self.gen_helper.roty90(imnet_pointcloud, inv=True)
+        if not self.is_skip_realign:
+            imnet_pointcloud = data.get('pointcloud.imnet_points').to(
+                device).float()
+            imnet_pointcloud = self.gen_helper.roty90(imnet_pointcloud,
+                                                      inv=True)
 
         kwargs = {}
 
@@ -96,28 +110,66 @@ class Generator3D(object):
             stats_dict['time (eval points)'] = t
 
             t0 = time.time()
-            fout2, t = self.gen_helper.gen_mesh(model_float,
-                                                out_m,
-                                                measure_time=True)
-
-            if fout2 == None:
-                print('invalid convex generation')
-                if return_stats:
-                    return None, {}, None, None, None
-                else:
-                    return None, None, None, None
-
-            stats_dict['time (Generate mesh from convex)'] = t
-
-            mesh = self.gen_helper.gen_trimesh(fout2)
+            if self.is_gen_primitive_wise_watertight_mesh:
+                mesh, t = self.gen_helper.gen_primitive_wise_watertight_mesh(
+                    model_float, out_m, measure_time=True)
+
+                if mesh == None:
+                    print('invalid convex generation')
+                    if return_stats:
+                        return None, {}, None, None, None
+                    else:
+                        return None, None, None, None
+
+                stats_dict['time (Generate mesh from convex)'] = t
+
+            if self.is_gen_integrated_mesh:
+                mesh, t = self.gen_helper.gen_integrated_mesh(
+                    model_float, out_m, measure_time=True)
+                stats_dict['time (Generate mesh from convex)'] = t
+
+            else:
+                fout2, t = self.gen_helper.gen_mesh(model_float,
+                                                    out_m,
+                                                    measure_time=True)
+
+                if fout2 == None:
+                    print('invalid convex generation')
+                    if return_stats:
+                        return None, {}, None, None, None
+                    else:
+                        return None, None, None, None
+
+                stats_dict['time (Generate mesh from convex)'] = t
+
+                mesh = self.gen_helper.gen_trimesh(fout2)
+
+        if self.is_just_measuring_time:
+            if return_stats:
+                return mesh, stats_dict, None, None, None
+            else:
+                return mesh, None, None, None
 
         verts = torch.from_numpy(mesh.vertices).float().to(
             self.device).unsqueeze(0)
         verts = self.gen_helper.roty90(verts, inv=True)
-        verts = utils.realign(verts, imnet_pointcloud, occ_pointcloud)
+        if not self.is_skip_realign:
+            verts = utils.realign(verts, imnet_pointcloud, occ_pointcloud)
+
+        if self.is_fit_to_gt_loc_scale:
+            verts = utils.realign(verts,
+                                  verts,
+                                  occ_pointcloud,
+                                  adjust_bbox=True)
+
         verts = verts[0].cpu().numpy()
         mesh.vertices = verts
 
+        if self.is_gen_skip_vertex_attributes:
+            if return_stats:
+                return mesh, stats_dict, None, None, None
+            else:
+                return mesh, None, None, None
         # (P, dim), (P, dim), (P,)
         with torch.no_grad():
             points, normal, visibility = self.gen_helper.sample_primitive_agnostic_surface_points(
@@ -129,7 +181,15 @@ class Generator3D(object):
         visibility = visibility[perm_idx]
 
         points = self.gen_helper.roty90(points.unsqueeze(0), inv=True)
-        points = utils.realign(points, imnet_pointcloud, occ_pointcloud)
+        if not self.is_skip_realign:
+            points = utils.realign(points, imnet_pointcloud, occ_pointcloud)
+
+        if self.is_fit_to_gt_loc_scale:
+            points = utils.realign(points,
+                                   points,
+                                   occ_pointcloud,
+                                   adjust_bbox=True)
+
         points = points[0].cpu().numpy()
 
         normal = self.gen_helper.roty90(normal.unsqueeze(0),
diff --git a/im2mesh/bspnet/scripts/generate_mesh.sh b/im2mesh/bspnet/scripts/generate_mesh.sh
deleted file mode 100644
index 720a4d6..0000000
--- a/im2mesh/bspnet/scripts/generate_mesh.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/bin/bash
-config=$1
-GPU=${2:-0}
-CUDA_VISIBLE_DEVICES=$GPU python3 generate.py $1 \
---explicit \
---data.is_generate_mesh true
-#--data.classes [\"02691156\"] \
diff --git a/im2mesh/data/fields.py b/im2mesh/data/fields.py
index 18e148c..54c8774 100644
--- a/im2mesh/data/fields.py
+++ b/im2mesh/data/fields.py
@@ -266,20 +266,35 @@ class PointCloudField(Field):
                  file_name,
                  transform=None,
                  cfg=None,
-                 with_transforms=False):
+                 with_transforms=False,
+                 force_disable_atlasnetv2_mode=False,
+                 force_disable_bspnet_mode=False):
         self.file_name = file_name
         self.transform = transform
         self.with_transforms = with_transforms
         self.cfg = cfg
+        self.force_disable_bspnet_mode = force_disable_bspnet_mode
+        self.force_disable_atlasnetv2_mode = force_disable_atlasnetv2_mode
 
         self.is_bspnet = False
-        if cfg is not None and self.cfg['method'] == 'bspnet':
+        if cfg is not None and self.cfg[
+                'method'] == 'bspnet' and not self.force_disable_bspnet_mode:
+
             self.is_bspnet = True
             assert 'bspnet' in self.cfg['data']
             bspnet_config = self.cfg['data']['bspnet']
             assert bspnet_config['path'].startswith('data')
             assert 'pointcloud_file' in bspnet_config
 
+        self.is_atlasnetv2 = False
+        if cfg is not None and self.cfg[
+                'method'] == 'atlasnetv2' and not self.force_disable_atlasnetv2_mode:
+
+            self.is_atlasnetv2 = True
+            assert 'atlasnetv2' in self.cfg['data']
+            atlasnetv2_config = self.cfg['data']['atlasnetv2']
+            assert atlasnetv2_config['path'].startswith('data')
+
     def load(self, model_path, idx, category):
         ''' Loads the data point.
 
@@ -309,6 +324,17 @@ class PointCloudField(Field):
                     os.path.join(bsp_model_path, self.cfg['data']['bspnet']
                                  ['pointcloud_file'])).vertices)
 
+        if self.is_atlasnetv2:
+            modelname = model_path.split('/')[-1]
+            model_path = model_path.replace(modelname, '').replace(
+                self.cfg['data']['path'],
+                self.cfg['data']['atlasnetv2']['path'])
+
+            data['atlasnetv2_points'] = torch.from_numpy(
+                trimesh.load(
+                    os.path.join(model_path, 'ply',
+                                 modelname + '.points.ply')).vertices)
+
         if self.with_transforms and 'loc' in data and 'scale' in data:
             data['loc'] = pointcloud_dict['loc'].astype(np.float32)
             data['scale'] = pointcloud_dict['scale'].astype(np.float32)
diff --git a/im2mesh/data/transforms.py b/im2mesh/data/transforms.py
index b471e37..dc36540 100644
--- a/im2mesh/data/transforms.py
+++ b/im2mesh/data/transforms.py
@@ -52,6 +52,13 @@ class SubsamplePointcloud(object):
         data_out[None] = points[indices, :]
         data_out['normals'] = normals[indices, :]
 
+        if 'atlasnetv2_points' in data:
+            N = min(self.N, data_out['atlasnetv2_points'].shape[0])
+            indices = np.random.randint(data_out['atlasnetv2_points'].shape[0],
+                                        size=N)
+            data_out['atlasnetv2_points'] = data_out['atlasnetv2_points'][
+                indices, :]
+
         return data_out
 
 
diff --git a/im2mesh/encoder/pointnet.py b/im2mesh/encoder/pointnet.py
index 64b60d5..cc39a4c 100644
--- a/im2mesh/encoder/pointnet.py
+++ b/im2mesh/encoder/pointnet.py
@@ -120,9 +120,9 @@ class AtlasNetV2PointNet(nn.Module):
         dim (int): input points dimension
         hidden_dim (int): hidden dimension of the network
     '''
-    def __init__(self, c_dim=2500, dim=3, hidden_dim=1024):
+    def __init__(self, npoint=2500, dim=3, nlatent=1024):
         super().__init__()
-        self.pointnet = PointNetfeat(npoint=c_dim, nlatent=hidden_dim)
+        self.pointnet = PointNetfeat(npoint=npoint, nlatent=nlatent)
 
     def forward(self, x):
         x = x.transpose(2, 1).contiguous()
diff --git a/im2mesh/eval.py b/im2mesh/eval.py
index 19549da..0139d6d 100644
--- a/im2mesh/eval.py
+++ b/im2mesh/eval.py
@@ -39,13 +39,17 @@ class MeshEvaluator(object):
     Args:
         n_points (int): number of points to be used for evaluation
     '''
-    def __init__(self,
-                 n_points=100000,
-                 is_sample_from_surface=False,
-                 is_normalize_by_side_length=False):
+    def __init__(
+        self,
+        n_points=100000,
+        is_sample_from_surface=False,
+        is_normalize_by_side_length=False,
+        is_eval_iou_by_split=False,
+    ):
         self.n_points = n_points
         self.is_sample_from_surface = is_sample_from_surface
         self.is_normalize_by_side_length = is_normalize_by_side_length
+        self.is_eval_iou_by_split = is_eval_iou_by_split
 
     def eval_mesh(self,
                   mesh,
@@ -55,6 +59,7 @@ class MeshEvaluator(object):
                   occ_tgt,
                   is_eval_explicit_mesh=False,
                   vertex_visibility=None,
+                  mesh_for_iou=None,
                   skip_iou=False):
         ''' Evaluates a mesh.
 
@@ -118,13 +123,28 @@ class MeshEvaluator(object):
         #print('eval point cloud', time.time() - t0)
 
         t0 = time.time()
-        if len(mesh.vertices) != 0 and len(
-                mesh.faces) != 0 and not skip_iou and False:
-            occ = check_mesh_contains(mesh, points_iou)
+        if mesh_for_iou is None:
+            mesh_for_iou = mesh
+        if len(mesh_for_iou.vertices) != 0 and len(
+                mesh_for_iou.faces) != 0 and not skip_iou:
+            if self.is_eval_iou_by_split:
+                meshes = mesh_for_iou.split()
+            else:
+                meshes = [mesh_for_iou]
+
+            if len(meshes) != 0:
+                for idx, mesh in enumerate(meshes):
+                    if idx == 0:
+                        occ = check_mesh_contains(mesh, points_iou)
+                    else:
+                        occ |= check_mesh_contains(mesh, points_iou)
+            else:
+                occ = check_mesh_contains(mesh_for_iou, points_iou)
+
             out_dict['iou'] = compute_iou(occ, occ_tgt)
         else:
             out_dict['iou'] = 0.
-        #print('iou', time.time() - t0)
+        print('iou', time.time() - t0)
 
         #print("eval_mesh", time.time() - t0)
         return out_dict
diff --git a/im2mesh/onet/config.py b/im2mesh/onet/config.py
index ec15fab..cf7dc3d 100644
--- a/im2mesh/onet/config.py
+++ b/im2mesh/onet/config.py
@@ -26,33 +26,30 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
     encoder_kwargs = cfg['model']['encoder_kwargs']
     encoder_latent_kwargs = cfg['model']['encoder_latent_kwargs']
 
-    decoder = models.decoder_dict[decoder](
-        dim=dim, z_dim=z_dim, c_dim=c_dim,
-        **decoder_kwargs
-    )
+    decoder = models.decoder_dict[decoder](dim=dim,
+                                           z_dim=z_dim,
+                                           c_dim=c_dim,
+                                           **decoder_kwargs)
 
     if z_dim != 0:
         encoder_latent = models.encoder_latent_dict[encoder_latent](
-            dim=dim, z_dim=z_dim, c_dim=c_dim,
-            **encoder_latent_kwargs
-        )
+            dim=dim, z_dim=z_dim, c_dim=c_dim, **encoder_latent_kwargs)
     else:
         encoder_latent = None
 
     if encoder == 'idx':
         encoder = nn.Embedding(len(dataset), c_dim)
     elif encoder is not None:
-        encoder = encoder_dict[encoder](
-            c_dim=c_dim,
-            **encoder_kwargs
-        )
+        encoder = encoder_dict[encoder](c_dim=c_dim, **encoder_kwargs)
     else:
         encoder = None
 
     p0_z = get_prior_z(cfg, device)
-    model = models.OccupancyNetwork(
-        decoder, encoder, encoder_latent, p0_z, device=device
-    )
+    model = models.OccupancyNetwork(decoder,
+                                    encoder,
+                                    encoder_latent,
+                                    p0_z,
+                                    device=device)
 
     return model
 
@@ -72,9 +69,12 @@ def get_trainer(model, optimizer, cfg, device, **kwargs):
     input_type = cfg['data']['input_type']
 
     trainer = training.Trainer(
-        model, optimizer,
-        device=device, input_type=input_type,
-        vis_dir=vis_dir, threshold=threshold,
+        model,
+        optimizer,
+        device=device,
+        input_type=input_type,
+        vis_dir=vis_dir,
+        threshold=threshold,
         eval_sample=cfg['training']['eval_sample'],
     )
 
@@ -100,6 +100,8 @@ def get_generator(model, cfg, device, **kwargs):
         sample=cfg['generation']['use_sampling'],
         refinement_step=cfg['generation']['refinement_step'],
         simplify_nfaces=cfg['generation']['simplify_nfaces'],
+        is_fit_to_gt_loc_scale=cfg['generation'].get('is_fit_to_gt_loc_scale',
+                                                     False),
         preprocessor=preprocessor,
     )
     return generator
@@ -113,10 +115,8 @@ def get_prior_z(cfg, device, **kwargs):
         device (device): pytorch device
     '''
     z_dim = cfg['model']['z_dim']
-    p0_z = dist.Normal(
-        torch.zeros(z_dim, device=device),
-        torch.ones(z_dim, device=device)
-    )
+    p0_z = dist.Normal(torch.zeros(z_dim, device=device),
+                       torch.ones(z_dim, device=device))
 
     return p0_z
 
@@ -133,11 +133,22 @@ def get_data_fields(mode, cfg):
 
     fields = {}
     fields['points'] = data.PointsField(
-        cfg['data']['points_file'], points_transform,
+        cfg['data']['points_file'],
+        points_transform,
         with_transforms=with_transforms,
         unpackbits=cfg['data']['points_unpackbits'],
     )
 
+    if cfg['generation'].get('is_fit_to_gt_loc_scale', False):
+        pointcloud_transform = data.SubsamplePointcloud(
+            cfg['data']['pointcloud_target_n'])
+
+        fields['pointcloud'] = data.PointCloudField(
+            cfg['data']['pointcloud_file'],
+            pointcloud_transform,
+            cfg,
+            with_transforms=True)
+
     if mode in ('val', 'test'):
         points_iou_file = cfg['data']['points_iou_file']
         voxels_file = cfg['data']['voxels_file']
diff --git a/im2mesh/onet/generation.py b/im2mesh/onet/generation.py
index 49786de..1150a2c 100644
--- a/im2mesh/onet/generation.py
+++ b/im2mesh/onet/generation.py
@@ -8,6 +8,7 @@ from im2mesh.utils import libmcubes
 from im2mesh.common import make_3d_grid
 from im2mesh.utils.libsimplify import simplify_mesh
 from im2mesh.utils.libmise import MISE
+from bspnet import utils as bsp_utils
 import time
 
 
@@ -30,12 +31,19 @@ class Generator3D(object):
         simplify_nfaces (int): number of faces the mesh should be simplified to
         preprocessor (nn.Module): preprocessor for inputs
     '''
-
-    def __init__(self, model, points_batch_size=100000,
-                 threshold=0.5, refinement_step=0, device=None,
-                 resolution0=16, upsampling_steps=3,
-                 with_normals=False, padding=0.1, sample=False,
+    def __init__(self,
+                 model,
+                 points_batch_size=100000,
+                 threshold=0.5,
+                 refinement_step=0,
+                 device=None,
+                 resolution0=16,
+                 upsampling_steps=3,
+                 with_normals=False,
+                 padding=0.1,
+                 sample=False,
                  simplify_nfaces=None,
+                 is_fit_to_gt_loc_scale=False,
                  preprocessor=None):
         self.model = model.to(device)
         self.points_batch_size = points_batch_size
@@ -49,6 +57,7 @@ class Generator3D(object):
         self.sample = sample
         self.simplify_nfaces = simplify_nfaces
         self.preprocessor = preprocessor
+        self.is_fit_to_gt_loc_scale = is_fit_to_gt_loc_scale
 
     def generate_mesh(self, data, return_stats=True):
         ''' Generates the output mesh.
@@ -77,9 +86,22 @@ class Generator3D(object):
             c = self.model.encode_inputs(inputs)
         stats_dict['time (encode inputs)'] = time.time() - t0
 
-        z = self.model.get_z_from_prior((1,), sample=self.sample).to(device)
+        z = self.model.get_z_from_prior((1, ), sample=self.sample).to(device)
         mesh = self.generate_from_latent(z, c, stats_dict=stats_dict, **kwargs)
 
+        if self.is_fit_to_gt_loc_scale:
+            pointcloud = data.get('pointcloud').to(self.device).float()
+            verts = torch.from_numpy(mesh.vertices).to(
+                self.device).float().unsqueeze(0)
+            try:
+                verts = bsp_utils.realign(verts,
+                                          verts,
+                                          pointcloud,
+                                          adjust_bbox=True)
+            except:
+                print('Realignment failed')
+            mesh.vertices = verts[0].detach().cpu().numpy()
+
         if return_stats:
             return mesh, stats_dict
         else:
@@ -102,14 +124,13 @@ class Generator3D(object):
         # Shortcut
         if self.upsampling_steps == 0:
             nx = self.resolution0
-            pointsf = box_size * make_3d_grid(
-                (-0.5,)*3, (0.5,)*3, (nx,)*3
-            )
+            pointsf = box_size * make_3d_grid((-0.5, ) * 3, (0.5, ) * 3,
+                                              (nx, ) * 3)
             values = self.eval_points(pointsf, z, c, **kwargs).cpu().numpy()
             value_grid = values.reshape(nx, nx, nx)
         else:
-            mesh_extractor = MISE(
-                self.resolution0, self.upsampling_steps, threshold)
+            mesh_extractor = MISE(self.resolution0, self.upsampling_steps,
+                                  threshold)
 
             points = mesh_extractor.query()
 
@@ -120,8 +141,8 @@ class Generator3D(object):
                 pointsf = pointsf / mesh_extractor.resolution
                 pointsf = box_size * (pointsf - 0.5)
                 # Evaluate model and update
-                values = self.eval_points(
-                    pointsf, z, c, **kwargs).cpu().numpy()
+                values = self.eval_points(pointsf, z, c,
+                                          **kwargs).cpu().numpy()
                 values = values.astype(np.float64)
                 mesh_extractor.update(points, values)
                 points = mesh_extractor.query()
@@ -171,17 +192,16 @@ class Generator3D(object):
         threshold = np.log(self.threshold) - np.log(1. - self.threshold)
         # Make sure that mesh is watertight
         t0 = time.time()
-        occ_hat_padded = np.pad(
-            occ_hat, 1, 'constant', constant_values=-1e6)
-        vertices, triangles = libmcubes.marching_cubes(
-            occ_hat_padded, threshold)
+        occ_hat_padded = np.pad(occ_hat, 1, 'constant', constant_values=-1e6)
+        vertices, triangles = libmcubes.marching_cubes(occ_hat_padded,
+                                                       threshold)
         stats_dict['time (marching cubes)'] = time.time() - t0
         # Strange behaviour in libmcubes: vertices are shifted by 0.5
         vertices -= 0.5
         # Undo padding
         vertices -= 1
         # Normalize to bounding box
-        vertices /= np.array([n_x-1, n_y-1, n_z-1])
+        vertices /= np.array([n_x - 1, n_y - 1, n_z - 1])
         vertices = box_size * (vertices - 0.5)
 
         # mesh_pymesh = pymesh.form_mesh(vertices, triangles)
@@ -197,7 +217,8 @@ class Generator3D(object):
             normals = None
 
         # Create mesh
-        mesh = trimesh.Trimesh(vertices, triangles,
+        mesh = trimesh.Trimesh(vertices,
+                               triangles,
                                vertex_normals=normals,
                                process=False)
 
@@ -261,7 +282,7 @@ class Generator3D(object):
 
         # Some shorthands
         n_x, n_y, n_z = occ_hat.shape
-        assert(n_x == n_y == n_z)
+        assert (n_x == n_y == n_z)
         # threshold = np.log(self.threshold) - np.log(1. - self.threshold)
         threshold = self.threshold
 
@@ -290,10 +311,9 @@ class Generator3D(object):
             face_normal = face_normal / \
                 (face_normal.norm(dim=1, keepdim=True) + 1e-10)
             face_value = torch.sigmoid(
-                self.model.decode(face_point.unsqueeze(0), z, c).logits
-            )
-            normal_target = -autograd.grad(
-                [face_value.sum()], [face_point], create_graph=True)[0]
+                self.model.decode(face_point.unsqueeze(0), z, c).logits)
+            normal_target = -autograd.grad([face_value.sum()], [face_point],
+                                           create_graph=True)[0]
 
             normal_target = \
                 normal_target / \
diff --git a/im2mesh/pnet/config.py b/im2mesh/pnet/config.py
index 13c638b..87f7e1e 100644
--- a/im2mesh/pnet/config.py
+++ b/im2mesh/pnet/config.py
@@ -106,6 +106,12 @@ def get_generator(model, cfg, device, **kwargs):
         preprocessor=preprocessor,
         pnet_point_scale=cfg['trainer']['pnet_point_scale'],
         is_explicit_mesh=cfg['generation'].get('is_explicit_mesh', False),
+        is_skip_surface_mask_generation_time=cfg['generation'].get(
+            'is_skip_surface_mask_generation_time', False),
+        is_just_measuring_time=cfg['generation'].get('is_just_measuring_time',
+                                                     False),
+        is_fit_to_gt_loc_scale=cfg['generation'].get('is_fit_to_gt_loc_scale',
+                                                     False),
         **cfg['generation'].get('mesh_kwargs', {}),
     )
     return generator
diff --git a/im2mesh/pnet/generation.py b/im2mesh/pnet/generation.py
index e2b5bd7..5b2c714 100644
--- a/im2mesh/pnet/generation.py
+++ b/im2mesh/pnet/generation.py
@@ -9,6 +9,7 @@ from im2mesh.common import make_3d_grid
 from im2mesh.utils.libsimplify import simplify_mesh
 from im2mesh.utils.libmise import MISE
 from periodic_shapes.models import model_utils
+from bspnet import utils as bsp_utils
 import time
 
 
@@ -46,6 +47,9 @@ class Generator3D(object):
                  preprocessor=None,
                  pnet_point_scale=6,
                  is_explicit_mesh=False,
+                 is_skip_surface_mask_generation_time=False,
+                 is_just_measuring_time=False,
+                 is_fit_to_gt_loc_scale=False,
                  **kwargs):
         self.model = model.to(device)
         self.points_batch_size = points_batch_size
@@ -61,6 +65,9 @@ class Generator3D(object):
         self.preprocessor = preprocessor
         self.pnet_point_scale = pnet_point_scale
         self.is_explicit_mesh = is_explicit_mesh
+        self.is_skip_surface_mask_generation_time = is_skip_surface_mask_generation_time
+        self.is_just_measuring_time = is_just_measuring_time
+        self.is_fit_to_gt_loc_scale = is_fit_to_gt_loc_scale
 
     def generate_mesh(self, data, return_stats=True):
         ''' Generates the output mesh.
@@ -119,14 +126,26 @@ class Generator3D(object):
         if self.is_explicit_mesh:
             normal_faces = data.get('angles.normal_face').to(self.device)
             normal_angles = data.get('angles.normal_angles').to(self.device)
+
+            if self.is_skip_surface_mask_generation_time:
+                t0 = time.time()
+                output = self.model.decode(None,
+                                           z,
+                                           c,
+                                           angles=normal_angles,
+                                           only_return_points=True,
+                                           **kwargs)
+                stats_dict['time (eval points)'] = time.time() - t0
             t0 = time.time()
-            output = self.model.decode(None,
-                                       z,
-                                       c,
-                                       angles=normal_angles,
-                                       **kwargs)
+            if not self.is_just_measuring_time:
+                output = self.model.decode(None,
+                                           z,
+                                           c,
+                                           angles=normal_angles,
+                                           **kwargs)
+            if not self.is_skip_surface_mask_generation_time:
+                stats_dict['time (eval points)'] = time.time() - t0
             normal_vertices, normal_mask, _, _, _ = output
-            stats_dict['time (eval points)'] = time.time() - t0
 
             t0 = time.time()
             B, N, P, D = normal_vertices.shape
@@ -135,11 +154,25 @@ class Generator3D(object):
                                          axis=1)
             assert B == 1
             mem_t = time.time()
+            if self.is_fit_to_gt_loc_scale:
+                pointcloud = data.get('pointcloud').to(self.device).float()
+                normal_vertices = normal_vertices.view(B, N * P, D)
+                normal_vertices = bsp_utils.realign(
+                    normal_vertices,
+                    normal_vertices.clone().detach(),
+                    pointcloud,
+                    adjust_bbox=True)
+                normal_vertices = normal_vertices.view(B, N, P, D)
+
             verts = normal_vertices.view(
                 N * P, D).to('cpu').detach().numpy() / self.pnet_point_scale
+
             faces = normal_faces_all.view(-1, 3).to('cpu').detach().numpy()
-            visbility = (normal_mask > 0.5).view(N *
-                                                 P).to('cpu').detach().numpy()
+            if self.is_just_measuring_time:
+                visbility = None
+            else:
+                visbility = (normal_mask > 0.5).view(
+                    N * P).to('cpu').detach().numpy()
             skip_t = time.time() - mem_t
 
             mesh = trimesh.Trimesh(
diff --git a/im2mesh/pnet/models/__init__.py b/im2mesh/pnet/models/__init__.py
index 85fd048..37801d1 100644
--- a/im2mesh/pnet/models/__init__.py
+++ b/im2mesh/pnet/models/__init__.py
@@ -98,7 +98,7 @@ class PeriodicShapeNetwork(nn.Module):
 
         return c
 
-    def decode(self, p, z, c, angles=None, **kwargs):
+    def decode(self, p, z, c, angles=None, only_return_points=False, **kwargs):
         ''' Returns occupancy probabilities for the sampled points.
 
         Args:
@@ -108,7 +108,7 @@ class PeriodicShapeNetwork(nn.Module):
         '''
 
         assert angles is not None
-        return self.decoder(p, z, c, angles=angles, **kwargs)
+        return self.decoder(p, z, c, angles=angles, only_return_points=only_return_points, **kwargs)
 
     def infer_z(self, p, occ, c, **kwargs):
         ''' Infers z.
diff --git a/im2mesh/pnet/models/decoder.py b/im2mesh/pnet/models/decoder.py
index 32d791d..ea3d0f4 100644
--- a/im2mesh/pnet/models/decoder.py
+++ b/im2mesh/pnet/models/decoder.py
@@ -123,7 +123,13 @@ class PeriodicShapeDecoderSimplest(nn.Module):
                                                            n_primitives,
                                                            dim=dim)
 
-    def forward(self, coord, _, color_feature, angles=None, **kwargs):
+    def forward(self,
+                coord,
+                _,
+                color_feature,
+                angles=None,
+                only_return_points=False,
+                **kwargs):
         params = self.primitive(color_feature)
 
         if self.is_train_periodic_shape_sampler:
@@ -137,12 +143,18 @@ class PeriodicShapeDecoderSimplest(nn.Module):
                     raise NotImplementedError
             else:
                 feature = color_feature
-
-            output = self.p_sampler(params,
-                                    thetas=angles,
-                                    coord=coord,
-                                    points=feature,
-                                    return_surface_mask=True)
+            if only_return_points:
+                output = self.p_sampler(params,
+                                        thetas=angles,
+                                        coord=None,
+                                        points=feature,
+                                        return_surface_mask=False)
+            else:
+                output = self.p_sampler(params,
+                                        thetas=angles,
+                                        coord=coord,
+                                        points=feature,
+                                        return_surface_mask=True)
             pcoord, o1, o2, o3 = output
         else:
             output = self.simple_sampler(params,
diff --git a/im2mesh/pnet/scripts/generate_mesh.sh b/im2mesh/pnet/scripts/generate_mesh.sh
index 8327182..aeb2b8a 100644
--- a/im2mesh/pnet/scripts/generate_mesh.sh
+++ b/im2mesh/pnet/scripts/generate_mesh.sh
@@ -1,13 +1,21 @@
 #!/bin/bash
 config=$1
 GPU=${2:-0}
+length=$3
 CUDA_VISIBLE_DEVICES=$GPU python3 generate.py $1 \
 --explicit \
+--unique_name f60k \
+--data.voxel_file null \
+--data.is_normal_uv_sphere true \
+--data.uv_sphere_length $length \
+--test.is_eval_explicit_mesh true \
+--generation.is_explicit_mesh true
+
+# Gen icosahedron mesh
+--explicit \
 --data.voxel_file null \
 --data.is_normal_icosahedron true \
 --data.icosahedron_subdiv 4 \
 --test.is_eval_explicit_mesh true \
 --generation.is_explicit_mesh true
-#--unique_name max_surface_extract \
-#--model.decoder_kwargs.extract_surface_point_by_max true
-#--data.classes [\"02691156\"] \
+
diff --git a/paper_resources/primitive_visualization/scripts/render_primitives.py b/paper_resources/primitive_visualization/scripts/render_primitives.py
index 0bd7ce8..f1558a2 100644
--- a/paper_resources/primitive_visualization/scripts/render_primitives.py
+++ b/paper_resources/primitive_visualization/scripts/render_primitives.py
@@ -25,6 +25,10 @@ from im2mesh.utils.io import export_pointcloud
 from im2mesh.utils.visualize import visualize_data
 import eval_utils
 from tqdm import tqdm
+import yaml
+import pickle
+from bspnet import utils as bsp_utils
+from bspnet import modelSVR
 os.chdir('/home/mil/kawana/workspace/occupancy_networks')
 
 
@@ -40,21 +44,27 @@ def separate_mesh_and_color(mesh_color_list):
 shapenetv1_path = '/data/unagi0/kawana/workspace/ShapeNetCore.v1'
 shapenetv2_path = '/data/unagi0/kawana/workspace/ShapeNetCore.v2'
 shapenetocc_path = '/home/mil/kawana/workspace/occupancy_networks/data/ShapeNet'
-side_length_scale = 0.0107337006427915
+side_length_scale = 0.01
 
 topk = 50
 colormap_name = 'jet'
 class_names = ['airplane', 'chair']
-primitive_id_map = {'airplane': [0, 15, 29], 'chair': [0, 15, 29]}
-base_eval_dir = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_20200413_015954'
+primitive_id_map = {'airplane': [3, 4, 6], 'chair': [3, 4, 6]}
+#primitive_id_map = {'airplane': list(range(10)), 'chair': list(range(10))}
+
+models_config_path = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/model_configs.yaml'
+configs = yaml.load(open(models_config_path, 'r'))
+attrs = configs['SHNet_10']
+
+base_eval_dir = attrs['base_eval_dir']
+fscore_pkl_path = attrs['vis_fscore']
+config_path = attrs['config_path']
+
 rendering_script_path = '/home/mil/kawana/workspace/occupancy_networks/scripts/render_3dobj.sh'
 rendering_out_base_dir = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/primitive_visualization'
 rendering_out_dir = os.path.join(rendering_out_base_dir, 'resources')
 camera_param_path = os.path.join(rendering_out_base_dir, 'camera_param.txt')
 mesh_dir_path = os.path.join(base_eval_dir, 'generation_explicit')
-fscore_pkl_path = os.path.join(mesh_dir_path,
-                               'eval_fscore_from_meshes_full_explicit.pkl')
-config_path = os.path.join(base_eval_dir, 'config.yaml')
 
 # %%
 synset_to_label = {
@@ -141,6 +151,7 @@ for idx in range(len(dataset)):
         indices.append(idx)
 # %%
 # TODO: for loop here
+"""
 for idx in indices:
     data = dataset[idx]
     model_dict = dataset.get_model_dict(idx)
@@ -245,6 +256,153 @@ for idx in indices:
 
             eval_utils.export_colored_mesh([mesh], [color], model_path)
 
+            eval_utils.render_by_blender(rendering_script_path,
+                                         camera_param_path,
+                                         model_path,
+                                         rendering_out_dir,
+                                         filename_template.format(
+                                             class_name=class_name,
+                                             model_id=model_id,
+                                             type=model_name,
+                                             id=pidx),
+                                         skip_reconvert=True)
+"""
+# %%
+# Model
+configs = yaml.load(open(models_config_path, 'r'))
+attrs = configs[
+    'BSPNet_30']  # airplane becomes almost same number of n primitives
+attrs = configs[
+    'BSPNet_256']  # airplane becomes almost same number of n primitives
+
+cfg = yaml.load(open(attrs['config_path']))
+
+# Dataset
+dataset = config.get_dataset('test', cfg, return_idx=True)
+
+model = config.get_model(cfg, device=device, dataset=dataset)
+
+checkpoint_io = CheckpointIO(base_eval_dir, model=model)
+checkpoint_io.load(cfg['test']['model_file'])
+
+# Loader
+test_loader = torch.utils.data.DataLoader(dataset,
+                                          batch_size=1,
+                                          num_workers=4,
+                                          shuffle=False)
+# Generate
+model.eval()
+indices = []
+for idx in range(len(dataset)):
+    model_dict = dataset.get_model_dict(idx)
+    model_id = model_dict['model']
+    class_id = model_dict.get('category', 'n/a')
+    if (class_id, model_id) in samples_to_render:
+        indices.append(idx)
+
+gen_helper = modelSVR.BSPNetMeshGenerator(model, device=device)
+# %%
+for idx in indices:
+    data = dataset[idx]
+    model_dict = dataset.get_model_dict(idx)
+    model_id = model_dict['model']
+    class_id = model_dict.get('category', 'n/a')
+    class_name = synset_to_label[class_id]
+
+    inputs = data.get('inputs', torch.empty(1, 0)).to(device).unsqueeze(0)
+
+    occ_pointcloud = torch.from_numpy(
+        data.get('pointcloud')).to(device).unsqueeze(0)
+
+    imnet_pointcloud = data.get('pointcloud.imnet_points').to(
+        device).float().unsqueeze(0)
+    imnet_pointcloud = gen_helper.roty90(imnet_pointcloud, inv=True)
+
+    kwargs = {}
+    print(model_id)
+    # Encode inputs
+    with torch.no_grad():
+        out_m, t = gen_helper.encode(inputs, measure_time=True)
+
+        model_float, t = gen_helper.eval_points(out_m, measure_time=True)
+
+        mesh, t = gen_helper.gen_primitive_wise_watertight_mesh(
+            model_float, out_m, measure_time=True)
+
+        verts = torch.from_numpy(mesh.vertices).float().to(device).unsqueeze(0)
+    verts = gen_helper.roty90(verts, inv=True)
+    verts = bsp_utils.realign(verts, imnet_pointcloud, occ_pointcloud)
+    verts = verts[0].cpu().numpy()
+
+    primitive_verts_for_rendering = eval_utils.normalize_verts_in_occ_way(
+        verts)
+    mesh.vertices = primitive_verts_for_rendering
+
+    meshes = mesh.split()
+    nparts = len(meshes)
+    bspnet_primitive_id_map = {
+        'airplane': list(range(nparts)),
+        'chair': list(range(nparts))
+    }
+    cm = plt.get_cmap(colormap_name, nparts)
+    # normed to 0 - 1
+    rgbas = [np.array(cm(idx)) for idx in range(nparts)]
+
+    colors = []
+    colors_emphasis = []
+    for pidx, rgba in enumerate(rgbas):
+
+        colors.append(rgba)
+        if pidx in bspnet_primitive_id_map[synset_to_label[class_id]]:
+            rgba_tr = rgba
+        else:
+            rgba_tr = [0.5, 0.5, 0.5, 0.2]
+        colors_emphasis.append(rgba_tr)
+
+    filename_template = 'bspnet_{class_name}_{model_id}_{type}_{id}'
+    with tempfile.TemporaryDirectory() as dname:
+        dname = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/primitive_visualization/cache'
+
+        model_name = 'colored_mesh'
+        model_path = os.path.join(dname, '{}.obj'.format(model_name))
+
+        eval_utils.export_colored_mesh(meshes, colors, model_path)
+
+        eval_utils.render_by_blender(rendering_script_path,
+                                     camera_param_path,
+                                     model_path,
+                                     rendering_out_dir,
+                                     filename_template.format(
+                                         class_name=class_name,
+                                         model_id=model_id,
+                                         type=model_name,
+                                         id=0),
+                                     skip_reconvert=True)
+
+        model_name = 'emphasis_mesh'
+        model_path = os.path.join(dname, '{}.obj'.format(model_name))
+
+        eval_utils.export_colored_mesh(meshes, colors_emphasis, model_path)
+
+        eval_utils.render_by_blender(rendering_script_path,
+                                     camera_param_path,
+                                     model_path,
+                                     rendering_out_dir,
+                                     filename_template.format(
+                                         class_name=class_name,
+                                         model_id=model_id,
+                                         type=model_name,
+                                         id=0),
+                                     skip_reconvert=True)
+
+        for pidx, (mesh, color) in enumerate(zip(meshes, colors)):
+            if not pidx in bspnet_primitive_id_map[synset_to_label[class_id]]:
+                continue
+            model_name = 'colored_primitive_mesh'
+            model_path = os.path.join(dname, '{}.obj'.format(model_name))
+
+            eval_utils.export_colored_mesh([mesh], [color], model_path)
+
             eval_utils.render_by_blender(rendering_script_path,
                                          camera_param_path,
                                          model_path,
diff --git a/paper_resources/shapenet_svr_comparison/scripts/generate_svr_table.py b/paper_resources/shapenet_svr_comparison/scripts/generate_svr_table.py
index 47113b7..94bd933 100644
--- a/paper_resources/shapenet_svr_comparison/scripts/generate_svr_table.py
+++ b/paper_resources/shapenet_svr_comparison/scripts/generate_svr_table.py
@@ -24,26 +24,24 @@ footer = """
 resource_base_dir_path = '/home/mil/kawana/workspace/occupancy_networks/paper_resources/shapenet_svr_comparison'
 csv_dir_path = os.path.join(resource_base_dir_path, 'csv')
 onet_new_fscore_table_path = '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/onet_pretrained/pretrained/eval_fscore_from_meshes.csv'
-fscore_key = 'fscore_th=0.0107337006427915 (mesh)'
+fscore_key = 'fscore_th=0.01 (mesh)'
 id_to_dir_path_map = {
-    'Sphere30':
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_20200414_051415',
-    'PSNet30':
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_20200413_015954'
-}
-id_to_mesh_dir_name = {
-    'Sphere30': 'generation_explicit',
-    'PSNet30': 'generation_explicit'
+    #'Sphere30':
+    #'/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_20200414_051415',
+    'SHNet':
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal_20200502_202018/generation_explicit_f60k_20200511_163104'
 }
+
 id_to_fscore_map = {
-    'Sphere30': 'eval_fscore_from_meshes_explicit.csv',
-    'PSNet30': 'eval_fscore_from_meshes_explicit.csv'
+    #'Sphere30': 'eval_fscore_from_meshes_explicit.csv',
+    'SHNet':
+    'eval_fscore_from_meshes_mesh_iou_normalize__explicit_20200511_214918.csv'
 }
-id_to_cd1_map = {
-    'Sphere30': 'eval_meshes_explicit.csv',
-    'PSNet30': 'eval_meshes_explicit.csv'
+id_to_cd1_iou_map = {
+    #'Sphere30': 'eval_meshes_explicit.csv',
+    'SHNet': 'eval_meshes_mesh_iou_normalize__explicit_20200512_003806.csv'
 }
-ids = list(id_to_cd1_map.keys())
+ids = list(id_to_cd1_iou_map.keys())
 
 # %%
 ioudf = pd.read_csv(os.path.join(csv_dir_path, 'iou.csv'))
@@ -56,7 +54,7 @@ fdf = pd.read_csv(os.path.join(csv_dir_path, 'fscore.csv'))
 
 for idx in ids:
     s = pd.read_csv(
-        os.path.join(id_to_dir_path_map[idx], id_to_mesh_dir_name[idx],
+        os.path.join(id_to_dir_path_map[idx],
                      id_to_fscore_map[idx]))[fscore_key]
     s = s * 100
     new_data = {
@@ -68,8 +66,8 @@ for idx in ids:
 
 for idx in ids:
     s = pd.read_csv(
-        os.path.join(id_to_dir_path_map[idx], id_to_mesh_dir_name[idx],
-                     id_to_cd1_map[idx]))['chamfer-L1 (mesh)']
+        os.path.join(id_to_dir_path_map[idx],
+                     id_to_cd1_iou_map[idx]))['chamfer-L1 (mesh)']
     new_data = {
         key: value
         for key, value in
@@ -79,7 +77,9 @@ for idx in ids:
     cd1df = cd1df.append(new_data, ignore_index=True)
 
 for idx in ids:
-    s = pd.read_csv(os.path.join(id_to_dir_path_map[idx], 'eval.csv'))['iou']
+    s = pd.read_csv(
+        os.path.join(id_to_dir_path_map[idx],
+                     id_to_cd1_iou_map[idx]))['iou (mesh)']
     new_data = {
         key: value
         for key, value in
@@ -88,6 +88,31 @@ for idx in ids:
     }
     ioudf = ioudf.append(new_data, ignore_index=True)
 
+# %%
+
+
+def cutdeci(s, deci=3):
+    if isinstance(s, str):
+        return s
+    deci_str = "{" + ":.{}".format(deci) + "f}"
+    return deci_str.format(s)
+
+
+def bold_if_neccessary(els, df, dfidx, elsidx, is_max=True, deci=3):
+    try:
+        num = float(els)
+    except:
+        return els
+    strs = [cutdeci(s, deci=deci) for s in df.iloc[:, elsidx].tolist()]
+    floats = map(float, strs)
+    is_bold = (is_max and num == max(floats)) or (not is_max
+                                                  and num == min(floats))
+    if is_bold:
+        return '{\bf ' + els + '}'
+    else:
+        return els
+
+
 # %%
 body = ""
 names = ' & '.join(['', '', *cd1df.columns[1:]]) + " \\ \hline"
@@ -98,13 +123,12 @@ for idx in range(len(cd1df)):
     else:
         r1 = ""
 
-    def cutdeci(s):
-        if isinstance(s, str):
-            return s
-        return "{:.3f}".format(s)
-
-    els = [r1, *map(cutdeci, cd1df.loc[idx].tolist())]
-    row = ' & '.join(els) + " \\ "
+    els = map(cutdeci, cd1df.loc[idx].tolist())
+    row = r1
+    for elsidx, els in enumerate(els):
+        row += (' & ' +
+                bold_if_neccessary(els, cd1df, idx, elsidx, is_max=False))
+    row += " \\ "
     body += ('\n' + row)
 body += '\hline'
 
@@ -114,13 +138,13 @@ for idx in range(len(fdf)):
     else:
         r1 = ""
 
-    def cutdeci(s):
-        if isinstance(s, str):
-            return s
-        return "{:.2f}".format(s)
+    els = [cutdeci(s, deci=2) for s in fdf.loc[idx].tolist()]
+    row = r1
+    for elsidx, els in enumerate(els):
+        row += (' & ' +
+                bold_if_neccessary(els, fdf, idx, elsidx, is_max=True, deci=2))
+    row += " \\ "
 
-    els = [r1, *map(cutdeci, fdf.loc[idx].tolist())]
-    row = ' & '.join(els) + " \\ "
     body += ('\n' + row)
 body = body + '\hline'
 
@@ -130,17 +154,18 @@ for idx in range(len(ioudf)):
     else:
         r1 = ""
 
-    def cutdeci(s):
-        if isinstance(s, str):
-            return s
-        return "{:.3f}".format(s)
+    els = map(cutdeci, ioudf.loc[idx].tolist())
+    row = r1
+    for elsidx, els in enumerate(els):
+        row += (' & ' +
+                bold_if_neccessary(els, ioudf, idx, elsidx, is_max=True))
 
-    els = [r1, *map(cutdeci, ioudf.loc[idx].tolist())]
-    row = ' & '.join(els) + " \\ "
+    row += " \\ "
     body += '\n' + row
 
 with open(os.path.join(resource_base_dir_path, 'table.txt'), 'w') as f:
-    print(body.replace('\\', '\\\\').replace('\\\h',
-                                             '\h').replace('\\\mu', '\mu'),
+    print(body.replace('\\', '\\\\').replace('\\\h', '\h').replace(
+        '\b', '\\b').replace('\\\mu', '\mu'),
           file=f)
 
+# %%
diff --git a/paper_resources/shapenet_svr_comparison/table.txt b/paper_resources/shapenet_svr_comparison/table.txt
index a09655e..26e02f7 100644
--- a/paper_resources/shapenet_svr_comparison/table.txt
+++ b/paper_resources/shapenet_svr_comparison/table.txt
@@ -1,24 +1,25 @@
  &  & airplane & bench & cabinet & car & chair & display & lamp & speaker & rifle & sofa & table & phone & vessel & mean \\ \hline
-\multirow{8}{*}{CD1} & AtlasNet & 0.104 & 0.138 & 0.175 & 0.141 & 0.209 & 0.198 & 0.305 & 0.245 & 0.115 & 0.177 & 0.190 & 0.128 & 0.151 & 0.175 \\ 
- & Pixel2Mesh & 0.187 & 0.201 & 0.196 & 0.180 & 0.265 & 0.239 & 0.308 & 0.285 & 0.164 & 0.212 & 0.218 & 0.149 & 0.212 & 0.216 \\ 
+\multirow{9}{*}{CD1} & Pixel2Mesh & 0.187 & 0.201 & 0.196 & 0.180 & 0.265 & 0.239 & 0.308 & 0.285 & 0.164 & 0.212 & 0.218 & 0.149 & 0.212 & 0.216 \\ 
+ & AtlasNet & 0.104 & 0.138 & 0.175 & 0.141 & 0.209 & {\bf 0.198} & {\bf 0.305} & {\bf 0.245} & 0.115 & 0.177 & 0.190 & 0.128 & {\bf 0.151} & 0.175 \\ 
+ & AtlasNetV2 & 0.119 & 0.164 & 0.246 & 0.176 & 0.256 & 0.209 & 0.313 & 0.340 & {\bf 0.099} & 0.210 & 0.221 & 0.131 & 0.159 & 0.203 \\ 
  & OccNet & 0.147 & 0.155 & 0.167 & 0.159 & 0.228 & 0.278 & 0.479 & 0.300 & 0.141 & 0.194 & 0.189 & 0.140 & 0.218 & 0.215 \\ 
- & OccNet* & 0.134 & 0.150 & 0.153 & 0.149 & 0.206 & 0.258 & 0.368 & 0.266 & 0.143 & 0.181 & 0.182 & 0.127 & 0.201 & 0.194 \\ 
+ & OccNet* & 0.141 & 0.154 & {\bf 0.149} & 0.150 & 0.206 & 0.214 & 0.369 & 0.254 & 0.142 & 0.182 & 0.175 & 0.124 & 0.194 & 0.189 \\ 
  & SIF & 0.167 & 0.261 & 0.233 & 0.161 & 0.380 & 0.401 & 1.096 & 0.554 & 0.193 & 0.272 & 0.454 & 0.159 & 0.208 & 0.349 \\ 
- & CvxNet & 0.093 & 0.133 & 0.160 & 0.103 & 0.337 & 0.223 & 0.795 & 0.462 & 0.106 & 0.164 & 0.358 & 0.083 & 0.173 & 0.245 \\ 
- & Sphere30 & 0.016 & 0.017 & 0.020 & 0.015 & 0.023 & 0.024 & 0.032 & 0.030 & 0.014 & 0.021 & 0.021 & 0.014 & 0.019 & 0.020 \\ 
- & PSNet30 & 0.011 & 0.013 & 0.016 & 0.014 & 0.018 & 0.020 & 0.027 & 0.024 & 0.011 & 0.018 & 0.015 & 0.012 & 0.017 & 0.017 \\ \hline
-\multirow{7}{*}{F-score} & AtlasNet & 67.24 & 54.50 & 46.43 & 51.51 & 38.89 & 42.79 & 33.04 & 35.75 & 64.22 & 43.46 & 44.93 & 58.85 & 49.87 & 48.57 \\ 
+ & CvxNet & {\bf 0.093} & {\bf 0.133} & 0.160 & {\bf 0.103} & 0.337 & 0.223 & 0.795 & 0.462 & 0.106 & {\bf 0.164} & 0.358 & {\bf 0.083} & 0.173 & 0.245 \\ 
+ & BSPNet & 0.119 & 0.151 & 0.171 & 0.144 & 0.200 & 0.212 & 0.322 & 0.253 & 0.111 & 0.180 & 0.182 & 0.140 & 0.173 & 0.181 \\ 
+ & SHNet & 0.111 & 0.135 & 0.155 & 0.136 & {\bf 0.191} & 0.205 & 0.320 & 0.251 & 0.118 & 0.177 & {\bf 0.167} & 0.110 & 0.174 & {\bf 0.173} \\ \hline
+\multirow{8}{*}{F-score} & AtlasNet & 67.24 & 54.50 & 46.43 & 51.51 & 38.89 & {\bf 42.79} & 33.04 & 35.75 & {\bf 64.22} & 43.46 & 44.93 & 58.85 & {\bf 49.87} & 48.57 \\ 
+ & AtlasNetV2 & 54.99 & 50.67 & 31.95 & 39.73 & 29.10 & 33.55 & 28.35 & 22.54 & 62.27 & 30.15 & 45.93 & 51.45 & 39.91 & 40.05 \\ 
  & OccNet & 62.87 & 56.91 & 61.79 & 56.91 & 42.41 & 38.96 & 38.35 & 42.48 & 56.52 & 48.62 & 58.49 & 66.09 & 42.37 & 51.75 \\ 
- & OccNet* & 65.54 & 59.42 & 64.24 & 63.54 & 45.24 & 42.94 & 42.73 & 46.36 & 60.12 & 51.65 & 61.05 & 67.95 & 47.23 & 55.23 \\ 
+ & OccNet* & 63.56 & 57.39 & {\bf 63.03} & 61.41 & {\bf 43.61} & 41.54 & {\bf 41.13} & {\bf 45.39} & 57.94 & {\bf 49.86} & {\bf 59.62} & 66.11 & 45.00 & 53.51 \\ 
  & SIF & 52.81 & 37.31 & 31.68 & 37.66 & 26.90 & 27.22 & 20.59 & 22.42 & 53.20 & 30.94 & 30.78 & 45.61 & 36.04 & 34.86 \\ 
- & CvxNet & 68.16 & 54.64 & 46.09 & 47.33 & 38.49 & 40.69 & 31.41 & 29.45 & 63.74 & 42.11 & 48.10 & 59.64 & 45.88 & 47.36 \\ 
- & Sphere30 & 53.25 & 52.33 & 39.53 & 53.08 & 37.83 & 36.28 & 33.45 & 28.57 & 57.27 & 38.43 & 51.17 & 58.51 & 44.55 & 44.94 \\ 
- & PSNet30 & 69.97 & 60.71 & 58.68 & 64.80 & 45.62 & 43.21 & 41.07 & 42.83 & 66.19 & 50.17 & 60.18 & 69.20 & 49.13 & 55.52 \\ \hline
-\multirow{8}{*}{IoU} & AtlasNet & nan & nan & nan & nan & nan & nan & nan & nan & nan & nan & nan & nan & nan & nan \\ 
- & Pixel2Mesh & 0.420 & 0.323 & 0.664 & 0.552 & 0.396 & 0.490 & 0.323 & 0.599 & 0.402 & 0.613 & 0.395 & 0.661 & 0.397 & 0.480 \\ 
+ & CvxNet & {\bf 68.16} & 54.64 & 46.09 & 47.33 & 38.49 & 40.69 & 31.41 & 29.45 & 63.74 & 42.11 & 48.10 & 59.64 & 45.88 & 47.36 \\ 
+ & BSPNet & 63.90 & 55.59 & 44.83 & 52.96 & 39.08 & 35.74 & 32.14 & 34.63 & 63.80 & 44.88 & 48.38 & 54.79 & 46.05 & 47.44 \\ 
+ & SHNet & 67.96 & {\bf 60.37} & 59.26 & {\bf 63.54} & 43.58 & 41.81 & 38.83 & 43.09 & 63.31 & 48.97 & 57.91 & {\bf 70.65} & 46.49 & {\bf 54.29} \\ \hline
+\multirow{7}{*}{IoU} & Pixel2Mesh & 0.420 & 0.323 & 0.664 & 0.552 & 0.396 & 0.490 & 0.323 & 0.599 & 0.402 & 0.613 & 0.395 & 0.661 & 0.397 & 0.480 \\ 
  & OccNet & 0.571 & 0.485 & 0.733 & 0.737 & 0.501 & 0.471 & 0.371 & 0.647 & 0.474 & 0.680 & 0.506 & 0.720 & 0.530 & 0.571 \\ 
- & OccNet* & 0.591 & 0.492 & 0.750 & 0.746 & 0.530 & 0.518 & 0.400 & 0.677 & 0.480 & 0.693 & 0.542 & 0.746 & 0.547 & 0.593 \\ 
+ & OccNet* & 0.591 & {\bf 0.492} & {\bf 0.750} & {\bf 0.746} & {\bf 0.530} & 0.518 & {\bf 0.400} & {\bf 0.677} & 0.480 & {\bf 0.693} & {\bf 0.542} & 0.746 & 0.547 & {\bf 0.593} \\ 
  & SIF & 0.530 & 0.333 & 0.648 & 0.657 & 0.389 & 0.491 & 0.260 & 0.577 & 0.463 & 0.606 & 0.372 & 0.658 & 0.502 & 0.499 \\ 
- & CvxNet & 0.598 & 0.461 & 0.709 & 0.675 & 0.491 & 0.576 & 0.311 & 0.620 & 0.515 & 0.677 & 0.473 & 0.719 & 0.552 & 0.567 \\ 
- & Sphere30 & 0.471 & 0.368 & 0.597 & 0.645 & 0.402 & 0.457 & 0.290 & 0.560 & 0.423 & 0.585 & 0.422 & 0.680 & 0.460 & 0.489 \\ 
- & PSNet30 & 0.590 & 0.436 & 0.725 & 0.738 & 0.500 & 0.555 & 0.361 & 0.666 & 0.488 & 0.682 & 0.487 & 0.743 & 0.543 & 0.578 \\ 
+ & CvxNet & 0.598 & 0.461 & 0.709 & 0.675 & 0.491 & {\bf 0.576} & 0.311 & 0.620 & 0.515 & 0.677 & 0.473 & 0.719 & {\bf 0.552} & 0.567 \\ 
+ & BSPNet & 0.512 & 0.369 & 0.636 & 0.681 & 0.440 & 0.500 & 0.305 & 0.613 & 0.465 & 0.633 & 0.392 & 0.677 & 0.511 & 0.518 \\ 
+ & SHNet & {\bf 0.613} & 0.461 & 0.719 & 0.742 & 0.515 & 0.553 & 0.368 & 0.667 & {\bf 0.516} & 0.689 & 0.511 & {\bf 0.760} & 0.550 & 0.589 \\ 
diff --git a/run_generate_part_label_all.py b/run_generate_part_label_all.py
index f16c105..8f9f3b1 100644
--- a/run_generate_part_label_all.py
+++ b/run_generate_part_label_all.py
@@ -32,29 +32,16 @@ config_paths = [
 script = 'generate_part_label.py'
 
 config_paths = [
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn3_20200501_191343/part_assignment_20200505_025509.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn6_20200501_191325/part_assignment_20200505_025549.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn8_20200502_134305/part_assignment_20200505_025607.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn10_20200501_191222/part_assignment_20200505_025630.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn15_20200504_012215/part_assignment_20200505_025652.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn20_20200502_133608/part_assignment_20200505_025712.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/bspnet_pn50_20200504_013233/part_assignment_20200505_025731.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn3_target_n_4096_no_overlap_reg_20200502_042724/part_assignment_20200505_025835.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn6_target_n_4096_no_overlap_reg_20200502_042856/part_assignment_20200505_025848.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn8_target_n_4096_no_overlap_reg_20200502_042147/part_assignment_20200505_025907.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_20200502_001739/part_assignment_20200505_025922.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg_20200502_041907/part_assignment_20200505_025938.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn20_target_n_4096_no_overlap_reg2_20200502_043525/part_assignment_20200505_025956.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_20200502_104902/part_assignment_20200505_030037.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal_20200502_202018/part_assignment_20200505_030058.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn50_target_n_4096_no_overlap_reg_20200502_041227/part_assignment_20200505_030119.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn3_20200503_220859/part_assignment_20200505_034110.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn6_20200503_220859/part_assignment_20200505_034110.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn8_20200503_222342/part_assignment_20200505_034108.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn10_20200503_220859/part_assignment_20200505_034110.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn15_20200503_221008/part_assignment_20200505_034110.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn20_20200503_220954/part_assignment_20200505_034110.yaml',
-    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn50_20200503_221053/part_assignment_20200505_034110.yaml'
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn10_target_n_4096_no_overlap_reg_20200502_001739/part_assignment_20200512_015430.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn15_target_n_4096_no_overlap_reg_20200502_041907/part_assignment_20200512_015529.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn20_target_n_4096_no_overlap_reg2_20200502_043525/part_assignment_20200512_015600.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn30_no_regs_no_normal_20200502_202018/part_assignment_20200512_015609.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/pnet_finetue_only_transition_cceff10_pn50_target_n_4096_no_overlap_reg_20200502_041227/part_assignment_20200512_015617.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn10_20200503_220859/part_assignment_20200512_013017.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn15_20200503_221008/part_assignment_20200512_013304.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn20_20200503_220954/part_assignment_20200512_025123.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn50_20200503_221053/part_assignment_20200512_013307.yaml',
+    '/home/mil/kawana/workspace/occupancy_networks/out/submission/eval/img/atlasnetv2_pn50_20200503_221053/part_assignment_20200512_013307.yaml',
 ]
 script = 'eval_part_label.py'
 
diff --git a/scripts/render_3dobj.sh b/scripts/render_3dobj.sh
index 61ed991..1717bb7 100755
--- a/scripts/render_3dobj.sh
+++ b/scripts/render_3dobj.sh
@@ -5,6 +5,7 @@ model_path=$2
 out=$3
 name=$4
 skip_reconvert_flag=${5:-"false"}
+use_cycles=${6:-"false"}
 RENDER_FOR_CNN_PATH=/home/mil/kawana/workspace/RenderForCNN
 
 PYTHONPATH=$PYTHONPATH:RENDER_FOR_CNN_PATH \
@@ -12,6 +13,7 @@ PYTHONPATH=$PYTHONPATH:RENDER_FOR_CNN_PATH \
 ${RENDER_FOR_CNN_PATH}/render_pipeline/blank.blend \
 --background \
 --python ${RENDER_FOR_CNN_PATH}/render_pipeline/render_model_views.py  \
+${use_cycles} \
 ${skip_reconvert_flag} \
 ${model_path} \
 ${name}  ${param} ${out}
diff --git a/train.py b/train.py
index 6a2c7d9..1042ada 100755
--- a/train.py
+++ b/train.py
@@ -165,9 +165,21 @@ nparameters = sum(p.numel() for p in model.parameters())
 wandb.watch(model)
 print('Total number of parameters: %d' % nparameters)
 
+is_linear_decay = 'learning_rage_decay_at' in cfg['training']
+current_lr = learning_rate
+kill_epoch = cfg['training'].get('kill_epoch_at', None)
 while True:
     epoch_it += 1
+
     #     scheduler.step()
+    if is_linear_decay and epoch_it in cfg['training'][
+            'learning_rage_decay_at']:
+        print('Decay learning rate from {} to {}'.format(
+            current_lr, current_lr / 10))
+        current_lr /= 10
+        optimizer = torch.optim.Adam(model.parameters(), lr=current_lr)
+        checkpoint_io.module_dict['optimizer'] = optimizer
+        trainer.optimizer = optimizer
 
     for batch in train_loader:
         it += 1
@@ -225,7 +237,8 @@ while True:
                                    loss_val_best=metric_val_best)
 
         # Exit if necessary
-        if exit_after > 0 and (time.time() - t0) >= exit_after:
+        if exit_after > 0 and (time.time() -
+                               t0) >= exit_after or kill_epoch < epoch_it:
             print('Time limit reached. Exiting.')
             checkpoint_io.save('model.pt',
                                epoch_it=epoch_it,
