diff --git a/configs/default.yaml b/configs/default.yaml
index 556a071..52c7f35 100644
--- a/configs/default.yaml
+++ b/configs/default.yaml
@@ -8,6 +8,7 @@ data:
   val_split: val
   test_split: test
   dim: 3
+  sdf_points_file: sdf_points.npz
   points_file: points.npz
   points_iou_file: points.npz
   points_subsample: 1024
@@ -42,6 +43,7 @@ training:
   out_dir:  out/default
   batch_size: 64
   val_batch_size: 64
+  vis_batch_size: 5
   print_every: 10
   visualize_every: 2000
   checkpoint_every: 1000
@@ -58,6 +60,8 @@ test:
   threshold: 0.5
   eval_mesh: true
   eval_pointcloud: true
+  eval_fscore: true
+  fscore_thresholds: [0.005, 0.01, 0.0107337006427915, 0.02, 0.05, 0.1, 0.2]
   model_file: model_best.pt
 generation:
   batch_size: 100000
diff --git a/configs/img/debug_pnet.yaml b/configs/img/debug_pnet.yaml
index 6b8c891..54766e0 100644
--- a/configs/img/debug_pnet.yaml
+++ b/configs/img/debug_pnet.yaml
@@ -21,8 +21,8 @@ model:
 training:
   out_dir:  out/img/test
   batch_size: 10
-  visualize_every: 500
-  validate_every: 100
+  visualize_every: 100
+  validate_every: 1000
   checkpoint_every: 10000
   wandb_resume: null
   skip_load_pretrained_optimizer: true
diff --git a/configs/img/onet_legacy_pretrained.yaml b/configs/img/onet_legacy_pretrained.yaml
index a74775e..fa9772b 100644
--- a/configs/img/onet_legacy_pretrained.yaml
+++ b/configs/img/onet_legacy_pretrained.yaml
@@ -1,11 +1,11 @@
-inherit_from: configs/img/onet.yaml
+inherit_from: configs/img/onet_original.yaml
 data:
   img_augment: true
 model:
   decoder_kwargs:
     legacy: true
 training:
-  out_dir:  out/img/onet_legacy
+  out_dir:  out/submission/eval/img/onet_legacy
 test:
   model_file: https://s3.eu-central-1.amazonaws.com/avg-projects/occupancy_networks/models/onet_img2mesh-0c7780d1.pt
 generation:
diff --git a/configs/img/onet_pretrained.yaml b/configs/img/onet_pretrained.yaml
index 2585eef..9135757 100644
--- a/configs/img/onet_pretrained.yaml
+++ b/configs/img/onet_pretrained.yaml
@@ -1,5 +1,7 @@
-inherit_from: configs/img/onet.yaml
+inherit_from: configs/img/onet_original.yaml
 test:
   model_file: https://s3.eu-central-1.amazonaws.com/avg-projects/occupancy_networks/models/onet_img2mesh_3-f786b04a.pt
+training:
+  out_dir:  out/submission/eval/img/onet_pretrained
 generation:
   generation_dir: pretrained
diff --git a/configs/img/pnet_finetue_only_transition_cceff10.yaml b/configs/img/pnet_finetue_only_transition_cceff10.yaml
index c9a8d9f..527f6a5 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10.yaml
@@ -31,6 +31,7 @@ model:
     is_feature_radius: false
     is_feature_coord: true
     is_feature_angles: false
+    layer_depth: 0
     disable_learn_pose_but_transition: true
 trainer:
   overlap_reg_coef: 1
@@ -53,6 +54,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: bve2kmpq
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
index 10fd5ed..3455101 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg.yaml
@@ -5,7 +5,7 @@ data:
   img_size: 224 
   points_subsample: 2048 
   pointcloud_target_n: 4096
-  primitive_points_sample_n: 20 # will be powered by dim - 1, if dim == 3, then 900
+  primitive_points_sample_n: 12 # will be powered by dim - 1, if dim == 3, then 900
   is_normal_icosahedron: false
   icosahedron_subdiv: 2
   icosahedron_uv_margin: 0.00001
@@ -45,7 +45,7 @@ trainer:
 training:
   out_dir:  out/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg
   batch_size: 20
-  val_batch_size: 2
+  val_batch_size: 1
   model_selection_metric: iou
   model_selection_mode: maximize
   visualize_every: 1000
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml
index ec3e6bd..dcd8812 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: 7o949ydv
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml
index 1c635a8..d7e1eb6 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_normal_transition_feature.yaml
@@ -57,6 +57,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: bvovwjdm
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml
index d612575..faa0696 100644
--- a/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml
+++ b/configs/img/pnet_finetue_only_transition_cceff10_pn30_target_n_4096_no_overlap_reg_sphere.yaml
@@ -54,6 +54,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: false
   learning_rate: 1e-4
+  wandb_resume: g0d0yzjc
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml b/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml
index b0d390f..6ac9b38 100644
--- a/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml
+++ b/configs/img/pnet_oceff10_dense_normal_loss_pointcloud_n_4096_finetune_only_transition.yaml
@@ -53,6 +53,7 @@ training:
   checkpoint_every: 10000
   skip_load_pretrained_optimizer: true
   learning_rate: 1e-4
+  wandb_resume: e5w6ge0h
 test:
   threshold: 0.5
   eval_mesh: true
diff --git a/data/.gitkeep b/data/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/data/metadata.yaml b/data/metadata.yaml
deleted file mode 100644
index 1b06258..0000000
--- a/data/metadata.yaml
+++ /dev/null
@@ -1,54 +0,0 @@
-{
-    "04256520": {
-        "id": "04256520",
-        "name": "sofa,couch,lounge"
-    },
-    "02691156": {
-        "id": "02691156",
-        "name": "airplane,aeroplane,plane"
-    },
-    "03636649": {
-        "id": "03636649",
-        "name": "lamp"
-    },
-    "04401088": {
-        "id": "04401088",
-        "name": "telephone,phone,telephone set"
-    },
-    "04530566": {
-        "id": "04530566",
-        "name": "vessel,watercraft"
-    },
-    "03691459": {
-        "id": "03691459",
-        "name": "loudspeaker,speaker,speaker unit,loudspeaker system,speaker system"
-    },
-    "03001627": {
-        "id": "03001627",
-        "name": "chair"
-    },
-    "02933112": {
-        "id": "02933112",
-        "name": "cabinet"
-    },
-    "04379243": {
-        "id": "04379243",
-        "name": "table"
-    },
-    "03211117": {
-        "id": "03211117",
-        "name": "display,video display"
-    },
-    "02958343": {
-        "id": "02958343",
-        "name": "car,auto,automobile,machine,motorcar"
-    },
-    "02828884": {
-        "id": "02828884",
-        "name": "bench"
-    },
-    "04090263": {
-        "id": "04090263",
-        "name": "rifle"
-    }
-}
diff --git a/dist_train.py b/dist_train.py
index fc82025..d5a82df 100755
--- a/dist_train.py
+++ b/dist_train.py
@@ -75,7 +75,7 @@ def train(gpu_idx, args):
     if rank == 0:
         val_loader = torch.utils.data.DataLoader(
             val_dataset,
-            batch_size=2,
+            batch_size=cfg['training']['val_batch_size'],
             num_workers=4,
             shuffle=False,
             collate_fn=data.collate_remove_none,
@@ -84,7 +84,7 @@ def train(gpu_idx, args):
         # For visualizations
         vis_loader = torch.utils.data.DataLoader(
             val_dataset,
-            batch_size=12,
+            batch_size=cfg['training']['vis_batch_size'],
             shuffle=False,
             collate_fn=data.collate_remove_none,
             worker_init_fn=data.worker_init_fn)
diff --git a/eval.py b/eval.py
index 07c4f36..a17d80f 100644
--- a/eval.py
+++ b/eval.py
@@ -1,27 +1,83 @@
 import argparse
 import os
+import subprocess
+import hashlib
 import pandas as pd
 import torch
 import numpy as np
+from datetime import datetime
 from tqdm import tqdm
 from im2mesh import config, data
 from im2mesh.checkpoints import CheckpointIO
+import shutil
+import yaml
+from collections import OrderedDict
 
 
-parser = argparse.ArgumentParser(
-    description='Evaluate mesh algorithms.'
-)
+def represent_odict(dumper, instance):
+    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
+
+
+yaml.add_representer(OrderedDict, represent_odict)
+
+
+def construct_odict(loader, node):
+    return OrderedDict(loader.construct_pairs(node))
+
+
+yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
+
+parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
 
 # Get configuration and basic arguments
-args = parser.parse_args()
+args, unknown_args = parser.parse_known_args()
 cfg = config.load_config(args.config, 'configs/default.yaml')
+for idx, arg in enumerate(unknown_args):
+    if arg.startswith('--'):
+        arg = arg.replace('--', '')
+        value = unknown_args[idx + 1]
+        keys = arg.split('.')
+        if keys[0] not in cfg:
+            cfg[keys[0]] = {}
+        child_cfg = cfg.get(keys[0], {})
+        for key in keys[1:]:
+            item = child_cfg.get(key, None)
+            if isinstance(item, dict):
+                child_cfg = item
+            elif item is None:
+                child_cfg[key] = value
+            else:
+                child_cfg[key] = type(item)(value)
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
 # Shorthands
-out_dir = cfg['training']['out_dir']
+if '--dontcopy' in unknown_args:
+    out_dir = cfg['training']['out_dir']
+else:
+    base_out_dir = cfg['training']['out_dir']
+    out_dir = os.path.join(
+        os.path.dirname(base_out_dir).replace('out', 'out/submission/eval'),
+        os.path.basename(base_out_dir)) + '_' + datetime.now().strftime(
+            ('%Y%m%d_%H%M%S'))
+print('out dir for eval: ', out_dir)
+if not '--dontcopy' in unknown_args:
+    if not os.path.exists(out_dir):
+        shutil.copytree(base_out_dir, out_dir)
+    else:
+        raise ValueError('out dir already exists')
+
+if not '--dontcopy' in unknown_args:
+    patch_path = os.path.join(out_dir, 'diff.patch')
+    subprocess.run('git diff > {}'.format(patch_path), shell=True)
+    weight_path = os.path.join(out_dir, cfg['test']['model_file'])
+    with open(weight_path, 'rb') as f:
+        md5 = hashlib.md5(f.read()).hexdigest()
+    cfg['test']['model_file_hash'] = md5
+    yaml.dump(cfg, open(os.path.join(out_dir, 'config.yaml'), 'w'))
+
 out_file = os.path.join(out_dir, 'eval_full.pkl')
 out_file_class = os.path.join(out_dir, 'eval.csv')
 
@@ -47,14 +103,13 @@ print('Total number of parameters: %d' % nparameters)
 # Evaluate
 model.eval()
 
-eval_dicts = []   
+eval_dicts = []
 print('Evaluating networks...')
-
-
-test_loader = torch.utils.data.DataLoader(
-    dataset, batch_size=1, shuffle=False,
-    collate_fn=data.collate_remove_none,
-    worker_init_fn=data.worker_init_fn)
+test_loader = torch.utils.data.DataLoader(dataset,
+                                          batch_size=1,
+                                          shuffle=False,
+                                          collate_fn=data.collate_remove_none,
+                                          worker_init_fn=data.worker_init_fn)
 
 # Handle each dataset separately
 for it, data in enumerate(tqdm(test_loader)):
@@ -68,7 +123,7 @@ for it, data in enumerate(tqdm(test_loader)):
         model_dict = dataset.get_model_dict(idx)
     except AttributeError:
         model_dict = {'model': str(idx), 'category': 'n/a'}
-    
+
     modelname = model_dict['model']
     category_id = model_dict['category']
 
@@ -81,13 +136,12 @@ for it, data in enumerate(tqdm(test_loader)):
         'idx': idx,
         'class id': category_id,
         'class name': category_name,
-        'modelname':modelname,
+        'modelname': modelname,
     }
     eval_dicts.append(eval_dict)
     eval_data = trainer.eval_step(data)
     eval_dict.update(eval_data)
 
-
 # Create pandas dataframe and save
 eval_df = pd.DataFrame(eval_dicts)
 eval_df.set_index(['idx'], inplace=True)
@@ -99,4 +153,4 @@ eval_df_class.to_csv(out_file_class)
 
 # Print results
 eval_df_class.loc['mean'] = eval_df_class.mean()
-print(eval_df_class)
\ No newline at end of file
+print(eval_df_class)
diff --git a/eval_meshes.py b/eval_meshes.py
index cba5ce0..44e5d88 100644
--- a/eval_meshes.py
+++ b/eval_meshes.py
@@ -5,17 +5,16 @@ from tqdm import tqdm
 import pandas as pd
 import trimesh
 import torch
+from torch.utils import data as torch_data
 from im2mesh import config, data
 from im2mesh.eval import MeshEvaluator
 from im2mesh.utils.io import load_pointcloud
-
-
-parser = argparse.ArgumentParser(
-    description='Evaluate mesh algorithms.'
-)
+import numpy as np
+parser = argparse.ArgumentParser(description='Evaluate mesh algorithms.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
-parser.add_argument('--eval_input', action='store_true',
+parser.add_argument('--eval_input',
+                    action='store_true',
                     help='Evaluate inputs instead.')
 
 args = parser.parse_args()
@@ -23,24 +22,33 @@ cfg = config.load_config(args.config, 'configs/default.yaml')
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
+is_eval_explicit_mesh = cfg['test'].get('is_eval_explicit_mesh', False)
+
 # Shorthands
-out_dir = cfg['training']['out_dir']
+out_dir = os.path.dirname(args.config)
+#out_dir = cfg['training']['out_dir']
 generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
 if not args.eval_input:
-    out_file = os.path.join(generation_dir, 'eval_meshes_full.pkl')
-    out_file_class = os.path.join(generation_dir, 'eval_meshes.csv')
+    out_file = os.path.join(
+        generation_dir, 'eval_meshes_full{}.pkl'.format(
+            '_explicit' if is_eval_explicit_mesh else ''))
+    out_file_class = os.path.join(
+        generation_dir, 'eval_meshes{}.csv'.format(
+            '_explicit' if is_eval_explicit_mesh else ''))
 else:
-    out_file = os.path.join(generation_dir, 'eval_input_full.pkl')
-    out_file_class = os.path.join(generation_dir, 'eval_input.csv')
+    out_file = os.path.join(
+        generation_dir, 'eval_input_full{}.pkl'.format(
+            '_explicit' if is_eval_explicit_mesh else ''))
+    out_file_class = os.path.join(
+        generation_dir, 'eval_input{}.csv'.format(
+            '_explicit' if is_eval_explicit_mesh else ''))
 
 # Dataset
 points_field = data.PointsField(
-    cfg['data']['points_iou_file'], 
+    cfg['data']['points_iou_file'],
     unpackbits=cfg['data']['points_unpackbits'],
 )
-pointcloud_field = data.PointCloudField(
-    cfg['data']['pointcloud_chamfer_file']
-)
+pointcloud_field = data.PointCloudField(cfg['data']['pointcloud_chamfer_file'])
 fields = {
     'points_iou': points_field,
     'pointcloud_chamfer': pointcloud_field,
@@ -50,17 +58,23 @@ fields = {
 print('Test split: ', cfg['data']['test_split'])
 
 dataset_folder = cfg['data']['path']
-dataset = data.Shapes3dDataset(
-    dataset_folder, fields,
-    cfg['data']['test_split'],
-    categories=cfg['data']['classes'])
+dataset = data.Shapes3dDataset(dataset_folder,
+                               fields,
+                               cfg['data']['test_split'],
+                               categories=cfg['data']['classes'])
+
+if 'debug' in cfg['data']:
+    dataset = torch_data.Subset(dataset,
+                                range(cfg['data']['debug']['sample_n']))
 
 # Evaluator
 evaluator = MeshEvaluator(n_points=100000)
 
 # Loader
-test_loader = torch.utils.data.DataLoader(
-    dataset, batch_size=1, num_workers=0, shuffle=False)
+test_loader = torch.utils.data.DataLoader(dataset,
+                                          batch_size=1,
+                                          num_workers=0,
+                                          shuffle=False)
 
 # Evaluate all classes
 eval_dicts = []
@@ -85,7 +99,7 @@ for it, data in enumerate(tqdm(test_loader)):
         model_dict = dataset.get_model_dict(idx)
     except AttributeError:
         model_dict = {'model': str(idx), 'category': 'n/a'}
-    
+
     modelname = model_dict['model']
     category_id = model_dict['category']
 
@@ -118,10 +132,24 @@ for it, data in enumerate(tqdm(test_loader)):
     if cfg['test']['eval_mesh']:
         mesh_file = os.path.join(mesh_dir, '%s.off' % modelname)
 
+        if is_eval_explicit_mesh:
+            visbility_file = os.path.join(
+                mesh_dir, '%s_vertex_visbility.npz' % modelname)
         if os.path.exists(mesh_file):
             mesh = trimesh.load(mesh_file, process=False)
+            if is_eval_explicit_mesh:
+                vertex_visibility = np.load(
+                    visbility_file)['vertex_visibility']
+            else:
+                vertex_visibility = None
             eval_dict_mesh = evaluator.eval_mesh(
-                mesh, pointcloud_tgt, normals_tgt, points_tgt, occ_tgt)
+                mesh,
+                pointcloud_tgt,
+                normals_tgt,
+                points_tgt,
+                occ_tgt,
+                is_eval_explicit_mesh=is_eval_explicit_mesh,
+                vertex_visibility=vertex_visibility)
             for k, v in eval_dict_mesh.items():
                 eval_dict[k + ' (mesh)'] = v
         else:
@@ -129,18 +157,16 @@ for it, data in enumerate(tqdm(test_loader)):
 
     # Evaluate point cloud
     if cfg['test']['eval_pointcloud']:
-        pointcloud_file = os.path.join(
-            pointcloud_dir, '%s.ply' % modelname)
+        pointcloud_file = os.path.join(pointcloud_dir, '%s.ply' % modelname)
 
         if os.path.exists(pointcloud_file):
             pointcloud = load_pointcloud(pointcloud_file)
-            eval_dict_pcl = evaluator.eval_pointcloud(
-                pointcloud, pointcloud_tgt)
+            eval_dict_pcl = evaluator.eval_pointcloud(pointcloud,
+                                                      pointcloud_tgt)
             for k, v in eval_dict_pcl.items():
                 eval_dict[k + ' (pcl)'] = v
         else:
-            print('Warning: pointcloud does not exist: %s'
-                    % pointcloud_file)
+            print('Warning: pointcloud does not exist: %s' % pointcloud_file)
 
 # Create pandas dataframe and save
 eval_df = pd.DataFrame(eval_dicts)
diff --git a/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py b/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py
index 16a8f4a..935495f 100644
--- a/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py
+++ b/external/periodic_shapes/periodic_shapes/layers/super_shape_functions.py
@@ -9,6 +9,7 @@ def polar2cartesian(radius, angles):
         r: radius (B, N, P, 1 or 2) last dim is one in 2D mode, two in 3D.
         angles: angle (B, 1, P, 1 or 2) 
     """
+    print('radius angles in s2c mean', radius[..., -1].mean(), angles.mean())
     dim = radius.shape[-1]
     dim2 = angles.shape[-1]
     P = radius.shape[-2]
@@ -41,6 +42,7 @@ def sphere2cartesian(radius, angles):
         r: radius (B, N, P, 1 or 2) last dim is one in 2D mode, two in 3D.
         angles: angle (B, 1, P, 1 or 2) 
     """
+    print('radius angles in s2c mean', radius[..., 0].mean(), angles.mean())
     dim = radius.shape[-1]
     dim2 = angles.shape[-1]
     P = radius.shape[-2]
@@ -74,16 +76,18 @@ def cartesian2sphere(coord):
     x = coord[..., 0]
     y = coord[..., 1]
     z = torch.zeros([1], device=coord.device) if dim == 2 else coord[..., 2]
-    x_non_zero = torch.where(x == 0, x.sign() * EPS, x)
+    x_non_zero = torch.where(x == 0, x + EPS, x)
     theta = torch.atan2(y, x_non_zero)
 
     assert not torch.isnan(theta).any(), (theta)
-    r = (coord**2).sum(-1).sqrt()
+    r = (coord**2).sum(-1).clamp(min=EPS).sqrt()
+    assert not torch.isnan(r).any(), (r)
 
-    xysq = (x_non_zero**2 + y**2).sqrt()
+    xysq_non_zero = (x_non_zero**2 + y**2).clamp(min=EPS).sqrt()
     #xysq_non_zero = torch.where(xysq == 0, EPS + xysq, xysq)
-    xysq_non_zero = xysq.clamp(min=EPS)
-    phi = torch.atan(z / xysq_non_zero)
+    #xysq_non_zero = xysq.clamp(min=EPS)
+    #phi = torch.atan(z / xysq_non_zero)
+    phi = torch.atan((xysq_non_zero / z.clamp(min=EPS)).clamp(min=EPS))
 
     assert not torch.isnan(phi).any(), (phi)
 
diff --git a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
index 5530413..d41d3c3 100644
--- a/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
+++ b/external/periodic_shapes/periodic_shapes/models/base_shape_sampler.py
@@ -112,7 +112,6 @@ class BaseShapeSampler(nn.Module):
                 scaled_cartesian_coord, rotation)
         else:
             rotated_cartesian_coord = scaled_cartesian_coord
-        assert not torch.isnan(rotated_cartesian_coord).any()
         posed_cartesian_coord = rotated_cartesian_coord + transition.view(
             B, self.n_primitives, 1, self.dim)
         assert not torch.isnan(posed_cartesian_coord).any()
@@ -222,6 +221,42 @@ class BaseShapeSampler(nn.Module):
         # (B, N, P)
         return r1, r2, theta, phi
 
+    def cartesian2sphere(self, coord, params, *args, **kwargs):
+        """Convert polar coordinate to cartesian coordinate.
+        Args:
+            coord: (B, N, P, D)
+        """
+        dim = coord.shape[-1]
+        B, _, P, dim = coord.shape
+        x = coord[..., 0]
+        y = coord[..., 1]
+        z = torch.zeros([1], device=coord.device) if dim == 2 else coord[...,
+                                                                         2]
+        x_non_zero = torch.where(x == 0, x + EPS, x)
+        theta = torch.atan2(y, x_non_zero)
+
+        assert not torch.isnan(theta).any(), (theta)
+        #r = (coord**2).sum(-1).clamp(min=EPS).sqrt()
+        r = self.get_r_check_shape(theta.view(B, self.n_primitives, P, 1),
+                                   params, *args, **kwargs)[..., 0]
+        print('r in c2p', r.mean())
+        assert not torch.isnan(r).any(), (r)
+
+        xysq_non_zero = (x_non_zero**2 + y**2).clamp(min=EPS).sqrt()
+        #xysq_non_zero = torch.where(xysq == 0, EPS + xysq, xysq)
+        #xysq_non_zero = xysq.clamp(min=EPS)
+        #phi = torch.atan(z / xysq_non_zero)
+        phi = torch.atan2(z, xysq_non_zero)
+        #phi = torch.atan(xysq_non_zero / z)
+        #phi = torch.acos((z / r_phi))
+
+        assert not torch.isnan(phi).any(), (phi)
+
+        # (B, N, P)
+        return r.unsqueeze(-1).expand([*coord.shape[:-1],
+                                       dim - 1]), torch.stack([theta, phi],
+                                                              axis=-1)
+
     def extract_super_shapes_surface_point(self, super_shape_point,
                                            primitive_params, *args, **kwargs):
         """Extract surface point for visualziation purpose"""
diff --git a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
index 9b599de..6a23afd 100644
--- a/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
+++ b/external/periodic_shapes/periodic_shapes/models/periodic_shape_sampler.py
@@ -19,15 +19,16 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
                  act='leaky',
                  decoder_class='PrimitiveWiseGroupConvDecoder',
                  is_shape_sampler_sphere=False,
+                 spherical_angles=False,
                  no_encoder=False,
                  is_feature_angles=True,
                  is_feature_coord=True,
                  is_feature_radius=True,
                  no_last_bias=False,
+                 return_sdf=False,
                  **kwargs):
         super().__init__(*args, **kwargs)
         self.clamp = True
-        self.spherical_angles = False
         self.factor = factor
         self.num_points = num_points
         self.num_labels = 1  # Only infer r2 for 3D
@@ -37,6 +38,8 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
         self.act = act
         self.no_encoder = no_encoder
         self.is_shape_sampler_sphere = is_shape_sampler_sphere
+        self.spherical_angles = spherical_angles
+        self.return_sdf = return_sdf
 
         c64 = 64 // self.factor
         self.encoder_dim = c64 * 2
@@ -80,7 +83,7 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
             coord = super_shape_functions.sphere2cartesian(r, thetas_reshaped)
         else:
             coord = super_shape_functions.polar2cartesian(r, thetas_reshaped)
-
+        assert not torch.isnan(coord).any()
         # posed_coord = B, n_primitives, P, dim
         if self.learn_pose:
             posed_coord = self.project_primitive_to_world(
@@ -88,11 +91,20 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
         else:
             posed_coord = coord
 
+        print('')
+        print('get pr from points')
         periodic_net_r = self.get_periodic_net_r(thetas.unsqueeze(1), points,
                                                  r[..., -1], posed_coord)
 
         final_r = r.clone()
-        final_r[..., -1] = r[..., -1] + periodic_net_r.squeeze(-1)
+        if self.is_shape_sampler_sphere and self.spherical_angles:
+            print('mean r1 in points', r[..., 0].mean())
+            final_r[..., 0] = r[..., 0] + periodic_net_r.squeeze(-1)
+            print('mean final r in points', final_r[..., 0].mean())
+        else:
+            print('mean r1 in points', r[..., -1].mean())
+            final_r[..., -1] = r[..., -1] + periodic_net_r.squeeze(-1)
+            print('mean final r in points', final_r[..., -1].mean())
 
         if self.clamp:
             final_r = final_r.clamp(min=EPS)
@@ -125,6 +137,7 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
 
     def get_periodic_net_r(self, thetas, points, radius, coord):
         # B, 1 or N, P, dim - 1
+        print('mean coord in pr', coord.mean())
         assert len(thetas.shape) == 4, thetas.shape
         assert thetas.shape[-1] == self.dim - 1
         assert points.shape[0] == thetas.shape[0]
@@ -148,6 +161,7 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
         radius = self.decoder(encoded, thetas, radius, coord)
         radius = radius * self.last_scale
 
+        print('mean from pr ', radius.mean())
         return radius
 
     def get_indicator(self,
@@ -185,16 +199,26 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
         if self.learn_pose:
             posed_coord = self.project_primitive_to_world(posed_coord, params)
 
+        print('get pr from sgn')
         rp = self.get_periodic_net_r(angles, points, radius, posed_coord)
 
+        print('mean r1 in sgn', r1.mean())
         numerator = (coord**2).sum(-1)
         if self.is_shape_sampler_sphere:
             r1 = r1 + rp.squeeze(-1)
+            print('mean final r in sgn', r1.mean())
             if self.clamp:
                 r1 = r1.clamp(min=EPS)
+                nep = numerator.clamp(min=EPS)
             else:
                 r1 = nn.functional.relu(r1) + EPS
-            indicator = 1 - numerator.clamp(min=EPS).sqrt() / r1
+                nep = (numerator + EPS)
+            if self.return_sdf:
+                dist = nep.sqrt() - r1
+                indicator = dist.sign() * dist**2
+            else:
+                indicator = 1 - numerator.clamp(min=EPS).sqrt() / r1
+
         else:
             if is3d:
                 r2 = r2 + rp.squeeze(-1)
@@ -211,18 +235,28 @@ class PeriodicShapeSampler(sphere_sampler.SphereSampler):
             if self.clamp:
                 denominator = ((r1**2) * (r2**2) * (phi.cos()**2) + (r2**2) *
                                (phi.sin()**2)).clamp(min=EPS)
-                indicator = 1. - (numerator /
-                                  denominator).clamp(min=EPS).sqrt()
             else:
                 denominator = ((r1**2) * (r2**2) * (phi.cos()**2) + (r2**2) *
                                (phi.sin()**2)) + EPS
-                indicator = 1. - (numerator / denominator + EPS).sqrt()
+            if self.return_sdf:
+                if self.clamp:
+                    nep = numerator.clamp(min=EPS)
+                else:
+                    nep = (numerator + EPS)
+                dist = nep.sqrt() - denominator.sqrt()
+                indicator = (dist).sign() * dist**2
+            else:
+                if self.clamp:
+                    indicator = 1. - (numerator /
+                                      denominator).clamp(min=EPS).sqrt()
+                else:
+                    indicator = 1. - (numerator / denominator + EPS).sqrt()
 
         return indicator
 
     def get_sgn(self, coord, params, *args, **kwargs):
         if self.is_shape_sampler_sphere and self.spherical_angles:
-            r, angles = super_shape_functions.cartesian2sphere(coord)
+            r, angles = self.cartesian2sphere(coord, params, *args, **kwargs)
             r1 = r[..., 0]
             r2 = r[..., 1]
             theta = angles[..., 0]
diff --git a/external/periodic_shapes/test_polar2cart.py b/external/periodic_shapes/test_polar2cart.py
index 0fb3f5e..8efd25d 100644
--- a/external/periodic_shapes/test_polar2cart.py
+++ b/external/periodic_shapes/test_polar2cart.py
@@ -10,8 +10,8 @@ print(radius2.shape, angles2.shape)
 coord2 = super_shape_functions.sphere2cartesian(radius2, angles2)
 print(coord)
 print(coord2)
-assert torch.all(torch.eq(radius.mean(), radius2.mean()))
 assert torch.allclose(coord, coord2), (coord - coord2)
+assert torch.all(torch.eq(radius.mean(), radius2.mean()))
 assert torch.allclose(
     angles[..., 0].sin(),
     angles2[...,
diff --git a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
index 1896ca9..4a46eae 100644
--- a/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
+++ b/external/periodic_shapes/tests/unittest/models/test_periodic_shape_sampler.py
@@ -780,10 +780,11 @@ def test_decoder_consistency_OtherDecoders():
         decoder_class='PrimitiveWiseGroupConvDecoder',
         #decoder_class='BatchNormDecoderSharedWeight',
         #decoder_class='PrimitiveWiseGroupConvDecoderLegacy',
-        last_scale=.1,
+        last_scale=10,
         no_encoder=True,
         is_shape_sampler_sphere=True,
-        is_feature_coord=False,
+        spherical_angles=True,
+        is_feature_coord=True,
         is_feature_angles=False,
         is_feature_radius=False,
         dim=dim)
@@ -832,4 +833,91 @@ def test_decoder_consistency_OtherDecoders():
                               atol=1e-5), (sgn.max(), sgn.min(), sgn.median(),
                                            sgn.mean())
 
-    assert False
+
+def test_decoder_sdf_consistency_OtherDecoders():
+    batch = 3
+    m = 3
+    n = 1
+    n1 = 1
+    n2 = 10
+    n3 = 3
+    a = 1
+    b = 1
+    theta = math.pi / 2.
+    sample_num = 200
+    points_num = 5
+    P = 10
+
+    dim = 3
+
+    rotations = [[0., 0., 0.]] * n
+    #rotations = [[0., math.pi / 2, 0.]] * n
+    #rotations = [[0., math.pi / 2., math.pi / 2.],
+    #             [0., math.pi / 2., math.pi / 4.], [0., math.pi, math.pi / 2.]]
+    transitions = [[0., 0., 0.]] * n
+    #transitions = [[0., 1., 0.], [0., 1., 10.], [0., 1., 1.]]
+    linear_scales = [[1., 1., 1.]] * n
+    #linear_scales = [[1., 1., 1.], [0.2, 0.8, 1.2], [1., 1.2, 2.2]]
+
+    sampler = periodic_shape_sampler.PeriodicShapeSampler(
+        points_num,
+        m,
+        n,
+        #decoder_class='MLPDecoder',
+        #decoder_class='BatchNormDecoder',
+        decoder_class='PrimitiveWiseGroupConvDecoder',
+        #decoder_class='BatchNormDecoderSharedWeight',
+        #decoder_class='PrimitiveWiseGroupConvDecoderLegacy',
+        last_scale=10,
+        no_encoder=True,
+        is_shape_sampler_sphere=True,
+        spherical_angles=False,
+        is_feature_coord=True,
+        is_feature_angles=False,
+        is_feature_radius=False,
+        return_sdf=False,
+        dim=dim)
+    preset_params = utils.generate_multiple_primitive_params(
+        m,
+        n1,
+        n2,
+        n3,
+        a,
+        b,
+        rotations_angle=rotations,
+        transitions=transitions,
+        linear_scales=linear_scales,
+        nn=n,
+        batch=batch)
+
+    batched_theta_test_tensor = utils.sample_spherical_angles(
+        sample_num=P, batch=batch, sgn_convertible=True, dim=dim)
+
+    batched_points = torch.rand([batch, points_num]).float()
+
+    # B, N, P
+    radius = sampler.transform_circumference_angle_to_super_shape_radius(
+        batched_theta_test_tensor, preset_params, points=batched_points)
+    # B, P, dim
+    coords = sampler.transform_circumference_angle_to_super_shape_world_cartesian_coord(
+        batched_theta_test_tensor,
+        radius,
+        preset_params,
+        points=batched_points)
+
+    print('coord mean main', coords.mean(), (coords**2).sum(-1).sqrt().mean())
+
+    print('check coord')
+    sgn = sampler.transform_world_cartesian_coord_to_tsd(coords.view(
+        batch, -1, dim),
+                                                         preset_params,
+                                                         points=batched_points)
+    print('check coord done')
+    for idx in range(n):
+        coord = coords[:, idx, :, :]
+        sgn = sampler.transform_world_cartesian_coord_to_tsd(
+            coord, preset_params, points=batched_points)[:, idx, :]
+        print(sgn.shape)
+        assert torch.allclose(sgn, torch.zeros_like(sgn),
+                              atol=1e-5), (sgn.max(), sgn.min(), sgn.median(),
+                                           sgn.mean())
diff --git a/generate.py b/generate.py
index c21a7b0..bbf4e8d 100644
--- a/generate.py
+++ b/generate.py
@@ -5,31 +5,109 @@ import shutil
 import argparse
 from tqdm import tqdm
 import time
-from collections import defaultdict
+from collections import defaultdict, OrderedDict
 import pandas as pd
 from im2mesh import config
 from im2mesh.checkpoints import CheckpointIO
 from im2mesh.utils.io import export_pointcloud
 from im2mesh.utils.visualize import visualize_data
 from im2mesh.utils.voxels import VoxelGrid
+import numpy as np
+import hashlib
+import subprocess
+import yaml
+from datetime import datetime
+import subprocess
 
 
+def represent_odict(dumper, instance):
+    return dumper.represent_mapping('tag:yaml.org,2002:map', instance.items())
+
+
+def construct_odict(loader, node):
+    return OrderedDict(loader.construct_pairs(node))
+
+
+yaml.add_representer(OrderedDict, represent_odict)
+yaml.add_constructor('tag:yaml.org,2002:map', construct_odict)
+
 parser = argparse.ArgumentParser(
-    description='Extract meshes from occupancy process.'
-)
+    description='Extract meshes from occupancy process.')
 parser.add_argument('config', type=str, help='Path to config file.')
 parser.add_argument('--no-cuda', action='store_true', help='Do not use cuda.')
+parser.add_argument(
+    '--explicit',
+    action='store_true',
+    help=
+    'to generate mesh with explicit rep, run: python3 generate.py --explicit --data.is_normal_icosahedron true --data.icosahedron_subdiv 4'
+)
+parser.add_argument('--unique_name',
+                    default='',
+                    type=str,
+                    help='String name for generation.')
+
+args, unknown_args = parser.parse_known_args()
 
-args = parser.parse_args()
 cfg = config.load_config(args.config, 'configs/default.yaml')
+
+for idx, arg in enumerate(unknown_args):
+    if arg.startswith('--'):
+        arg = arg.replace('--', '')
+        value = unknown_args[idx + 1]
+        keys = arg.split('.')
+        if keys[0] not in cfg:
+            cfg[keys[0]] = {}
+        child_cfg = cfg.get(keys[0], {})
+        for key in keys[1:]:
+            item = child_cfg.get(key, None)
+            if isinstance(item, dict):
+                child_cfg = item
+            elif item is None:
+                if value == 'true':
+                    value = True
+                if value == 'false':
+                    value = False
+                if value == 'null':
+                    value = None
+                if isinstance(value, str) and value.isdigit():
+                    value = float(value)
+                child_cfg[key] = value
+            else:
+                child_cfg[key] = type(item)(value)
+date_str = datetime.now().strftime(('%Y%m%d_%H%M%S'))
+if args.explicit:
+    assert cfg['data'].get('is_normal_icosahedron', False) or cfg['data'].get(
+        'is_normal_uv_sphere', False)
+    cfg['generation']['is_explicit_mesh'] = True
+    cfg['test']['is_eval_explicit_mesh'] = True
+if args.explicit:
+    cfg['generation']['generation_dir'] += '_explicit'
+cfg['generation']['generation_dir'] += ('_' + date_str)
+
 is_cuda = (torch.cuda.is_available() and not args.no_cuda)
 device = torch.device("cuda" if is_cuda else "cpu")
 
-out_dir = cfg['training']['out_dir']
+out_dir = os.path.dirname(args.config)
+
 generation_dir = os.path.join(out_dir, cfg['generation']['generation_dir'])
+if not os.path.exists(generation_dir):
+    os.makedirs(generation_dir)
 out_time_file = os.path.join(generation_dir, 'time_generation_full.pkl')
 out_time_file_class = os.path.join(generation_dir, 'time_generation.pkl')
 
+patch_path = os.path.join(generation_dir, 'gen_diff.patch')
+subprocess.run('git diff > {}'.format(patch_path), shell=True)
+weight_path = os.path.join(out_dir, cfg['test']['model_file'])
+with open(weight_path, 'rb') as f:
+    md5 = hashlib.md5(f.read()).hexdigest()
+cfg['test']['model_file_hash'] = md5
+yaml.dump(
+    cfg,
+    open(
+        os.path.join(
+            out_dir, 'gen_config_{}_{}.yaml'.format(args.unique_name,
+                                                    date_str)), 'w'))
+
 batch_size = cfg['generation']['batch_size']
 input_type = cfg['data']['input_type']
 vis_n_outputs = cfg['generation']['vis_n_outputs']
@@ -60,10 +138,11 @@ if generate_pointcloud and not hasattr(generator, 'generate_pointcloud'):
     generate_pointcloud = False
     print('Warning: generator does not support pointcloud generation.')
 
-
 # Loader
-test_loader = torch.utils.data.DataLoader(
-    dataset, batch_size=1, num_workers=0, shuffle=False)
+test_loader = torch.utils.data.DataLoader(dataset,
+                                          batch_size=1,
+                                          num_workers=0,
+                                          shuffle=False)
 
 # Statistics
 time_dicts = []
@@ -79,7 +158,10 @@ for it, data in enumerate(tqdm(test_loader)):
     mesh_dir = os.path.join(generation_dir, 'meshes')
     pointcloud_dir = os.path.join(generation_dir, 'pointcloud')
     in_dir = os.path.join(generation_dir, 'input')
-    generation_vis_dir = os.path.join(generation_dir, 'vis', )
+    generation_vis_dir = os.path.join(
+        generation_dir,
+        'vis',
+    )
 
     # Get index etc.
     idx = data['idx'].item()
@@ -88,7 +170,7 @@ for it, data in enumerate(tqdm(test_loader)):
         model_dict = dataset.get_model_dict(idx)
     except AttributeError:
         model_dict = {'model': str(idx), 'category': 'n/a'}
-    
+
     modelname = model_dict['model']
     category_id = model_dict.get('category', 'n/a')
 
@@ -120,7 +202,7 @@ for it, data in enumerate(tqdm(test_loader)):
 
     if not os.path.exists(in_dir):
         os.makedirs(in_dir)
-    
+
     # Timing dict
     time_dict = {
         'idx': idx,
@@ -135,9 +217,8 @@ for it, data in enumerate(tqdm(test_loader)):
 
     # Also copy ground truth
     if cfg['generation']['copy_groundtruth']:
-        modelpath = os.path.join(
-            dataset.dataset_folder, category_id, modelname, 
-            cfg['data']['watertight_file'])
+        modelpath = os.path.join(dataset.dataset_folder, category_id,
+                                 modelname, cfg['data']['watertight_file'])
         out_file_dict['gt'] = modelpath
 
     if generate_mesh:
@@ -156,13 +237,19 @@ for it, data in enumerate(tqdm(test_loader)):
         mesh_out_file = os.path.join(mesh_dir, '%s.off' % modelname)
         mesh.export(mesh_out_file)
         out_file_dict['mesh'] = mesh_out_file
+        if cfg['generation'].get('is_explicit_mesh', False):
+            visibility = mesh.vertex_attributes['vertex_visibility']
+            visibility_out_file = os.path.join(
+                mesh_dir, '%s_vertex_visbility.npz' % modelname)
+            np.savez(visibility_out_file, vertex_visibility=visibility)
+            out_file_dict['vertex_visibility'] = visibility_out_file
 
     if generate_pointcloud:
         t0 = time.time()
         pointcloud = generator.generate_pointcloud(data)
         time_dict['pcl'] = time.time() - t0
-        pointcloud_out_file = os.path.join(
-            pointcloud_dir, '%s.ply' % modelname)
+        pointcloud_out_file = os.path.join(pointcloud_dir,
+                                           '%s.ply' % modelname)
         export_pointcloud(pointcloud, pointcloud_out_file)
         out_file_dict['pointcloud'] = pointcloud_out_file
 
@@ -192,8 +279,8 @@ for it, data in enumerate(tqdm(test_loader)):
         img_name = '%02d.off' % c_it
         for k, filepath in out_file_dict.items():
             ext = os.path.splitext(filepath)[1]
-            out_file = os.path.join(generation_vis_dir, '%02d_%s%s'
-                                    % (c_it, k, ext))
+            out_file = os.path.join(generation_vis_dir,
+                                    '%02d_%s%s' % (c_it, k, ext))
             shutil.copyfile(filepath, out_file)
 
     model_counter[category_id] += 1
diff --git a/im2mesh/checkpoints.py b/im2mesh/checkpoints.py
index 332600c..75ff6ab 100644
--- a/im2mesh/checkpoints.py
+++ b/im2mesh/checkpoints.py
@@ -112,7 +112,7 @@ class CheckpointIO(object):
                         new_pretrained_dict.keys())
                     print('ignored parameters')
                     for key in diff:
-                        print(key)
+                        print(key, pretrained_dict[key].shape)
                     pretrained_dict_new_param = {
                         key: val
                         for key, val in model_dict.items()
@@ -120,7 +120,7 @@ class CheckpointIO(object):
                     }
                     print('new parameters')
                     for key in pretrained_dict_new_param:
-                        print(key)
+                        print(key, pretrained_dict_new_param[key].shape)
                     new_pretrained_dict.update(pretrained_dict_new_param)
                     v.load_state_dict(new_pretrained_dict)
                 else:
diff --git a/im2mesh/config.py b/im2mesh/config.py
index 007aca6..00e3fc4 100644
--- a/im2mesh/config.py
+++ b/im2mesh/config.py
@@ -1,7 +1,7 @@
 import yaml
 from torchvision import transforms
 from im2mesh import data
-from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet
+from im2mesh import onet, r2n2, psgn, pix2mesh, dmc, pnet, atlasnetv2
 from im2mesh import preprocess
 from torch.utils import data as torch_data
 
@@ -12,6 +12,7 @@ method_dict = {
     'pix2mesh': pix2mesh,
     'dmc': dmc,
     'pnet': pnet,
+    'atlasnetv2': atlasnetv2
 }
 
 
@@ -229,6 +230,8 @@ def get_inputs_field(mode, cfg):
         inputs_field = data.VoxelsField(cfg['data']['voxels_file'])
     elif input_type == 'idx':
         inputs_field = data.IndexField()
+    elif input_type == 'raw_id':
+        inputs_field = data.RawIDField()
     else:
         raise ValueError('Invalid input type (%s)' % input_type)
     return inputs_field
diff --git a/im2mesh/data/__init__.py b/im2mesh/data/__init__.py
index 4f0aab1..870a44f 100644
--- a/im2mesh/data/__init__.py
+++ b/im2mesh/data/__init__.py
@@ -4,11 +4,11 @@ from im2mesh.data.core import (
 )
 from im2mesh.data.fields import (
     IndexField, CategoryField, ImagesField, PointsField,
-    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField
+    VoxelsField, PointCloudField, MeshField, SphericalCoordinateField, RawIDField, SDFPointsField, PlanarPatchField
 )
 from im2mesh.data.transforms import (
     PointcloudNoise, SubsamplePointcloud,
-    SubsamplePoints
+    SubsamplePoints, SubsampleSDFPoints
 )
 from im2mesh.data.real import (
     KittiDataset, OnlineProductDataset,
@@ -24,12 +24,15 @@ __all__ = [
     # Fields
     IndexField,
     CategoryField,
+    RawIDField,
+    SDFPointsField,
     ImagesField,
     PointsField,
     VoxelsField,
     PointCloudField,
     MeshField,
     SphericalCoordinateField,
+    PlanarPatchField,
     # Transforms
     PointcloudNoise,
     SubsamplePointcloud,
diff --git a/im2mesh/data/core.py b/im2mesh/data/core.py
index 96d5c86..a224c72 100644
--- a/im2mesh/data/core.py
+++ b/im2mesh/data/core.py
@@ -3,8 +3,7 @@ import logging
 from torch.utils import data
 import numpy as np
 import yaml
-
-
+import traceback
 logger = logging.getLogger(__name__)
 
 
@@ -12,7 +11,6 @@ logger = logging.getLogger(__name__)
 class Field(object):
     ''' Data fields class.
     '''
-
     def load(self, data_path, idx, category):
         ''' Loads a data point.
 
@@ -35,9 +33,13 @@ class Field(object):
 class Shapes3dDataset(data.Dataset):
     ''' 3D Shapes dataset class.
     '''
-
-    def __init__(self, dataset_folder, fields, split=None,
-                 categories=None, no_except=True, transform=None):
+    def __init__(self,
+                 dataset_folder,
+                 fields,
+                 split=None,
+                 categories=None,
+                 no_except=True,
+                 transform=None):
         ''' Initialization of the the 3D shape dataset.
 
         Args:
@@ -57,8 +59,10 @@ class Shapes3dDataset(data.Dataset):
         # If categories is None, use all subfolders
         if categories is None:
             categories = os.listdir(dataset_folder)
-            categories = [c for c in categories
-                          if os.path.isdir(os.path.join(dataset_folder, c))]
+            categories = [
+                c for c in categories
+                if os.path.isdir(os.path.join(dataset_folder, c))
+            ]
 
         # Read metadata file
         metadata_file = os.path.join(dataset_folder, 'metadata.yaml')
@@ -67,10 +71,8 @@ class Shapes3dDataset(data.Dataset):
             with open(metadata_file, 'r') as f:
                 self.metadata = yaml.load(f)
         else:
-            self.metadata = {
-                c: {'id': c, 'name': 'n/a'} for c in categories
-            } 
-        
+            self.metadata = {c: {'id': c, 'name': 'n/a'} for c in categories}
+
         # Set index
         for c_idx, c in enumerate(categories):
             self.metadata[c]['idx'] = c_idx
@@ -85,11 +87,8 @@ class Shapes3dDataset(data.Dataset):
             split_file = os.path.join(subpath, split + '.lst')
             with open(split_file, 'r') as f:
                 models_c = f.read().split('\n')
-            
-            self.models += [
-                {'category': c, 'model': m}
-                for m in models_c
-            ]
+
+            self.models += [{'category': c, 'model': m} for m in models_c]
 
     def __len__(self):
         ''' Returns the length of the dataset.
@@ -113,11 +112,11 @@ class Shapes3dDataset(data.Dataset):
             try:
                 field_data = field.load(model_path, idx, c_idx)
             except Exception:
+                traceback.print_exc()
                 if self.no_except:
                     logger.warn(
-                        'Error occured when loading field %s of model %s'
-                        % (field_name, model)
-                    )
+                        'Error occured when loading field %s of model %s' %
+                        (field_name, model))
                     return None
                 else:
                     raise
@@ -149,8 +148,8 @@ class Shapes3dDataset(data.Dataset):
         files = os.listdir(model_path)
         for field_name, field in self.fields.items():
             if not field.check_complete(files):
-                logger.warn('Field "%s" is incomplete: %s'
-                            % (field_name, model_path))
+                logger.warn('Field "%s" is incomplete: %s' %
+                            (field_name, model_path))
                 return False
 
         return True
diff --git a/im2mesh/data/fields.py b/im2mesh/data/fields.py
index 71e5e7b..64f1ae8 100644
--- a/im2mesh/data/fields.py
+++ b/im2mesh/data/fields.py
@@ -7,6 +7,7 @@ import trimesh
 from im2mesh.data.core import Field
 from im2mesh.utils import binvox_rw
 from periodic_shapes import utils
+from im2mesh.atlasnetv2 import utils as atv2_utils
 import torch
 
 
@@ -173,7 +174,7 @@ class PointsField(Field):
             'occ': occupancies,
         }
 
-        if self.with_transforms:
+        if self.with_transforms and 'loc' in data and 'scale' in data:
             data['loc'] = points_dict['loc'].astype(np.float32)
             data['scale'] = points_dict['scale'].astype(np.float32)
 
@@ -262,7 +263,7 @@ class PointCloudField(Field):
             'normals': normals,
         }
 
-        if self.with_transforms:
+        if self.with_transforms and 'loc' in data and 'scale' in data:
             data['loc'] = pointcloud_dict['loc'].astype(np.float32)
             data['scale'] = pointcloud_dict['scale'].astype(np.float32)
 
@@ -342,8 +343,10 @@ class SphericalCoordinateField(Field):
                  primitive_points_sample_n,
                  mode,
                  is_normal_icosahedron=False,
+                 is_normal_uv_sphere=False,
                  icosahedron_subdiv=2,
                  icosahedron_uv_margin=1e-5,
+                 icosahedron_uv_margin_phi=1e-5,
                  uv_sphere_length=20,
                  normal_mesh_no_invert=False,
                  *args,
@@ -351,6 +354,9 @@ class SphericalCoordinateField(Field):
         self.primitive_points_sample_n = primitive_points_sample_n
         self.mode = mode
         self.is_normal_icosahedron = is_normal_icosahedron
+        self.is_normal_uv_sphere = is_normal_uv_sphere
+        self.icosahedron_uv_margin = icosahedron_uv_margin
+        self.icosahedron_uv_margin_phi = icosahedron_uv_margin_phi
 
         if self.is_normal_icosahedron:
             icosamesh = trimesh.creation.icosphere(
@@ -371,6 +377,27 @@ class SphericalCoordinateField(Field):
             self.angles_for_nomal = torch.stack([uv_th, uv_ph], axis=-1)
             self.face_for_normal = torch.from_numpy(icosamesh.faces)
 
+        elif self.is_normal_uv_sphere:
+            thetas = utils.sample_spherical_angles(
+                batch=1,
+                sample_num=uv_sphere_length,
+                sampling='grid',
+                device='cpu',
+                dim=3,
+                sgn_convertible=True,
+                phi_margin=icosahedron_uv_margin_phi,
+                theta_margin=icosahedron_uv_margin)
+            mesh = trimesh.creation.uv_sphere(
+                theta=np.linspace(0, np.pi, uv_sphere_length),
+                phi=np.linspace(-np.pi, np.pi, uv_sphere_length))
+            #thetas = torch.where(thetas.abs() < icosahedron_uv_margin_phi,
+            #                     torch.tensor([icosahedron_uv_margin_phi]),
+            #                     thetas)
+            if not normal_mesh_no_invert:
+                mesh.invert()
+            self.angles_for_nomal = thetas[0]
+            self.face_for_normal = torch.from_numpy(mesh.faces)
+
     def load(self, model_path, idx, category):
         ''' Sample spherical coordinate.
 
@@ -386,11 +413,11 @@ class SphericalCoordinateField(Field):
             device='cpu',
             dim=3,  #sgn_convertible=True, phi_margin=1e-5, theta_margin=1e-5)
             sgn_convertible=True,
-            phi_margin=1e-5,
-            theta_margin=1e-5).squeeze(0)
+            phi_margin=self.icosahedron_uv_margin_phi,
+            theta_margin=self.icosahedron_uv_margin).squeeze(0)
 
         data = {None: angles}
-        if self.is_normal_icosahedron:
+        if self.is_normal_icosahedron or self.is_normal_uv_sphere:
             data.update({
                 'normal_angles': self.angles_for_nomal.clone(),
                 'normal_face': self.face_for_normal.clone()
@@ -403,3 +430,132 @@ class SphericalCoordinateField(Field):
         Returns: True
         '''
         return True
+
+
+class RawIDField(Field):
+    ''' Basic index field.'''
+    def load(self, model_path, idx, category):
+        ''' Loads the index field.
+
+        Args:
+            model_path (str): path to model
+            idx (int): ID of data point
+            category (int): index of category
+        '''
+        category_id, object_id = model_path.split('/')[-2:]
+        data = {'category': category_id, 'object': object_id}
+        return data
+
+    def check_complete(self, files):
+        ''' Check if field is complete.
+        
+        Args:
+            files: files
+        '''
+        return True
+
+
+# 3D Fields
+class SDFPointsField(Field):
+    ''' Point Field.
+
+    It provides the field to load point data. This is used for the points
+    randomly sampled in the bounding volume of the 3D shape.
+
+    Args:
+        file_name (str): file name
+        transform (list): list of transformations which will be applied to the
+            points tensor
+        with_transforms (bool): whether scaling and rotation data should be
+            provided
+
+    '''
+    def __init__(self, file_name, transform=None, with_transforms=False):
+        self.file_name = file_name
+        self.transform = transform
+        self.with_transforms = with_transforms
+
+    def load(self, model_path, idx, category):
+        ''' Loads the data point.
+
+        Args:
+            model_path (str): path to model
+            idx (int): ID of data point
+            category (int): index of category
+        '''
+        file_path = os.path.join(model_path, self.file_name)
+
+        points_dict = np.load(file_path)
+        points = points_dict['points']
+        # Break symmetry if given in float16:
+        if points.dtype == np.float16:
+            points = points.astype(np.float32)
+            points += 1e-4 * np.random.randn(*points.shape)
+        else:
+            points = points.astype(np.float32)
+
+        occupancies = points_dict['distances']
+        occupancies = occupancies.astype(np.float32)
+
+        data = {
+            None: points,
+            'distances': occupancies,
+        }
+
+        if self.with_transforms:
+            raise ValueError('data for transform not stored')
+
+        if self.transform is not None:
+            data = self.transform(data)
+
+        return data
+
+
+class PlanarPatchField(Field):
+    ''' Angle field class.
+
+    It provides the class used for spherical coordinate data.
+
+    Args:
+        file_name (str): file name
+        transform (list): list of transformations applied to data points
+    '''
+    def __init__(self,
+                 mode,
+                 patch_side_length=20,
+                 is_generate_mesh=False,
+                 *args,
+                 **kwargs):
+        self.mode = mode
+        self.patch_side_length = patch_side_length
+        self.is_generate_mesh = is_generate_mesh
+
+        if self.is_generate_mesh:
+            vertices, faces = atv2_utils.create_planar_mesh(patch_side_length)
+
+            self.vertices = torch.from_numpy(vertices)
+            self.faces = torch.from_numpy(faces)
+
+    def load(self, model_path, idx, category):
+        ''' Sample spherical coordinate.
+
+        Args:
+            model_path (str): path to model
+            idx (int): ID of data point
+            category (int): index of category
+        '''
+        plane_points = utils.generate_grid_samples(
+            [0, 1],
+            batch=1,
+            sample_num=self.patch_side_length,
+            sampling='uniform',
+            device='cpu',
+            dim=2)
+
+        data = {None: plane_points}
+        if self.is_generate_mesh:
+            data.update({
+                'mesh_vertices': self.vertices.clone(),
+                'mesh_faces': self.faces.clone()
+            })
+        return data
diff --git a/im2mesh/data/transforms.py b/im2mesh/data/transforms.py
index 26fec98..b471e37 100644
--- a/im2mesh/data/transforms.py
+++ b/im2mesh/data/transforms.py
@@ -10,7 +10,6 @@ class PointcloudNoise(object):
     Args:
         stddev (int): standard deviation
     '''
-
     def __init__(self, stddev):
         self.stddev = stddev
 
@@ -81,7 +80,7 @@ class SubsamplePoints(object):
             idx = np.random.randint(points.shape[0], size=self.N)
             data_out.update({
                 None: points[idx, :],
-                'occ':  occ[idx],
+                'occ': occ[idx],
             })
         else:
             Nt_out, Nt_in = self.N
@@ -109,3 +108,58 @@ class SubsamplePoints(object):
                 'volume': volume,
             })
         return data_out
+
+
+class SubsampleSDFPoints(object):
+    ''' Points subsampling transformation class.
+
+    It subsamples the points data.
+
+    Args:
+        N (int): number of points to be subsampled
+    '''
+    def __init__(self, N):
+        self.N = N
+
+    def __call__(self, data):
+        ''' Calls the transformation.
+
+        Args:
+            data (dictionary): data dictionary
+        '''
+        points = data[None]
+        occ = data['distances']
+
+        data_out = data.copy()
+        if isinstance(self.N, int):
+            idx = np.random.randint(points.shape[0], size=self.N)
+            data_out.update({
+                None: points[idx, :],
+                'distances': occ[idx],
+            })
+        else:
+            Nt_out, Nt_in = self.N
+            occ_binary = (occ >= 0.5)
+            points0 = points[~occ_binary]
+            points1 = points[occ_binary]
+
+            idx0 = np.random.randint(points0.shape[0], size=Nt_out)
+            idx1 = np.random.randint(points1.shape[0], size=Nt_in)
+
+            points0 = points0[idx0, :]
+            points1 = points1[idx1, :]
+            points = np.concatenate([points0, points1], axis=0)
+
+            occ0 = np.zeros(Nt_out, dtype=np.float32)
+            occ1 = np.ones(Nt_in, dtype=np.float32)
+            occ = np.concatenate([occ0, occ1], axis=0)
+
+            volume = occ_binary.sum() / len(occ_binary)
+            volume = volume.astype(np.float32)
+
+            data_out.update({
+                None: points,
+                'distances': occ,
+                'volume': volume,
+            })
+        return data_out
diff --git a/im2mesh/eval.py b/im2mesh/eval.py
index 02f6885..91052f8 100644
--- a/im2mesh/eval.py
+++ b/im2mesh/eval.py
@@ -1,12 +1,16 @@
 # from im2mesh import icp
 import logging
+import random
 import numpy as np
 import trimesh
 # from scipy.spatial import cKDTree
 from im2mesh.utils.libkdtree import KDTree
 from im2mesh.utils.libmesh import check_mesh_contains
 from im2mesh.common import compute_iou
-
+from pykeops.torch import LazyTensor
+import kaolin as kal
+import torch
+import warnings
 
 # Maximum values for bounding box [-0.5, 0.5]^3
 EMPTY_PCL_DICT = {
@@ -34,12 +38,17 @@ class MeshEvaluator(object):
     Args:
         n_points (int): number of points to be used for evaluation
     '''
-
     def __init__(self, n_points=100000):
         self.n_points = n_points
 
-    def eval_mesh(self, mesh, pointcloud_tgt, normals_tgt,
-                  points_iou, occ_tgt):
+    def eval_mesh(self,
+                  mesh,
+                  pointcloud_tgt,
+                  normals_tgt,
+                  points_iou,
+                  occ_tgt,
+                  is_eval_explicit_mesh=False,
+                  vertex_visibility=None):
         ''' Evaluates a mesh.
 
         Args:
@@ -49,16 +58,42 @@ class MeshEvaluator(object):
             points_iou (numpy_array): points tensor for IoU evaluation
             occ_tgt (numpy_array): GT occupancy values for IoU points
         '''
-        if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
-            pointcloud, idx = mesh.sample(self.n_points, return_index=True)
+        if is_eval_explicit_mesh:
+            assert vertex_visibility is not None
+            sampled_vertex_idx = np.zeros_like(vertex_visibility).astype(
+                np.bool)
+            pointcloud = mesh.vertices[vertex_visibility, :]
+            normals = mesh.vertex_normals[vertex_visibility, :]
             pointcloud = pointcloud.astype(np.float32)
-            normals = mesh.face_normals[idx]
+            normals = normals.astype(np.float32)
+
+            if pointcloud.shape[0] > self.n_points:
+                select_idx = random.sample(range(pointcloud.shape[0]),
+                                           self.n_points)
+                pointcloud = pointcloud[select_idx:]
+            if normals.shape[0] > self.n_points:
+                select_idx = random.sample(range(normals.shape[0]),
+                                           self.n_points)
+                normals = normals[select_idx:]
+            if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
+                select_idx = random.sample(range(pointcloud.shape[0]),
+                                           pointcloud.shape[0])
+                pointcloud_tgt = pointcloud_tgt[select_idx, :]
+            if normals_tgt.shape[0] > normals.shape[0]:
+                select_idx = random.sample(range(normals.shape[0]),
+                                           pointcloud.shape[0])
+                normals_tgt = normals_tgt[select_idx, :]
         else:
-            pointcloud = np.empty((0, 3))
-            normals = np.empty((0, 3))
+            if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
+                pointcloud, idx = mesh.sample(self.n_points, return_index=True)
+                pointcloud = pointcloud.astype(np.float32)
+                normals = mesh.face_normals[idx]
+            else:
+                pointcloud = np.empty((0, 3))
+                normals = np.empty((0, 3))
 
-        out_dict = self.eval_pointcloud(
-            pointcloud, pointcloud_tgt, normals, normals_tgt)
+        out_dict = self.eval_pointcloud(pointcloud, pointcloud_tgt, normals,
+                                        normals_tgt)
 
         if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
             occ = check_mesh_contains(mesh, points_iou)
@@ -68,8 +103,11 @@ class MeshEvaluator(object):
 
         return out_dict
 
-    def eval_pointcloud(self, pointcloud, pointcloud_tgt,
-                        normals=None, normals_tgt=None):
+    def eval_pointcloud(self,
+                        pointcloud,
+                        pointcloud_tgt,
+                        normals=None,
+                        normals_tgt=None):
         ''' Evaluates a point cloud.
 
         Args:
@@ -92,8 +130,7 @@ class MeshEvaluator(object):
         # Completeness: how far are the points of the target point cloud
         # from thre predicted point cloud
         completeness, completeness_normals = distance_p2p(
-            pointcloud_tgt, normals_tgt, pointcloud, normals
-        )
+            pointcloud_tgt, normals_tgt, pointcloud, normals)
         completeness2 = completeness**2
 
         completeness = completeness.mean()
@@ -102,9 +139,8 @@ class MeshEvaluator(object):
 
         # Accuracy: how far are th points of the predicted pointcloud
         # from the target pointcloud
-        accuracy, accuracy_normals = distance_p2p(
-            pointcloud, normals, pointcloud_tgt, normals_tgt
-        )
+        accuracy, accuracy_normals = distance_p2p(pointcloud, normals,
+                                                  pointcloud_tgt, normals_tgt)
         accuracy2 = accuracy**2
 
         accuracy = accuracy.mean()
@@ -113,9 +149,8 @@ class MeshEvaluator(object):
 
         # Chamfer distance
         chamferL2 = 0.5 * (completeness2 + accuracy2)
-        normals_correctness = (
-            0.5 * completeness_normals + 0.5 * accuracy_normals
-        )
+        normals_correctness = (0.5 * completeness_normals +
+                               0.5 * accuracy_normals)
         chamferL1 = 0.5 * (completeness + accuracy)
 
         out_dict = {
@@ -132,6 +167,83 @@ class MeshEvaluator(object):
 
         return out_dict
 
+    def eval_fscore_from_mesh(self,
+                              mesh,
+                              pointcloud_tgt,
+                              thresholds,
+                              is_eval_explicit_mesh=False,
+                              vertex_visibility=None):
+        ''' Evaluates a mesh.
+
+        Args:
+            mesh (trimesh): mesh which should be evaluated
+            pointcloud_tgt (numpy array): target point cloud
+            normals_tgt (numpy array): target normals
+            points_iou (numpy_array): points tensor for IoU evaluation
+            occ_tgt (numpy_array): GT occupancy values for IoU points
+        '''
+
+        if is_eval_explicit_mesh:
+            assert vertex_visibility is not None
+            pointcloud = mesh.vertices[vertex_visibility, :]
+            pointcloud = pointcloud.astype(np.float32)
+
+            if pointcloud.shape[0] > self.n_points:
+                select_idx = random.sample(range(pointcloud.shape[0]),
+                                           self.n_points)
+                pointcloud = pointcloud[select_idx:]
+
+            if pointcloud_tgt.shape[0] > pointcloud.shape[0]:
+                select_idx = random.sample(range(pointcloud.shape[0]),
+                                           pointcloud.shape[0])
+                pointcloud_tgt = pointcloud_tgt[select_idx, :]
+
+        else:
+            if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
+                pointcloud, idx = mesh.sample(self.n_points, return_index=True)
+                pointcloud = pointcloud.astype(np.float32)
+            else:
+                pointcloud = np.empty((0, 3))
+        """
+        if len(mesh.vertices) != 0 and len(mesh.faces) != 0:
+            pointcloud = mesh.sample(self.n_points, return_index=False)
+            pointcloud = pointcloud.astype(np.float32)
+        else:
+            pointcloud = np.empty((0, 3))
+        """
+
+        out_dict = fscore(pointcloud[np.newaxis, ...],
+                          pointcloud_tgt[np.newaxis, ...],
+                          thresholds=thresholds,
+                          mode='pykeops')
+        if out_dict is None:
+            return out_dict
+        else:
+            out_dict = {
+                k: v[0].item()
+                for k, v in out_dict.items() if v is not None
+            }
+
+        return out_dict
+
+    def eval_fscore_from_mesh_batch(self, pointcloud_pred, pointcloud_tgt,
+                                    thresholds):
+        ''' Evaluates a mesh.
+
+        Args:
+            mesh (trimesh): mesh which should be evaluated
+            pointcloud_tgt (numpy array): target point cloud
+            normals_tgt (numpy array): target normals
+            points_iou (numpy_array): points tensor for IoU evaluation
+            occ_tgt (numpy_array): GT occupancy values for IoU points
+        '''
+        out_dict = fscore(pointcloud_pred,
+                          pointcloud_tgt,
+                          thresholds=thresholds,
+                          mode='pykeops')
+
+        return out_dict
+
 
 def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
     ''' Computes minimal distances of each point in points_src to points_tgt.
@@ -156,8 +268,8 @@ def distance_p2p(points_src, normals_src, points_tgt, normals_tgt):
         # (mostly due to mehtod not caring about this in generation)
         normals_dot_product = np.abs(normals_dot_product)
     else:
-        normals_dot_product = np.array(
-            [np.nan] * points_src.shape[0], dtype=np.float32)
+        normals_dot_product = np.array([np.nan] * points_src.shape[0],
+                                       dtype=np.float32)
     return dist, normals_dot_product
 
 
@@ -171,3 +283,77 @@ def distance_p2m(points, mesh):
     '''
     _, dist, _ = trimesh.proximity.closest_point(mesh, points)
     return dist
+
+
+def chamfer_distance(pred, target, pykeops=True):
+    assert pykeops
+    # B, P, 1, dim
+    pred_lazy = LazyTensor(pred.unsqueeze(2))
+    # B, 1, P2, dim
+    target_lazy = LazyTensor(target.unsqueeze(1))
+
+    # B, P, P2, dim
+    dist = (pred_lazy - target_lazy).norm2()
+
+    # B, P, dim
+    pred2target = dist.min(2).squeeze(-1)
+
+    # B, P2, dim
+    target2pred = dist.min(1).squeeze(-1)
+
+    return pred2target, target2pred
+
+
+def fscore(pred_points, target_points, thresholds=[0.01], mode='pykeops'):
+    assert mode in ['kaolin', 'pykeops']
+    assert isinstance(thresholds, list)
+
+    if isinstance(pred_points, np.ndarray):
+        pred_points = torch.from_numpy(pred_points).to('cuda')
+    if isinstance(target_points, np.ndarray):
+        target_points = torch.from_numpy(target_points).to('cuda')
+    assert pred_points.ndim == 3 and target_points.ndim == 3
+    assert len(pred_points.shape) == 3 and len(
+        target_points.shape) == 3, (pred_points.shape, target_points.shape,
+                                    len(pred_points.shape),
+                                    len(target_points.shape))
+
+    try:
+        assert pred_points.shape[1] == target_points.shape[
+            1], pred_points.shape[1] == target_points.shape[1]
+    except:
+        warnings.warn('point shapes are not same!')
+        return {}
+
+    f_scores = {}
+    if mode == 'kaolin':
+        assert False
+        for threshold in thresholds:
+            b_f = []
+            for idx in range(target_points.shape[0]):
+                t = target_points[idx]
+                p = pred_points[idx]
+                f_score = kal.metrics.f_score(t, p, radius=threshold)
+                b_f.append(f_score)
+            f_scores['fscore_th={}'.format(threshold)] = torch.stack(
+                b_f, 0).detach().to('cpu')
+
+    elif mode == 'pykeops':
+
+        gt_distances, pred_distances = chamfer_distance(
+            pred_points, target_points)
+
+        for threshold in thresholds:
+            fn = (pred_distances > threshold).sum(-1).float()
+            fp = (gt_distances > threshold).sum(-1).float()
+            tp = (gt_distances <= threshold).sum(-1).float()
+
+            precision = tp / (tp + fp)
+            recall = tp / (tp + fn)
+
+            f_score = 2 * (precision * recall) / (precision + recall + 1e-8)
+            f_scores['fscore_th={}'.format(threshold)] = f_score.detach().to(
+                'cpu')
+    else:
+        raise NotImplementedError
+    return f_scores
diff --git a/im2mesh/pnet/config.py b/im2mesh/pnet/config.py
index 169bcc6..73fc786 100644
--- a/im2mesh/pnet/config.py
+++ b/im2mesh/pnet/config.py
@@ -26,6 +26,10 @@ def get_model(cfg, device=None, dataset=None, **kwargs):
     encoder_kwargs = cfg['model']['encoder_kwargs']
     encoder_latent_kwargs = cfg['model']['encoder_latent_kwargs']
 
+    decoder_kwargs['return_sdf'] = cfg['trainer'].get('is_sdf', False)
+    decoder_kwargs['is_radius_reg'] = cfg['trainer'].get(
+        'is_radius_reg', False)
+
     decoder = models.decoder_dict[decoder](dim=dim,
                                            z_dim=z_dim,
                                            c_dim=c_dim,
@@ -100,6 +104,9 @@ def get_generator(model, cfg, device, **kwargs):
         refinement_step=cfg['generation']['refinement_step'],
         simplify_nfaces=cfg['generation']['simplify_nfaces'],
         preprocessor=preprocessor,
+        pnet_point_scale=cfg['trainer']['pnet_point_scale'],
+        is_explicit_mesh=cfg['generation'].get('is_explicit_mesh', False),
+        **cfg['generation'].get('mesh_kwargs', {}),
     )
     return generator
 
@@ -126,6 +133,8 @@ def get_data_fields(mode, cfg):
         cfg (dict): imported yaml config
     '''
     points_transform = data.SubsamplePoints(cfg['data']['points_subsample'])
+    if cfg.get('sdf_generation', False):
+        points_transform = None
     with_transforms = cfg['model']['use_camera']
 
     fields = {}
@@ -136,8 +145,18 @@ def get_data_fields(mode, cfg):
         unpackbits=cfg['data']['points_unpackbits'],
     )
 
+    if not cfg.get('sdf_generation', False):
+        sdf_points_transform = data.SubsampleSDFPoints(
+            cfg['data']['points_subsample'])
+        fields['sdf_points'] = data.SDFPointsField(
+            cfg['data']['sdf_points_file'],
+            sdf_points_transform,
+            with_transforms=with_transforms)
+
     pointcloud_transform = data.SubsamplePointcloud(
         cfg['data']['pointcloud_target_n'])
+    if cfg.get('sdf_generation', False):
+        pointcloud_transform = None
 
     fields['pointcloud'] = data.PointCloudField(cfg['data']['pointcloud_file'],
                                                 pointcloud_transform,
@@ -146,8 +165,11 @@ def get_data_fields(mode, cfg):
         cfg['data']['primitive_points_sample_n'],
         mode,
         is_normal_icosahedron=cfg['data'].get('is_normal_icosahedron', False),
+        is_normal_uv_sphere=cfg['data'].get('is_normal_uv_sphere', False),
         icosahedron_subdiv=cfg['data'].get('icosahedron_subdiv', 2),
         icosahedron_uv_margin=cfg['data'].get('icosahedron_uv_margin', 1e-5),
+        icosahedron_uv_margin_phi=cfg['data'].get('icosahedron_uv_margin_phi',
+                                                  1e-5),
         uv_sphere_length=cfg['data'].get('uv_sphere_length', 20),
         normal_mesh_no_invert=cfg['data'].get('normal_mesh_no_invert', False))
     if mode in ('val', 'test'):
diff --git a/im2mesh/pnet/generation.py b/im2mesh/pnet/generation.py
index 49786de..e2b5bd7 100644
--- a/im2mesh/pnet/generation.py
+++ b/im2mesh/pnet/generation.py
@@ -8,6 +8,7 @@ from im2mesh.utils import libmcubes
 from im2mesh.common import make_3d_grid
 from im2mesh.utils.libsimplify import simplify_mesh
 from im2mesh.utils.libmise import MISE
+from periodic_shapes.models import model_utils
 import time
 
 
@@ -30,13 +31,22 @@ class Generator3D(object):
         simplify_nfaces (int): number of faces the mesh should be simplified to
         preprocessor (nn.Module): preprocessor for inputs
     '''
-
-    def __init__(self, model, points_batch_size=100000,
-                 threshold=0.5, refinement_step=0, device=None,
-                 resolution0=16, upsampling_steps=3,
-                 with_normals=False, padding=0.1, sample=False,
+    def __init__(self,
+                 model,
+                 points_batch_size=100000,
+                 threshold=0.5,
+                 refinement_step=0,
+                 device=None,
+                 resolution0=16,
+                 upsampling_steps=3,
+                 with_normals=False,
+                 padding=0.1,
+                 sample=False,
                  simplify_nfaces=None,
-                 preprocessor=None):
+                 preprocessor=None,
+                 pnet_point_scale=6,
+                 is_explicit_mesh=False,
+                 **kwargs):
         self.model = model.to(device)
         self.points_batch_size = points_batch_size
         self.refinement_step = refinement_step
@@ -49,6 +59,8 @@ class Generator3D(object):
         self.sample = sample
         self.simplify_nfaces = simplify_nfaces
         self.preprocessor = preprocessor
+        self.pnet_point_scale = pnet_point_scale
+        self.is_explicit_mesh = is_explicit_mesh
 
     def generate_mesh(self, data, return_stats=True):
         ''' Generates the output mesh.
@@ -77,15 +89,24 @@ class Generator3D(object):
             c = self.model.encode_inputs(inputs)
         stats_dict['time (encode inputs)'] = time.time() - t0
 
-        z = self.model.get_z_from_prior((1,), sample=self.sample).to(device)
-        mesh = self.generate_from_latent(z, c, stats_dict=stats_dict, **kwargs)
+        z = self.model.get_z_from_prior((1, ), sample=self.sample).to(device)
+        mesh = self.generate_from_latent(z,
+                                         c,
+                                         stats_dict=stats_dict,
+                                         data=data,
+                                         **kwargs)
 
         if return_stats:
             return mesh, stats_dict
         else:
             return mesh
 
-    def generate_from_latent(self, z, c=None, stats_dict={}, **kwargs):
+    def generate_from_latent(self,
+                             z,
+                             c=None,
+                             stats_dict={},
+                             data=None,
+                             **kwargs):
         ''' Generates mesh from latent.
 
         Args:
@@ -93,48 +114,88 @@ class Generator3D(object):
             c (tensor): latent conditioned code c
             stats_dict (dict): stats dictionary
         '''
-        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
+        assert data is not None
 
-        t0 = time.time()
-        # Compute bounding box size
-        box_size = 1 + self.padding
+        if self.is_explicit_mesh:
+            normal_faces = data.get('angles.normal_face').to(self.device)
+            normal_angles = data.get('angles.normal_angles').to(self.device)
+            t0 = time.time()
+            output = self.model.decode(None,
+                                       z,
+                                       c,
+                                       angles=normal_angles,
+                                       **kwargs)
+            normal_vertices, normal_mask, _, _, _ = output
+            stats_dict['time (eval points)'] = time.time() - t0
 
-        # Shortcut
-        if self.upsampling_steps == 0:
-            nx = self.resolution0
-            pointsf = box_size * make_3d_grid(
-                (-0.5,)*3, (0.5,)*3, (nx,)*3
-            )
-            values = self.eval_points(pointsf, z, c, **kwargs).cpu().numpy()
-            value_grid = values.reshape(nx, nx, nx)
-        else:
-            mesh_extractor = MISE(
-                self.resolution0, self.upsampling_steps, threshold)
-
-            points = mesh_extractor.query()
-
-            while points.shape[0] != 0:
-                # Query points
-                pointsf = torch.FloatTensor(points).to(self.device)
-                # Normalize to bounding box
-                pointsf = pointsf / mesh_extractor.resolution
-                pointsf = box_size * (pointsf - 0.5)
-                # Evaluate model and update
-                values = self.eval_points(
-                    pointsf, z, c, **kwargs).cpu().numpy()
-                values = values.astype(np.float64)
-                mesh_extractor.update(points, values)
-                points = mesh_extractor.query()
+            t0 = time.time()
+            B, N, P, D = normal_vertices.shape
+            normal_faces_all = torch.cat([(normal_faces + idx * P)
+                                          for idx in range(N)],
+                                         axis=1)
+            assert B == 1
+            mem_t = time.time()
+            verts = normal_vertices.view(
+                N * P, D).to('cpu').detach().numpy() / self.pnet_point_scale
+            faces = normal_faces_all.view(-1, 3).to('cpu').detach().numpy()
+            visbility = (normal_mask > 0.5).view(N *
+                                                 P).to('cpu').detach().numpy()
+            skip_t = time.time() - mem_t
+
+            mesh = trimesh.Trimesh(
+                verts,
+                faces,
+                process=False,
+                vertex_attributes={'vertex_visibility': visbility})
+            stats_dict['time (copy to trimesh)'] = time.time() - t0 - skip_t
 
-            value_grid = mesh_extractor.to_dense()
+        else:
+            t0 = time.time()
+            threshold = 0.
+            #threshold = np.log(self.threshold) - np.log(1. - self.threshold)
+
+            # Compute bounding box size
+            box_size = 1 + self.padding
+
+            # Shortcut
+            if self.upsampling_steps == 0:
+                nx = self.resolution0
+                pointsf = box_size * make_3d_grid((-0.5, ) * 3, (0.5, ) * 3,
+                                                  (nx, ) * 3)
+                values = self.eval_points(pointsf, z, c, data=data,
+                                          **kwargs).cpu().numpy()
+                value_grid = values.reshape(nx, nx, nx)
+            else:
+                mesh_extractor = MISE(self.resolution0, self.upsampling_steps,
+                                      threshold)
 
-        # Extract mesh
-        stats_dict['time (eval points)'] = time.time() - t0
+                points = mesh_extractor.query()
 
-        mesh = self.extract_mesh(value_grid, z, c, stats_dict=stats_dict)
+                while points.shape[0] != 0:
+                    # Query points
+                    pointsf = torch.FloatTensor(points).to(self.device)
+                    # Normalize to bounding box
+                    pointsf = pointsf / mesh_extractor.resolution
+                    pointsf = box_size * (pointsf - 0.5)
+                    # Evaluate model and update
+                    values = self.eval_points(pointsf,
+                                              z,
+                                              c,
+                                              data=data,
+                                              **kwargs).cpu().numpy()
+                    values = values.astype(np.float64)
+                    mesh_extractor.update(points, values)
+                    points = mesh_extractor.query()
+
+                value_grid = mesh_extractor.to_dense()
+
+            # Extract mesh
+            stats_dict['time (eval points)'] = time.time() - t0
+
+            mesh = self.extract_mesh(value_grid, z, c, stats_dict=stats_dict)
         return mesh
 
-    def eval_points(self, p, z, c=None, **kwargs):
+    def eval_points(self, p, z, c=None, data=None, **kwargs):
         ''' Evaluates the occupancy values for the points.
 
         Args:
@@ -142,13 +203,26 @@ class Generator3D(object):
             z (tensor): latent code z
             c (tensor): latent conditioned code c
         '''
+        assert data is not None
         p_split = torch.split(p, self.points_batch_size)
+
+        angles = data.get('angles').to(self.device)
         occ_hats = []
 
         for pi in p_split:
             pi = pi.unsqueeze(0).to(self.device)
+            an = angles.to(self.device)
             with torch.no_grad():
-                occ_hat = self.model.decode(pi, z, c, **kwargs).logits
+                #_, _, sgn, _ = self.model.decode(pi, z, c, **kwargs).logits
+                _, _, sgn, _, _ = self.model.decode(pi * self.pnet_point_scale,
+                                                    z,
+                                                    c,
+                                                    angles=an,
+                                                    **kwargs)
+
+                occ_hat = (
+                    model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1) >
+                    self.threshold).float()
 
             occ_hats.append(occ_hat.squeeze(0).detach().cpu())
 
@@ -168,20 +242,20 @@ class Generator3D(object):
         # Some short hands
         n_x, n_y, n_z = occ_hat.shape
         box_size = 1 + self.padding
-        threshold = np.log(self.threshold) - np.log(1. - self.threshold)
+        threshold = 0.
+        #threshold = np.log(self.threshold) - np.log(1. - self.threshold)
         # Make sure that mesh is watertight
         t0 = time.time()
-        occ_hat_padded = np.pad(
-            occ_hat, 1, 'constant', constant_values=-1e6)
-        vertices, triangles = libmcubes.marching_cubes(
-            occ_hat_padded, threshold)
+        occ_hat_padded = np.pad(occ_hat, 1, 'constant', constant_values=-1e6)
+        vertices, triangles = libmcubes.marching_cubes(occ_hat_padded,
+                                                       threshold)
         stats_dict['time (marching cubes)'] = time.time() - t0
         # Strange behaviour in libmcubes: vertices are shifted by 0.5
         vertices -= 0.5
         # Undo padding
         vertices -= 1
         # Normalize to bounding box
-        vertices /= np.array([n_x-1, n_y-1, n_z-1])
+        vertices /= np.array([n_x - 1, n_y - 1, n_z - 1])
         vertices = box_size * (vertices - 0.5)
 
         # mesh_pymesh = pymesh.form_mesh(vertices, triangles)
@@ -197,7 +271,8 @@ class Generator3D(object):
             normals = None
 
         # Create mesh
-        mesh = trimesh.Trimesh(vertices, triangles,
+        mesh = trimesh.Trimesh(vertices,
+                               triangles,
                                vertex_normals=normals,
                                process=False)
 
@@ -261,7 +336,7 @@ class Generator3D(object):
 
         # Some shorthands
         n_x, n_y, n_z = occ_hat.shape
-        assert(n_x == n_y == n_z)
+        assert (n_x == n_y == n_z)
         # threshold = np.log(self.threshold) - np.log(1. - self.threshold)
         threshold = self.threshold
 
@@ -290,10 +365,9 @@ class Generator3D(object):
             face_normal = face_normal / \
                 (face_normal.norm(dim=1, keepdim=True) + 1e-10)
             face_value = torch.sigmoid(
-                self.model.decode(face_point.unsqueeze(0), z, c).logits
-            )
-            normal_target = -autograd.grad(
-                [face_value.sum()], [face_point], create_graph=True)[0]
+                self.model.decode(face_point.unsqueeze(0), z, c).logits)
+            normal_target = -autograd.grad([face_value.sum()], [face_point],
+                                           create_graph=True)[0]
 
             normal_target = \
                 normal_target / \
diff --git a/im2mesh/pnet/models/__init__.py b/im2mesh/pnet/models/__init__.py
index fd4e062..85fd048 100644
--- a/im2mesh/pnet/models/__init__.py
+++ b/im2mesh/pnet/models/__init__.py
@@ -109,9 +109,6 @@ class PeriodicShapeNetwork(nn.Module):
 
         assert angles is not None
         return self.decoder(p, z, c, angles=angles, **kwargs)
-        logits = self.decoder(p, z, c, **kwargs)
-        p_r = dist.Bernoulli(logits=logits)
-        return p_r
 
     def infer_z(self, p, occ, c, **kwargs):
         ''' Infers z.
diff --git a/im2mesh/pnet/models/decoder.py b/im2mesh/pnet/models/decoder.py
index ab92e38..32fe043 100644
--- a/im2mesh/pnet/models/decoder.py
+++ b/im2mesh/pnet/models/decoder.py
@@ -8,6 +8,8 @@ sys.path.insert(0, './external.periodic_shapes')
 from periodic_shapes.models import super_shape, super_shape_sampler, periodic_shape_sampler, sphere_sampler
 import time
 
+EPS = 1e-7
+
 
 class PeriodicShapeDecoderSimplest(nn.Module):
     ''' Decoder with CBN class 2.
@@ -45,7 +47,7 @@ class PeriodicShapeDecoderSimplest(nn.Module):
         paramnet_hidden_size=128,
         paramnet_dense=True,
         is_single_paramnet=False,
-        layer_depth=4,
+        layer_depth=0,
         skip_position=3,  # count start from input fc
         is_skip=True,
         shape_sampler_decoder_class='PrimitiveWiseGroupConvDecoder',
@@ -54,12 +56,17 @@ class PeriodicShapeDecoderSimplest(nn.Module):
         no_last_bias=False,
         supershape_freeze_rotation_scale=False,
         get_features_from=[],
-        concat_input_feature_with_pose_feature=False):
+        concat_input_feature_with_pose_feature=False,
+        return_sdf=False,
+        is_radius_reg=False,
+        spherical_angles=False,
+        last_scale=.1):
         super().__init__()
         assert dim in [2, 3]
         self.is_train_periodic_shape_sampler = is_train_periodic_shape_sampler
         self.get_features_from = get_features_from
         self.concat_input_feature_with_pose_feature = concat_input_feature_with_pose_feature
+        self.is_radius_reg = is_radius_reg
 
         self.primitive = super_shape.SuperShapes(
             max_m,
@@ -97,13 +104,16 @@ class PeriodicShapeDecoderSimplest(nn.Module):
             dim=dim,
             factor=shape_sampler_decoder_factor,
             no_encoder=True,
+            last_scale=last_scale,
             disable_learn_pose_but_transition=disable_learn_pose_but_transition,
             is_shape_sampler_sphere=is_shape_sampler_sphere,
             decoder_class=shape_sampler_decoder_class,
             is_feature_angles=is_feature_angles,
             is_feature_coord=is_feature_coord,
             is_feature_radius=is_feature_radius,
-            no_last_bias=no_last_bias)
+            no_last_bias=no_last_bias,
+            spherical_angles=spherical_angles,
+            return_sdf=return_sdf)
         # simple_sampler = super_shape_sampler.SuperShapeSampler(max_m,
         #                                                        n_primitives,
         #                                                        dim=dim)
@@ -126,12 +136,25 @@ class PeriodicShapeDecoderSimplest(nn.Module):
             else:
                 feature = color_feature
 
-            assert feature.shape[-1] == 256 + 128
             output = self.p_sampler(params,
                                     thetas=angles,
                                     coord=coord,
                                     points=feature,
                                     return_surface_mask=True)
+            pcoord, o1, o2, o3 = output
+            if self.is_radius_reg:
+                # B, N, P, dim
+                # pcoord
+
+                # B, N, 1, dim
+                transition = params['transition'].unsqueeze(2)
+                pcentered_coord = pcoord - transition
+                radius = (pcentered_coord**2).sum(-1).clamp(min=EPS).sqrt()
+                output = (pcoord, o1, o2, o3, radius)
+
+            else:
+                output = (pcoord, o1, o2, o3, None)
+
         else:
             output = self.simple_sampler(params,
                                          thetas=angles,
diff --git a/im2mesh/pnet/training.py b/im2mesh/pnet/training.py
index c0ed9b9..53db728 100644
--- a/im2mesh/pnet/training.py
+++ b/im2mesh/pnet/training.py
@@ -49,7 +49,18 @@ class Trainer(BaseTrainer):
                  is_onet_style_occ_loss=False,
                  is_logits_by_softmax=False,
                  is_l2_occ_loss=False,
-                 sgn_scale=100):
+                 sgn_scale=100,
+                 is_sdf=False,
+                 is_logits_by_logsumexp=False,
+                 is_logits_by_min=False,
+                 is_cvx_net_merged_loss=False,
+                 cvx_net_merged_loss_topk_samples=10,
+                 cvx_net_merged_loss_coef=1,
+                 is_eval_logits_by_max=False,
+                 is_radius_reg=False,
+                 radius_reg_coef=1.,
+                 use_surface_mask=True,
+                 sgn_offset=0):
         self.model = model
         self.optimizer = optimizer
         self.device = device
@@ -75,10 +86,27 @@ class Trainer(BaseTrainer):
         self.is_logits_by_softmax = is_logits_by_softmax
         self.is_l2_occ_loss = is_l2_occ_loss
         self.sgn_scale = sgn_scale
+        self.is_sdf = is_sdf
+        self.is_logits_by_logsumexp = is_logits_by_logsumexp
+        self.is_logits_by_min = is_logits_by_min
+        self.is_eval_logits_by_max = is_eval_logits_by_max
+        self.is_cvx_net_merged_loss = is_cvx_net_merged_loss
+        self.cvx_net_merged_loss_topk_samples = cvx_net_merged_loss_topk_samples
+        self.cvx_net_merged_loss_coef = cvx_net_merged_loss_coef
+        self.sgn_offset = sgn_offset
+        self.is_radius_reg = is_radius_reg
+        self.radius_reg_coef = radius_reg_coef
+        self.use_surface_mask = use_surface_mask
 
         if vis_dir is not None and not os.path.exists(vis_dir):
             os.makedirs(vis_dir)
 
+        if self.add_pointcloud_occ:
+            assert not self.is_sdf
+
+        if self.is_eval_logits_by_max:
+            assert not self.is_sdf
+
     def train_step(self, data):
         ''' Performs a training step.
 
@@ -130,21 +158,29 @@ class Trainer(BaseTrainer):
         batch_size = points.size(0)
 
         with torch.no_grad():
-            _, _, sgn, _ = self.model(points_iou * self.pnet_point_scale,
-                                      inputs,
-                                      sample=self.eval_sample,
-                                      angles=angles,
-                                      **kwargs)
-
-        if self.is_logits_by_max:
-            logits = model_utils.convert_tsd_range_to_zero_to_one(
-                sgn.max(1)[0])
-        elif self.is_logits_by_sign_filter:
-            positive = torch.relu(sgn).sum(1)
-            negative = torch.relu(-sgn).sum(1)
-            logits = torch.where(positive >= negative, positive, -negative)
+            _, _, sgn, _, _ = self.model(points_iou * self.pnet_point_scale,
+                                         inputs,
+                                         sample=self.eval_sample,
+                                         angles=angles,
+                                         **kwargs)
+        if self.is_sdf:
+            if self.is_logits_by_min:
+                logits = (sgn.min(1)[0] <= 0).float()
+            else:
+                raise NotImplementedError
         else:
-            logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1)
+            if self.is_eval_logits_by_max:
+                logits = (sgn >= 0.).float().max(1)[0]
+            elif self.is_logits_by_max:
+                logits = model_utils.convert_tsd_range_to_zero_to_one(
+                    sgn.max(1)[0])
+            elif self.is_logits_by_sign_filter:
+                positive = torch.relu(sgn).sum(1)
+                negative = torch.relu(-sgn).sum(1)
+                logits = torch.where(positive >= negative, positive, -negative)
+            else:
+                logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(
+                    1)
 
         occ_iou_np = (occ_iou >= self.threshold).cpu().numpy()
         occ_iou_hat_np = (logits >= threshold).cpu().numpy()
@@ -160,23 +196,32 @@ class Trainer(BaseTrainer):
                                                  *points_voxels.size())
             points_voxels = points_voxels.to(device)
             with torch.no_grad():
-                _, _, sgn, _ = self.model(points_voxels *
-                                          self.pnet_point_scale,
-                                          inputs,
-                                          sample=self.eval_sample,
-                                          angles=angles,
-                                          **kwargs)
-
-            if self.is_logits_by_max:
-                logits = model_utils.convert_tsd_range_to_zero_to_one(
-                    sgn.max(1)[0])
-            elif self.is_logits_by_sign_filter:
-                positive = torch.relu(sgn).sum(1)
-                negative = torch.relu(-sgn).sum(1)
-                logits = torch.where(positive >= negative, positive, -negative)
+                _, _, sgn, _, _ = self.model(points_voxels *
+                                             self.pnet_point_scale,
+                                             inputs,
+                                             sample=self.eval_sample,
+                                             angles=angles,
+                                             **kwargs)
+
+            if self.is_sdf:
+                if self.is_logits_by_min:
+                    logits = (sgn.min(1)[0] <= 0).float()
+                else:
+                    raise NotImplementedError
             else:
-                logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(
-                    1)
+                if self.is_eval_logits_by_max:
+                    logits = (sgn >= 0.).float().max(1)[0]
+                elif self.is_logits_by_max:
+                    logits = model_utils.convert_tsd_range_to_zero_to_one(
+                        sgn.max(1)[0])
+                elif self.is_logits_by_sign_filter:
+                    positive = torch.relu(sgn).sum(1)
+                    negative = torch.relu(-sgn).sum(1)
+                    logits = torch.where(positive >= negative, positive,
+                                         -negative)
+                else:
+                    logits = model_utils.convert_tsd_range_to_zero_to_one(
+                        sgn).sum(1)
 
             voxels_occ_np = (voxels_occ >= 0.5).cpu().numpy()
             occ_hat_np = (logits >= threshold).cpu().numpy()
@@ -204,21 +249,28 @@ class Trainer(BaseTrainer):
 
         kwargs = {}
         with torch.no_grad():
-            _, _, sgn, _ = self.model(p * self.pnet_point_scale,
-                                      inputs,
-                                      sample=self.eval_sample,
-                                      angles=angles,
-                                      **kwargs)
-
-        if self.is_logits_by_max:
-            logits = model_utils.convert_tsd_range_to_zero_to_one(
-                sgn.max(1)[0])
-        elif self.is_logits_by_sign_filter:
-            positive = torch.relu(sgn).sum(1)
-            negative = torch.relu(-sgn).sum(1)
-            logits = torch.where(positive >= negative, positive, -negative)
+            _, _, sgn, _, _ = self.model(p * self.pnet_point_scale,
+                                         inputs,
+                                         sample=self.eval_sample,
+                                         angles=angles,
+                                         **kwargs)
+
+        if self.is_sdf:
+            if self.is_logits_by_min:
+                logits = (sgn.min(1)[0] <= 0).float()
+            else:
+                raise NotImplementedError
         else:
-            logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(1)
+            if self.is_logits_by_max:
+                logits = model_utils.convert_tsd_range_to_zero_to_one(
+                    sgn.max(1)[0])
+            elif self.is_logits_by_sign_filter:
+                positive = torch.relu(sgn).sum(1)
+                negative = torch.relu(-sgn).sum(1)
+                logits = torch.where(positive >= negative, positive, -negative)
+            else:
+                logits = model_utils.convert_tsd_range_to_zero_to_one(sgn).sum(
+                    1)
         occ_hat = logits.view(batch_size, *shape)
         voxels_out = (occ_hat >= self.threshold).cpu().numpy()
 
@@ -250,9 +302,13 @@ class Trainer(BaseTrainer):
             data (dict): data dictionary
         '''
         device = self.device
-        points = data.get('points').to(device)
-        occ = data.get('points.occ').to(device)
-        print(occ.max(), occ.min(), (occ == 1).sum(), (occ == 0).sum())
+        if self.is_sdf:
+            points = data.get('sdf_points').to(device)
+            sdf = data.get('sdf_points.distances').to(device)
+            occ = (sdf <= 0).float()
+        else:
+            points = data.get('points').to(device)
+            occ = data.get('points.occ').to(device)
         inputs = data.get('inputs', torch.empty(points.size(0), 0)).to(device)
         angles = data.get('angles').to(device)
         pointcloud = data['pointcloud'].to(device)
@@ -263,7 +319,7 @@ class Trainer(BaseTrainer):
             normal_angles = data.get('angles.normal_angles').to(device)
 
         kwargs = {}
-        if self.add_pointcloud_occ:
+        if self.add_pointcloud_occ and not self.is_sdf:
             points = torch.cat([points, pointcloud], axis=1)
             occ = torch.cat([
                 occ,
@@ -280,37 +336,61 @@ class Trainer(BaseTrainer):
 
         scaled_coord = points * self.pnet_point_scale
         output = self.model.decode(scaled_coord, z, c, angles=angles, **kwargs)
-        super_shape_point, surface_mask, sgn, sgn_BxNxNP = output
+        super_shape_point, surface_mask, sgn, sgn_BxNxNP, radius = output
 
         # losses
-        if self.is_logits_by_max:
-            logits = sgn.max(1)[0]
-        elif self.is_logits_by_sign_filter:
-            positive = torch.relu(sgn).sum(1)
-            negative = torch.relu(-sgn).sum(1)
-            logits = torch.where(positive >= negative, positive, -negative)
-        elif self.is_logits_by_softmax:
-            logits = (torch.softmax(sgn * 10, 1) * sgn).sum(1)
-        else:
-            logits = model_utils.convert_tsd_range_to_zero_to_one(
-                sgn, scale=self.sgn_scale).sum(1)
-
-        if self.is_onet_style_occ_loss:
-            loss_i = F.binary_cross_entropy_with_logits(logits,
-                                                        occ,
-                                                        reduction='none')
-            occupancy_loss = loss_i.sum(-1).mean()
-        elif self.is_l2_occ_loss:
-            occupancy_loss = ((logits - occ)**2).sum()
+        if self.is_sdf:
+            if self.is_logits_by_logsumexp:
+                raise NotImplementedError
+            if self.is_logits_by_sign_filter:
+                raise NotImplementedError
+            if self.is_logits_by_softmax:
+                raise NotImplementedError
+            if self.is_logits_by_max:
+                raise NotImplementedError
+            if self.is_logits_by_min:
+                logits = sgn.min(1)[0]
+            else:
+                raise NotImplementedError
+            """
+            delta = 0.01 * self.pnet_point_scale**2
+            occupancy_loss = (logits.clamp(min=-delta, max=delta) -
+                              (sdf * self.pnet_point_scale**2).clamp(
+                                  min=-delta, max=delta)).abs().mean()
+            """
+            occupancy_loss = (logits -
+                              (sdf * self.pnet_point_scale**2)).abs().mean()
+
         else:
-            occupancy_loss = F.binary_cross_entropy_with_logits(logits, occ)
+            if self.is_logits_by_max:
+                logits = sgn.max(1)[0]
+            elif self.is_logits_by_sign_filter:
+                positive = torch.relu(sgn).sum(1)
+                negative = torch.relu(-sgn).sum(1)
+                logits = torch.where(positive >= negative, positive, -negative)
+            elif self.is_logits_by_softmax:
+                logits = (torch.softmax(sgn * 10, 1) * sgn).sum(1)
+            else:
+                logits = model_utils.convert_tsd_range_to_zero_to_one(
+                    sgn, scale=self.sgn_scale).sum(1)
+
+            if self.is_onet_style_occ_loss:
+                loss_i = F.binary_cross_entropy_with_logits(logits,
+                                                            occ,
+                                                            reduction='none')
+                occupancy_loss = loss_i.sum(-1).mean()
+            elif self.is_l2_occ_loss:
+                occupancy_loss = ((logits - occ)**2).sum()
+            else:
+                occupancy_loss = F.binary_cross_entropy_with_logits(
+                    logits - self.sgn_offset, occ)
 
         scaled_target_point = pointcloud * self.pnet_point_scale
 
         chamfer_loss, _ = custom_chamfer_loss.custom_chamfer_loss(
             super_shape_point,
             scaled_target_point,
-            surface_mask=surface_mask,
+            surface_mask=(surface_mask if self.use_surface_mask else None),
             prob=None,
             pykeops=True,
             apply_surface_mask_before_chamfer=self.is_strict_chamfer)
@@ -321,7 +401,7 @@ class Trainer(BaseTrainer):
                                        c,
                                        angles=normal_angles,
                                        **kwargs)
-            normal_vertices, normal_mask, _, _ = output
+            normal_vertices, normal_mask, _, _, _ = output
 
             B, N, P, D = normal_vertices.shape
             normal_faces_all = torch.cat([(normal_faces + idx * P)
@@ -354,8 +434,24 @@ class Trainer(BaseTrainer):
                       overlap_reg * self.overlap_reg_coef +
                       self_overlap_reg * self.self_overlap_reg_coef)
 
+        if self.is_cvx_net_merged_loss:
+            merged_loss = torch.topk(sgn,
+                                     self.cvx_net_merged_loss_topk_samples,
+                                     2)[0]
+            merged_loss = (torch.relu(-merged_loss)**
+                           2).mean(-1).mean() * self.cvx_net_merged_loss_coef
+            print('cvx merged loss:', merged_loss.item())
+            total_loss = total_loss + merged_loss
+
         if self.is_normal_loss:
             total_loss = total_loss + normal_loss * self.normal_loss_coef
+
+        if self.is_radius_reg:
+            assert radius is not None
+            radius_reg_loss = torch.relu(1. -
+                                         radius.std()) * self.radius_reg_coef
+            print('radius reg loss', radius_reg_loss.item())
+            total_loss = total_loss + radius_reg_loss
         losses = {
             'total_loss': total_loss,
             'occupancy_loss': occupancy_loss * self.occupancy_loss_coef,
diff --git a/train.py b/train.py
index 7f90be1..d761de2 100755
--- a/train.py
+++ b/train.py
@@ -57,10 +57,15 @@ if not os.path.exists(out_dir):
 train_dataset = config.get_dataset('train', cfg)
 val_dataset = config.get_dataset('val', cfg)
 
+if 'debug' in cfg['data']:
+    train_shuffle = cfg['data']['debug'].get('train_shuffle', False)
+else:
+    train_shuffle = True
+
 train_loader = torch.utils.data.DataLoader(train_dataset,
                                            batch_size=batch_size,
                                            num_workers=4,
-                                           shuffle=True,
+                                           shuffle=train_shuffle,
                                            collate_fn=data.collate_remove_none,
                                            worker_init_fn=data.worker_init_fn,
                                            drop_last=True)
@@ -75,7 +80,8 @@ val_loader = torch.utils.data.DataLoader(
 
 # For visualizations
 vis_loader = torch.utils.data.DataLoader(val_dataset,
-                                         batch_size=5,
+                                         batch_size=cfg['training'].get(
+                                             'vis_batch_size', 1),
                                          shuffle=False,
                                          collate_fn=data.collate_remove_none,
                                          worker_init_fn=data.worker_init_fn)
