{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitenvvenv63b415f700194b88abf3dd42ee12a82f",
   "display_name": "Python 3.7.4 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import sys \n",
    "sys.path.insert(0, '..')\n",
    "import kaolin as kal\n",
    "from kaolin.datasets import shapenet\n",
    "from kaolin import rep\n",
    "from kaolin import conversions\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from models import periodic_shape_sampler_xyz\n",
    "from models import super_shape_sampler\n",
    "from models import super_shape\n",
    "from models import model_utils\n",
    "import utils\n",
    "from losses import custom_chamfer_loss\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import dotenv\n",
    "import plotly.graph_objects as go\n",
    "from visualize import plot\n",
    "import trimesh\n",
    "from metrics import metrics_functions\n",
    "from external.PyTorchEMD import emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fb521c9c650>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "random.seed(seed)  \n",
    "np.random.seed(seed)  \n",
    "# PyTorch のRNGを初期化  \n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(verbose=True)\n",
    "\n",
    "category = 'plane'\n",
    "cache_root = os.getenv('SHAPENET_KAOLIN_CACHE_ROOT')\n",
    "shapenet_root = os.getenv('SHAPENET_ROOT')\n",
    "cache_dir = os.path.join(cache_root, category)\n",
    "\n",
    "categories = [category]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "converting to voxels: 100%|██████████| 4045/4045 [00:00<00:00, 33220.40it/s]\nconverting to surface meshes: 100%|██████████| 4045/4045 [00:00<00:00, 32525.20it/s]\nconverting to sdf points: 100%|██████████| 4045/4045 [00:00<00:00, 32503.58it/s]\nconverting to voxels: 100%|██████████| 4045/4045 [00:00<00:00, 32776.80it/s]\nconverting to surface meshes: 100%|██████████| 4045/4045 [00:00<00:00, 32620.64it/s]\nconverting to points: 100%|██████████| 4045/4045 [00:00<00:00, 32669.38it/s]\nconverting to voxels: 100%|██████████| 4045/4045 [00:00<00:00, 33527.51it/s]\nconverting to surface meshes: 100%|██████████| 4045/4045 [00:00<00:00, 32770.09it/s]\n"
    }
   ],
   "source": [
    "sdf_set = shapenet.ShapeNet_SDF_Points(root=shapenet_root, categories=categories, cache_dir=cache_dir, train=True, split=1.)\n",
    "point_set = shapenet.ShapeNet_Points(root=shapenet_root, categories=categories, cache_dir=cache_dir, train=True, split=1.)\n",
    "surface_set = shapenet.ShapeNet_Surface_Meshes(root=shapenet_root, categories=categories, cache_dir=cache_dir, train=True, split=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 4\n",
    "n = 6\n",
    "dim = 3\n",
    "points_sample_num = 3000\n",
    "sample_idx = 0\n",
    "train_theta_sample_num = 100\n",
    "\n",
    "device_type = 'cuda:0'\n",
    "device = torch.device(device_type)\n",
    "\n",
    "primitive = super_shape.SuperShapes(m, n, quadrics=True, train_ab=False, dim=dim, transition_range=3)\n",
    "primitive.to(device)\n",
    "primitive.eval()\n",
    "primitive.load_state_dict(torch.load('primitive.pth', map_location=device))\n",
    "\n",
    "sampler = periodic_shape_sampler_xyz.PeriodicShapeSamplerXYZ(points_sample_num, m, n, factor=2, dim=dim)\n",
    "sampler.to(device)\n",
    "sampler.eval()\n",
    "sampler.load_state_dict(torch.load('periodic_sampler.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(-0.3528, device='cuda:0') tensor(0.3458, device='cuda:0')\ntorch.Size([1, 52029, 3]) torch.Size([5000, 3])\ntensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.8860, device='cuda:0')\n"
    }
   ],
   "source": [
    "\n",
    "points = point_set[sample_idx]['data']['points'].to(device) * 10\n",
    "all_points_num = points.shape[0]\n",
    "\n",
    "# grid_points_num, dim\n",
    "xyz = utils.generate_grid_samples(4, sampling='uniform', sample_num=100, dim=dim).to(device)\n",
    "mesh = surface_set[sample_idx]['data']\n",
    "meshkal = rep.TriangleMesh.from_tensors(mesh['vertices']*10,\n",
    "                                    mesh['faces'])\n",
    "meshkal.to('cuda:0')\n",
    "sdf_func = kal.conversions.trianglemesh_to_sdf(meshkal, xyz.shape[0])\n",
    "sgn = (sdf_func(xyz[0].to('cuda:0')).to(device) <= 0.001)\n",
    "\n",
    "\n",
    "sampled_points = points[random.sample(range(all_points_num), points_sample_num), :].view(1, -1, dim)\n",
    "print(sampled_points.min()/10, sampled_points.max()/10)\n",
    "thetas = utils.sample_spherical_angles(batch=1, sample_num=train_theta_sample_num, sampling='uniform', device=device, dim=dim)\n",
    "\n",
    "pred_points, pred_mask, pred_tsd = sampler(primitive(), points=sampled_points, thetas=thetas, coord=xyz)\n",
    "\n",
    "pred_surf_points = sampler.extract_super_shapes_surface_point(pred_points, primitive(), points=sampled_points)\n",
    "print(pred_surf_points.shape, points.shape)\n",
    "cd1 = metrics_functions.chamfer_distance_l1(pred_surf_points[0].to('cuda:0')[random.sample(range(pred_surf_points.shape[1]), all_points_num), :]/10, points.to('cuda:0')/10)\n",
    "fscr = kal.metrics.f_score(pred_surf_points[0].to('cuda:0')[random.sample(range(pred_surf_points.shape[1]), all_points_num), :]/10, points.to('cuda:0')/10)\n",
    "\n",
    "\"\"\"\n",
    "emd_src = pred_surf_points[:, random.sample(range(pred_surf_points.shape[1]), all_points_num), :]/10\n",
    "emd_dst = points.unsqueeze(0)/10\n",
    "print(emd_src.shape, emd_dst.shape)\n",
    "emdscr = emd.earth_mover_distance(emd_src, emd_dst, transpose=False)/all_points_num\n",
    "\"\"\"\n",
    "print(cd1, fscr)\n",
    "\n",
    "\n",
    "prd_sgn = model_utils.convert_tsd_range_to_zero_to_one(pred_tsd).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(0.9253, device='cuda:0')\n"
    }
   ],
   "source": [
    "prd_sgn_sgn = (prd_sgn >=0.5).view(-1)\n",
    "iou = ((prd_sgn_sgn & sgn).float().sum() / (prd_sgn_sgn | sgn).float().sum() )\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(-2.8966, device='cuda:0') tensor(3.3226, device='cuda:0')\n"
    }
   ],
   "source": [
    "normalizer = kal.transforms.NormalizePointCloud()\n",
    "normalized_pred_points = normalizer(points/10)\n",
    "print(normalized_pred_points.min(), normalized_pred_points.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(6.9861, device='cuda:0')\n"
    }
   ],
   "source": [
    "bb_min = sampled_points.min(1)[0]\n",
    "bb_max = sampled_points.max(1)[0]\n",
    "total_size = (bb_max - bb_min).max()\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0.0351, -0.0013, -0.0004]], device='cuda:0') tensor(0.1431, device='cuda:0')\ntensor([[-0.5000, -0.1600, -0.4775]], device='cuda:0') tensor([[0.5000, 0.1600, 0.4775]], device='cuda:0')\n"
    }
   ],
   "source": [
    "bb_min = sampled_points.min(1)[0]\n",
    "bb_max = sampled_points.max(1)[0]\n",
    "total_size = (bb_max - bb_min).max()\n",
    "\n",
    "# Set the center (although this should usually be the origin already).\n",
    "centers = (bb_min + bb_max) / 2\n",
    "\n",
    "# Scales all dimensions equally.\n",
    "scale = total_size\n",
    "\n",
    "translation = -centers\n",
    "scale_inv = 1./scale\n",
    "print(translation, scale_inv)\n",
    "\n",
    "scaled_points = (sampled_points + translation) * scale_inv\n",
    "print(scaled_points.min(1)[0], scaled_points.max(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf(coord):\n",
    "    \n",
    "    coord = coord.unsqueeze(0)\n",
    "    assert len(coord.shape) == 3\n",
    "    _, _, tsd = sampler(primitive(), points=sampled_points, coord=coord.to(device)/scale_inv - translation)\n",
    "    sgn = -((model_utils.convert_tsd_range_to_zero_to_one(tsd).sum(1) >= 0.5).float() - 0.5) \n",
    "    return sgn.view(-1).to('cpu')\n",
    "    \n",
    "    #return (coord**2).sum(-1).to('cpu') - 0.2\n",
    "#verts, faces = kal.conversions.sdf_to_trianglemesh(get_sdf, resolution=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nfrom visualize import plot\\npoints_list = [pred_points, points.view(1, 1, -1, dim)]\\nfig = plt.figure()\\nfor idx, points in enumerate(points_list):\\n    plot.plot_primitive_point_cloud_3d(points)\\n'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from visualize import plot\n",
    "points_list = [pred_points, points.view(1, 1, -1, dim)]\n",
    "fig = plt.figure()\n",
    "for idx, points in enumerate(points_list):\n",
    "    plot.plot_primitive_point_cloud_3d(points)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_from_sdf = (kal.conversions.sdf_to_pointcloud(get_sdf, resolution=64).to(device)/scale_inv-translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([5000, 3]) cuda:0 cuda:0 torch.Size([5000, 3])\n"
    }
   ],
   "source": [
    "print(points_from_sdf.shape, translation.device, scale_inv.device, points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(0.0186, device='cuda:0') tensor(0.9107, device='cuda:0')\n"
    }
   ],
   "source": [
    "cd1 = metrics_functions.chamfer_distance_l1(points_from_sdf/10, points.to(device)/10)\n",
    "fscr = kal.metrics.f_score(points_from_sdf/10, points.to(device)/10)\n",
    "print(cd1, fscr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0090, device='cuda:0')"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geomloss\n",
    "geomloss.SamplesLoss(p=1, blur=1e-7)(points_from_sdf/10, points.to(device)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 2400, 1, 3]) torch.Size([1, 1, 5000, 3])\n"
    },
    {
     "data": {
      "text/plain": "tensor(0.0369, device='cuda:0')"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykeops.torch import LazyTensor \n",
    "#a = points_from_sdf.view(1, -1, 1, 3)/10\n",
    "a = torch.zeros(1, 2400, 1, 3).float().to(device)\n",
    "b = points.to(device).view(1, 1, -1, 3)/10\n",
    "print(a.shape, b.shape)\n",
    "al = LazyTensor(a)\n",
    "bl = LazyTensor(b)\n",
    "((al - bl) ** 2).norm2().min(2)\n",
    "((al - bl) ** 2).norm2().min(1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 6, 10000, 3]) torch.Size([1, 3000, 3]) torch.Size([1, 6, 10000])\n"
    }
   ],
   "source": [
    "print(pred_points.shape, sampled_points.shape, pred_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0020, device='cuda:0', grad_fn=<DivBackward0>)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from losses import custom_chamfer_loss\n",
    "custom_chamfer_loss.custom_chamfer_loss(pred_points, sampled_points, surface_mask=pred_mask, pykeops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([0.3449], device='cuda:0')\n"
    }
   ],
   "source": [
    "emdscr = emd.earth_mover_distance(points_from_sdf.unsqueeze(0)/10, points.unsqueeze(0).to(device)/10, transpose=False)/5000*100\n",
    "print(emdscr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'verts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-fe0b4034b2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m mesh = trimesh.Trimesh(vertices=verts.cpu().numpy(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                        \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                        process=False)\n\u001b[1;32m      4\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'verts' is not defined"
     ]
    }
   ],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts.cpu().numpy(),\n",
    "                       faces=faces.cpu().numpy(),\n",
    "                       process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def refine_mesh(self, mesh, occ_hat, z, c=None):\n",
    "        ''' Refines the predicted mesh.\n",
    "        Args:   \n",
    "            mesh (trimesh object): predicted mesh\n",
    "            occ_hat (tensor): predicted occupancy grid\n",
    "            z (tensor): latent code z\n",
    "            c (tensor): latent conditioned code c\n",
    "        '''\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Some shorthands\n",
    "        n_x, n_y, n_z = occ_hat.shape\n",
    "        assert(n_x == n_y == n_z)\n",
    "        # threshold = np.log(self.threshold) - np.log(1. - self.threshold)\n",
    "        threshold = self.threshold\n",
    "\n",
    "        # Vertex parameter\n",
    "        v0 = torch.FloatTensor(mesh.vertices).to(self.device)\n",
    "        v = torch.nn.Parameter(v0.clone())\n",
    "\n",
    "        # Faces of mesh\n",
    "        faces = torch.LongTensor(mesh.faces).to(self.device)\n",
    "\n",
    "        # Start optimization\n",
    "        optimizer = optim.RMSprop([v], lr=1e-4)\n",
    "\n",
    "        for it_r in trange(self.refinement_step):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss\n",
    "            face_vertex = v[faces]\n",
    "            eps = np.random.dirichlet((0.5, 0.5, 0.5), size=faces.shape[0])\n",
    "            eps = torch.FloatTensor(eps).to(self.device)\n",
    "            face_point = (face_vertex * eps[:, :, None]).sum(dim=1)\n",
    "\n",
    "            face_v1 = face_vertex[:, 1, :] - face_vertex[:, 0, :]\n",
    "            face_v2 = face_vertex[:, 2, :] - face_vertex[:, 1, :]\n",
    "            face_normal = torch.cross(face_v1, face_v2)\n",
    "            face_normal = face_normal / \\\n",
    "                (face_normal.norm(dim=1, keepdim=True) + 1e-10)\n",
    "            face_value = torch.sigmoid(\n",
    "                self.model.decode(face_point.unsqueeze(0), z, c).logits\n",
    "            )\n",
    "            normal_target = -autograd.grad(\n",
    "                [face_value.sum()], [face_point], create_graph=True)[0]\n",
    "\n",
    "            normal_target = \\\n",
    "                normal_target / \\\n",
    "                (normal_target.norm(dim=1, keepdim=True) + 1e-10)\n",
    "            loss_target = (face_value - threshold).pow(2).mean()\n",
    "            loss_normal = \\\n",
    "                (face_normal - normal_target).pow(2).sum(dim=1).mean()\n",
    "\n",
    "            loss = loss_target + 0.01 * loss_normal\n",
    "\n",
    "            # Update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        mesh.vertices = v.data.cpu().numpy()\n",
    "\n",
    "        return mesh"
   ]
  }
 ]
}